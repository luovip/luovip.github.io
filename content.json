{"posts":[{"title":"Kubernetes企业级运维","text":"1 Kubernetes概述","link":"/2025/05/10/Kubernetes/Kubernetes%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%BF%90%E7%BB%B4/"},{"title":"Kubernetes网络系统原理","text":"1 网络通信基础","link":"/2025/05/10/Kubernetes/Kubernetes%E7%BD%91%E7%BB%9C%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/"},{"title":"Docker&amp;K8S","text":"1 Docker的概念与安装 1.1 Docker简介​ Docker是开源的虚拟化容器引擎，开发者可以打包应用及依赖到一个可移植的容器中，然后发布到Linux环境中以实现虚拟化的管理 ​ Linux环境包括CentOS、Redhat、Ubuntu等 ​ Docker中的虚拟化容器完全使用“沙箱”机制，相互之间不会有任何接口，容器管理是逻辑隔离的 ​ 一个完整的Docker由以下几部分组成： ​ Docker客户端、Docker守护进程(Daemon)、Docker镜像(image)、Docker容器(Container)、镜像仓库(Repository) ​ Docker实现了应用代码与底层运行环境之间的耦合，可将一个复杂系统中的各个模块进行容器化，同时提供了负载均衡和失败迁移功能 ​ 应用的容器化满足了敏捷开发、动态迁移、标准化的要求，提高了效率 1.2 Docker的体系架构与概念​ Docker是一个客户端服务器(C/S)架构 ​ Docker客户端和Docker守护进程交流，Docker守护进程是运行Docker的核心，如：构建、运行、分发Docker容器 ​ Docker客户端和Docker守护进程可运行在同样的系统上，也可运行在不同的系统上 ​ 用户可以将一个Docker客户端连接到一个远程Docker守护进程中 ​ Docker客户端和Docker守护进程通过sockets或RESTFul API进行沟通 ​ 使用Docker创建容器时需要有镜像，镜像是一个只读的模板，存放镜像的地方叫“镜像仓库” ​ 镜像仓库分为：公有镜像仓库(如:官方提供的公有镜像仓库Docker Hub)、私有镜像仓库(如：Harbor) ​ Docker体系架构中的组成部分及功能特性： Docker客户端 Docker提供的命令行工具，是Docker最基本的用户接口 Docker守护进程 在Docker宿主机上运行Docker，实际上运行的是Docker守护进程 Docker镜像 Docker镜像是可读的模板，分为公有和私有 Docker容器 Docker容器可看成是虚拟机 镜像仓库 用于保存Docker镜像 1.3 使用Yum方式安装Docker​ 利用Yum方式可以很方便的添加、删除和更新Linux系统的程序包，并且能自动解决包的依赖问题 ​ 使用Yum方式也能够方便的管理大量的系统更新问题 ​ 一般使用Yum方式需连接外部网络 ​ 1.为了验证虚拟机是否可访问外部网络，输入以下命令访问百度主页 123456[root@master ~]# ping www.baidu.comPING www.a.shifen.com (39.156.70.239) 56(84) bytes of data.64 bytes from 39.156.70.239 (39.156.70.239): icmp_seq=1 ttl=128 time=39.8 ms64 bytes from 39.156.70.239 (39.156.70.239): icmp_seq=2 ttl=128 time=39.4 ms64 bytes from 39.156.70.239 (39.156.70.239): icmp_seq=3 ttl=128 time=39.5 ms64 bytes from 39.156.70.239 (39.156.70.239): icmp_seq=4 ttl=128 time=39.3 ms ​ 2.执行以下命令使用Yum方式安装Docker 12[root@master ~]# yum -y install docker# 也可以指定具体版本 yum -y install docker-1.31.1 ​ 3.执行以下命令启动Docker服务 12[root@master ~]# systemctl start docker.service[root@master ~]# systemctl status docker.service ​ 4.确定Docker版本信息 1234567891011121314151617181920[root@master ~]# docker versionClient: Version: 1.13.1 API version: 1.26 Package version: docker-1.13.1-210.git7d71120.el7.centos.x86_64 Go version: go1.10.3 Git commit: 7d71120/1.13.1 Built: Wed Mar 20 16:04:34 2024 OS/Arch: linux/amd64Server: Version: 1.13.1 API version: 1.26 (minimum version 1.12) Package version: docker-1.13.1-210.git7d71120.el7.centos.x86_64 Go version: go1.10.3 Git commit: 7d71120/1.13.1 Built: Wed Mar 20 16:04:34 2024 OS/Arch: linux/amd64 Experimental: false # 从以上可看出，Docker分为Client端和Server端，当前安装的Docker是1.31.1版本 1.4 使用二进制包安装Docker​ 可使用Docker官方提供的二进制包进行Docker的离线安装 ​ 下载地址：https://download.docker.com/linux/static/stable/x86_64/ 1.安装wget工具并下载Docker安装包 12[root@master ~]# yum -y install wget[root@master ~]# wget https://download.docker.com/linux/static/stable/x86_64/docker-28.0.4.tgz 2.使用tar命令解压缩Docker二进制安装包 123456789101112[root@master ~]# file docker-28.0.4.tgzdocker-28.0.4.tgz: gzip compressed data, from Unix[root@master ~]# tar -zxvf docker-28.0.4.tgzdocker/docker/containerd-shim-runc-v2docker/containerddocker/dockerdocker/runcdocker/ctrdocker/dockerddocker/docker-initdocker/docker-proxy 3.查看Docker二进制包提供的执行命令 12[root@master ~]# ls dockercontainerd containerd-shim-runc-v2 ctr docker dockerd docker-init docker-proxy runc 4.将Docker的可执行命令复制到“/usr/bin/“目录下 12345[root@master ~]# cp docker/* /usr/bin/cp: overwrite ‘/usr/bin/containerd’? ycp: overwrite ‘/usr/bin/containerd-shim-runc-v2’? ycp: overwrite ‘/usr/bin/ctr’? yycp: overwrite ‘/usr/bin/runc’? y 5.执行以下命令启动Docker服务，启动成功后输出的日志信息如下 1234567[root@master ~]# /usr/bin/dockerdINFO[2025-04-05T19:54:53.383490501+08:00] Starting upWARN[2025-04-05T19:54:53.385072958+08:00] could not change group /var/run/docker.sock to docker: group docker not foundINFO[2025-04-05T19:54:53.385132079+08:00] containerd not running, starting managed containerdINFO[2025-04-05T19:54:53.405178539+08:00] started new containerd process address=/var/run/docker/containerd/containerd.sock module=libcontainerd pid=10553INFO[2025-04-05T19:54:53.454987934+08:00] starting containerd revision=05044ec0a9a75232cad458027ca83437aae3f4da version=v1.7.27...... 6.执行以下语句查看Docker版本信息 1234567891011121314151617181920212223242526272829[root@master ~]# /usr/bin/docker versionClient: Version: 28.0.4 API version: 1.48 Go version: go1.23.7 Git commit: b8034c0 Built: Tue Mar 25 15:06:08 2025 OS/Arch: linux/amd64 Context: defaultServer: Docker Engine - Community Engine: Version: 28.0.4 API version: 1.48 (minimum version 1.24) Go version: go1.23.7 Git commit: 6430e49 Built: Tue Mar 25 15:07:23 2025 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.7.27 GitCommit: 05044ec0a9a75232cad458027ca83437aae3f4da runc: Version: 1.2.6 GitCommit: v1.2.6-0-ge89a299 docker-init: Version: 0.19.0 GitCommit: de40ad0# 以上信息显示Docker的版本(客户端与服务端)为28.0.4 7.将Docker运行在后台 123[root@master ~]# nohup /usr/bin/dockerd &gt; /tmp/docker.log 2&gt;&amp; 1 &amp;[1] 10831# 以上命令除将Docker运行在后台外，还会保存Docker日志到“/tmp/docker.log&quot;文件中 8.在“/etc/systemd/system/docker.service“文件中输入以下内容: 123456789101112131415161718192021[root@master ~]# vim /etc/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyExecStart=/usr/bin/dockerdExecReload=/bin/ki11 -S HUP $MAINPIDLimitNOFILE=infinityLimitNPROC=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target 8.在创建“/etc/systemd/system/docker.service“文件后，需要给该文件添加可执行权限: 123[root@master ~]# chmod +x /etc/systemd/system/docker.service[root@master ~]# ls -l /etc/systemd/system/docker.service-rwxr-xr-x 1 root root 440 Apr 5 12:33 /etc/systemd/system/docker.service 9.启动Docker并设置Docker为开机自启模式 123[root@master ~]# systemctl start docker[root@master ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /etc/systemd/system/docker.service. 10.验证Docker环境 ​ 使用“docker info“命令可查看Docker运行状态的详细信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@master ~]# docker infoClient: Version: 28.0.4 Context: default Debug Mode: falseServer: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 1 Server Version: 28.0.4 Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Using metacopy: false Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog Swarm: inactive Runtimes: io.containerd.runc.v2 runc Default Runtime: runc Init Binary: docker-init containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da runc version: v1.2.6-0-ge89a299 init version: de40ad0 Security Options: seccomp Profile: builtin Kernel Version: 3.10.0-957.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 1 Total Memory: 1.934GiB Name: master ID: 054106da-23b5-44a3-9d78-93fd89f9bf68 Docker Root Dir: /var/lib/docker Debug Mode: false Experimental: false Insecure Registries: ::1/128 127.0.0.0/8 Live Restore Enabled: false Product License: Community Engine# 在Server端配置参数中，Registry表示仓库地址，这里默认使用官方提供的Docker Hub镜像仓库 11.查看Docker服务状态 1234567891011[root@master ~]# systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/etc/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2025-04-05 14:29:43 CST; 38s ago Docs: https://docs.docker.com Main PID: 12314 (dockerd) Tasks: 14 Memory: 33.7M CGroup: /system.slice/docker.service ├─12314 /usr/bin/dockerd └─12319 containerd --config /var/run/docker/containerd/containerd.toml --log-level info 1.5 在Docker中部署第一个应用​ 在成功安装Docker后，可以通过镜像来创建容器，从而运行应用 ​ 在Docker中通过使用Nginx镜像部署第一个应用，并在浏览器访问 1.在镜像仓库中搜索Nginx镜像，其中，OFFICAL列中标有【OK】的镜像是Docker官方提供的镜像 1[root@master ~]# docker search nginx 2.通过以下命令从镜像仓库拉取Nginx的镜像到本地 12345678910111213[root@master ~]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginx6e909acdb790: Pull complete5eaa34f5b9c2: Pull complete417c4bccf534: Pull completee7e0ca015e55: Pull complete373fe654e984: Pull complete97f5c0f51d43: Pull completec22eb46e871a: Pull completeDigest: sha256:124b44bfc9ccd1f3cedf4b592d4d1e8bddb78b51ec2ed5056c52d3692baebc19Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest 3.使用“docker images”命令查看本地的镜像信息 123[root@master ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest 53a18edff809 8 weeks ago 192MB 4.执行以下命令将使用镜像来创建Nginx的容器 12345[root@master ~]# docker run -d -p 8080:80 nginxac35595599b9f236f3ea59aadd401cbdf0c8b85e57660282c73d615b1de2e1a3# 参数说明:-d：启动容器的守护进程-p：将容器内的80端口映射到宿主机的8080端口，就可以通过宿主机访问容器内部 5.查看Docker容器的信息 12[root@master ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac35595599b9 nginx &quot;/docker-entrypoint.…&quot; 54 seconds ago Up 53 seconds 0.0.0.0:8080-&gt;80/tcp, [::]:8080-&gt;80/tcp vibrant_swanson 6.打开浏览器访问：“","link":"/2025/05/10/%E5%AE%B9%E5%99%A8/Docker&Kubernetes/"},{"title":"podman容器","text":"1 容器的概念 1.1 容器技术介绍&emsp;&emsp;软件应用通常依赖于运行时环境(runtimeenvironment)提供的系统库、配置文件或服务 &emsp;&emsp;传统软件应用的运行时环境安装在物理主机或虚拟机上运行的操作系统中。然后，管理员在操作系统上安装应用依赖项 &emsp;&emsp;在红帽企业Linux中，诸如RPM等打包系统可协助管理员管理应用依赖项。安装httpd软件包时，RPM系统会确保同时安装该软件包的正确库和其他依赖项 &emsp;&emsp;传统方式部署软件应用的主要弊端是这些依赖项会受到运行时环境的束缚，应用需要的支持软件的版本可能比操作系统提供的软件更旧或更新，同一系统上的两个应用可能需要同一软件互不兼容的不同版本，解决这些冲突的方式之一是将应用打包并作为容器进行部署 &emsp;&emsp;容器是由一个或多个与系统其余部分隔离的进程组成的集合，软件容器是打包应用以简化其部署和管理的一种方式 &emsp;&emsp;以实体集装箱为例。集装箱是打包和装运货物的标准方式。作为一个箱子进行标记、装载、卸载，以及从一个位置运输到另一个位置。集装箱中的内容与其他集装箱的内容隔离，因此互不影响。这些基本原则也适用于软件容器 1.2 容器技术的核心&emsp;&emsp;红帽企业Linux通过运用以下核心技术来支持容器: &emsp;&emsp;1.用于资源管理的控制组(cgroups) &emsp;&emsp;2.用于进程隔离的命名空间 &emsp;&emsp;3.加强安全边界的SELinux和Seccomp安全计算模式 1.3 容器和虚拟机的差异&emsp;&emsp;1.容器提供许多与虚拟机相同的益处，如安全、存储和网络隔离等 &emsp;&emsp;2.这两种技术都将其应用库和运行时资源与主机操作系统或虚拟机监控程序隔离开 &emsp;&emsp;3.容器和虚拟机以不同的方式与硬件和底层操作系统交互 &emsp;&emsp;4.虚拟机具有以下特征: &emsp;&emsp;&emsp;4.1 使多个操作系统能够同时在一个硬件平台上运行 &emsp;&emsp;&emsp;4.2 使用虚拟机监控程序将硬件分为多个虚拟硬件系统 &emsp;&emsp;&emsp;4.3 需要一个完整的操作系统环境来支持该应用 &emsp;&emsp;5.容器具有以下特征: &emsp;&emsp;&emsp;5.1 直接在操作系统上运行，从而跨系统上的所有容器共享资源 &emsp;&emsp;&emsp;5.2 共享主机的内核，但它将应用进程与系统其余部分隔离开来 &emsp;&emsp;&emsp;5.3 与虚拟机相比，它需要的硬件资源要少得多，因此容器的启动速度也更快 &emsp;&emsp;&emsp;5.4 包括所有依赖项，如系统和编程依赖项，以及配置设置 1.4 Rootless和Rootful容器&emsp;&emsp;在容器主机上，特权用户运行的容器称为Rootful容器、非特权用户运行的容器称为Rootless容器 &emsp;&emsp;Rootless容器不允许使用通常为特权用户保留的系统资源，例如访问受限目录、在受限端口(1024以下的端口)上发布网络服务，此功能可防止潜在攻击者获取容器主机上的root特权 &emsp;&emsp;可使用root用户身份直接运行容器，但如果有漏洞允许攻击者破坏容器，这样做会削弱系统的安全性 1.5 设计基于容器的架构&emsp;&emsp;容器是重复利用托管应用并使其可以移植的有效方式 &emsp;&emsp;容器可以轻松地从一个环境迁移到另一个环境，如从开发环境迁移到生产环境 &emsp;&emsp;可以保存一个容器的多个版本，并根据需要快速访问每个版本 &emsp;&emsp;容器通常是临时的，可将运行中容器所生成的数据永久保存到持久存储中，但容器本身通常会在需要时运行，然后停止并被删除，下次需要该特定容器时，将启动新的容器进程 &emsp;&emsp;可以在单个容器中安装含有多个服务的复杂软件应用。例如，Web服务器可能需要使用数据库和消息传递系统。不过，将一个容器用于多个服务会难以管理 &emsp;&emsp;更好的设计是在单独的容器中运行每个组件、Web服务器、数据库和消息传递系统。这样，更新和维护单个应用组件不会影响其他组件或应用堆栈 2 容器镜像和注册表&emsp;&emsp;运行容器必须使用容器镜像： &emsp;&emsp;1.容器镜像是包含编码步骤的静态文件，充当创建容器的蓝图 &emsp;&emsp;2.容器镜像打包应用及其所有依赖项，如系统库、编程语言运行时和库以及其他配置设置 &emsp;&emsp;3.容器镜像根据规范构建，如开放容器项目(OCI)镜像格式规范。这些规范定义容器镜像的格式，以及镜像支持的容器主机操作系统和硬件架构的元数据&emsp;&emsp;4.容器注册表是用于存储和检索容器镜像的存储库。开发人员将容器镜像推送或上传到容器注册表中，可以从注册表中将这些容器镜像拉取或下载到本地系统，以用于运行容器。可使用包含第三方镜像的公共注册表，也可使用贵组织控制的私有注册表 &emsp;&emsp;5.容器镜像来源很重要。和任何其他软件包一样，必须知道是否可以信任容器镜像中的代码。对于是否及如何提供、评估和测试提交给它们的容器镜像，不同的注册表具有不同的策略 &emsp;&emsp;红帽通过两个主容器注册表分发认证容器镜像，可以使用红帽登录凭据来访问这两个注册表: &emsp;&emsp;1.utility.redhat.io: 适用于基于官方红帽产品的容器 &emsp;&emsp;2.utilityconnect.redhat,com:适用于基于第三方产品的容器 &emsp;&emsp;3.红帽容器目录(https://access.redhat.com/containers)提供了一个基于Web的界面，通过它可以搜索这些注册表中的认证内容 2.1 安装容器&emsp;&emsp;使用的镜像仓库浏览器访问为: https://utility 账号&amp;密码是:admin/redhat321 &emsp;&emsp;镜像对应的地址为:utility.lab.example.com 12345678910111213141516[kiosk@foundation0 ~]$ cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6### rht-vm-hosts file listing the entries to be appended to /etc/hosts172.25.250.254 bastion.lab.example.com bastion172.25.250.10 servera.lab.example.com servera172.25.250.11 serverb.lab.example.com serverb172.25.250.220 utility.lab.example.com utility172.25.250.9 workstation.lab.example.com workstation# 登录servera请使用ssh方式，不要使用su切换。[root@foundation0 ~]# ssh root@servera[root@servera ~]# ssh student@localhost[student@servera ~]$ sudo dnf -y install container-tools # 安装podman容器 [student@servera ~]$ podman --version 2.2 登录容器&emsp;&emsp;需要红帽开发人员账户才能从红帽注册表下载镜像。可以使用podman login命令对注册表进行身份验证。如果不向podman login命令提供注册表URL，它会向默认配置的注册表进行身份验证 1234567891011121314151617181920212223242526272829$ podman login --help# 登录方法一(交互)：$ podman login utility.lab.example.com Username: adminPassword: redhat321Login Succeeded!# 登录方法二(非交互)： #$ podman login utility.lab.example.com -u admin -p redhat321 # 生产环境中是有https验证的Login Succeeded![student@servera ~]$ podman login -u admin -p redhat321 utility.lab.example.comError: authenticating creds for &quot;utility.lab.example.com&quot;: pinging container registry utility.lab.example.com: Get &quot;https://utility.lab.example.com/v2/&quot;: tls: failed to verify certificate: x509: certificate is not valid for any names, but wanted to match utility.lab.example.com# 如果出现以上报错，是要求https验证，需要通过选项--tls-verify进行手动关闭$ podman login utility.lab.example.com -u admin -p redhat321 --tls-verify=false[student@servera ~]$ podman login --help[student@servera ~]$ podman login -u admin -p redhat321 utility.lab.example.com --tls-verify=falseLogin Succeeded!# 登录方法三(非交互):# 使用podman login命令的--username和--password-sdtin选项，指定用于登录注册表的用户和密码# --password-stdin选项从stdin读取密码# 红帽建议不要使用--password选项直接提供密码，因为此选项会将密码存储在日志文件中$ echo redhat321 | podman login -u admin --password-stdin utility.lab.example.com Login Succeeded! 2.3 验证容器的登录&emsp;&emsp;要验证是否已登录到某一注册表，请使用 podman login命令的–get-login选项 &emsp;&emsp;退出登录：podman logout 12345678$ podman login --get-login # 查看登录的用户admin[student@servera ~]$ podman login utility.lab.example.com --get-login # 指定仓库地址，查看登录用户admin[student@servera ~]$ podman logout utility.lab.example.com # 登出Removed login credentials for utility.lab.example.com 2.4 配置容器注册表&emsp;&emsp;容器注册表的默认配置文件是： /etc/containers/registries.conf 123456789101112131415161718192021222324252627282930313233343536[student@servera ~]$ sudo vim /etc/containers/registries.conf[sudo] password for student: student#第22行 指定可搜索的镜像仓库地址，如果使用完全合格域名，此处可以留空unqualified-search-registries = [&quot;utility.lab.example.com&quot;,&quot;registry.access.redhat.com&quot;,&quot;registry.redhat.io&quot;,&quot;docker.io&quot;][[utility]] #第24行 解除注释开启以下功能insecure = true #false/true 开启https安全验证/关闭安全验证blocked = false #第40行 需要过滤掉的镜像仓库地址location = &quot;utility.lab.example.com&quot; #第56行 指定容器注册表位置# $注意 ~/.config/containers/registries.conf目录设置会覆盖/etc/containers/registries.conf# 推荐：【student】$ mkdir -p ~/.config/containers$ cp /etc/containers/registries.conf ~/.config/containers/registries.conf$ vim ~/.config/containers/registries.confunqualified-search-registries = [&quot;utility.lab.example.com&quot;] [[utility]] insecure = true blocked = false location = &quot;utility.lab.example.com&quot; # 登录容器注册表[student@servera ~]$ podman login -u admin -p redhat321 utility.lab.example.comLogin Succeeded!# 根据仓库地址搜索镜像[student@servera ~]$ podman search utility.lab.example.com/# 如果只访问本地仓库，unqualified-search-registries = [&quot;utility.lab.example.com&quot;]默认即可，但要需要访问外网，需要用root用户修改vim /etc/resolv.conf文件内容添加nameserver 8.8.8.8 优先解析。# 阿里容器 i2kldsde.mirror.aliyuncs.com 2.5 容器文件构建容器镜像&emsp;&emsp;容器文件是一种文本文件，内含用于构建容器镜像的指令 &emsp;&emsp;容器文件通常具有定义其文件和目录所在路径或URL的上下文。生成的容器镜像由只读层组成，每一层代表容器文件中的一条指令 &emsp;&emsp;以下是一个容器文件示例，它使用utility.access.redhat.com注册表中的UBI镜像,安装python3 软件包，并将hello字符串打印到控制台 1234$ cat ContainerfileFROM utility.access.redhat.com/ubi8/ubi:latestRUN dnf install -y python3CMD[&quot;/bin/bash&quot;，&quot;-c&quot;，&quot;echo hello&quot;] 2.6 规模化容器管理&emsp;&emsp;新应用越来越多地使用容器来实施功能组件，这些容器提供应用的其他部分使用的服务 &emsp;&emsp;组织管理越来越多的容器，可能很快就会不堪重负 &emsp;&emsp;在生产中大规模部署容器需要一个能够应对以下挑战的环境： &emsp;&emsp;&emsp;1.平台必须确保提供必要服务的容器的可用性 &emsp;&emsp;&emsp;2.环境必须通过增加或减少运行中的容器实例，并对流量进行负载平衡，从而应对应用的使用高峰 &emsp;&emsp;&emsp;3.平台必须检测容器或主机的故障，并相应地作出反应 &emsp;&emsp;&emsp;4.开发人员可能需要自动工作流，以便透明、安全地向客户交付新的应用版本 &emsp;&emsp;Kubernetes是一项编排服务，可以使在容器主机集群中部署、管理和扩展基于容器的应用变得更加轻而易举 &emsp;&emsp;Kubernetes通过负载平衡器将流量重定向到容器，以便可以扩展提供服务的容器数量 &emsp;&emsp;Kubernetes支持用户定义的健康检查，以便监控您的容器，并在容器出现故障时将其重新启动 &emsp;&emsp;红帽提供了一个名为红帽OpenShift 的kubernetes发行版。OpenShift是基于Kubernetes基础架构构建的一组模块化组件和服务，为开发人员提供的额外功能包括基于Web的远程管理、多租户、监控与审计、高级安全功能、应用生命周期管理和自助服务实例等 3 部署容器3.1 Podman实用程序&emsp;&emsp;Podman是来自container-tools元数据包的全功能容器引警，用于管理开放容器计划(OCI)容器和镜像 &emsp;&emsp;podman实用程序的运作不使用守护进程，因此开发人员无需系统上的特权用户帐户来启动或停止容器 &emsp;&emsp;Podman提供多个子命令来与容器和镜像交互 &emsp;&emsp;Podman的命令如下表： 命令 描述 podman build 使用容器文件构建容器镜像 podman run 在新容器中运行命令 podman images 列出本地存储中的镜像 podman ps 打印有关容器的信息 podman inspect 显示容器、镜像、卷、网络或容器集的配置 podman pull 从注册表下载镜像 podman cp 在容器和本地文件系统之间复制文件或目录 podman exec 在运行中的容器内执行命令 podman rm 删除一个或多个容器 podman rmi 删除一个或多个本地存储的镜像 podman search 在注册表中搜索镜像 &emsp;&emsp;有关各个子命令使用帮助手册的更多信息，将子命令附加到podman命令，并用连字符将两者分隔。例如，podman-build帮助手册介绍了podman build子命令的用法 3.2 安装容器实用工具&emsp;&emsp;container-tools软件包包含与容器和容器镜像交互所需的实用程序 &emsp;&emsp;若要在系统上下载、运行和比较容器，使用dnf install命令来安装container-tools元软件包 &emsp;&emsp;使用dnf info命令查看container-tools软件包的版本和内容 123[student@servera ~]$ sudo dnf -y install container-tools # 安装podman容器 [student@servera ~]$ podman --version[student@servera ~]$ dnf info container-tools &emsp;&emsp;container-tools元数据包提供所需的podman和skope实用程序，用于完成分配的任务 3.3 从注册表下载容器镜像文件&emsp;&emsp;1.确保podman实用程序已配置为从utility.lab.example.com注册表搜索和下载容器 &emsp;&emsp;2.podman info命令显示podman实用程序的配置信息，包括其配置的注册表 1[student@servera ~]$ podman info &emsp;&emsp;podman search命令使用registries.conf文件中指定的注册表列表搜索匹配的名称镜像。默认情况下，Podman在所有非限定搜索注册表中执行搜索 &emsp;&emsp;使用podman search命令，显示包含python-38软件包的已配置注册表的镜像列表 1234567891011121314151617# 在注册表中搜索镜像[student@servera ~]$ podman search utility.lab.example.com/# 从注册表中下载镜像[student@servera ~]$ podman pull utility.lab.example.com/ubi7/ubi# 列出本地存储中的镜像[student@servera ~]$ podman imagesREPOSITORY TAG IMAGE ID CREATED SIZEutility.lab.example.com/ubi7/ubi latest 87dd8ec61bbc 4 years ago 215 MB# 镜像信息注解:1.REPOSITORY 仓库地址2.TAG 标记，latest最近版本3.IMAGE ID 镜像ID，ID号唯一，保证镜像唯一性4.CREATED 创建时间；5.SIZE 镜像大小 3.4 从容器文件创建容器镜像&emsp;&emsp;您获得了以下容器文件，用于在 python36-app目录中创建容器镜像 1234567891011$ cat Containerfile FROM utility.access.redhat.com/ubi8/ubi:latestRUN dnf install -y python36CMD[&quot;/bin/bash&quot;，&quot;-c&quot;，&quot;sleep infinity&quot;]# 此容器文件是教材中例子默认报错，可以使用下面的容器文件[student@servera ~]$ vim ContainerfileFROM utility.lab.example.com/ubi9/ubi:latestRUN echo -e '[rhel-9.3-for-x86_64-baseos-rpms]\\nbaseurl = http://content.example.com/rhel9.3/x86_64/dvd/BaseOS\\nenabled = true\\ngpgcheck = false\\nname = Red Hat Enterprise Linux 9.3 BaseOS (dvd)\\n[rhel-9.3-for-x86_64-appstream-rpms]\\nbaseurl = http://content.example.com/rhel9.3/x86_64/dvd/AppStream\\nenabled = true\\ngpgcheck = false\\nname = Red Hat Enterprise Linux 9.3 Appstream (dvd)'&gt;/etc/yum.repos.d/rhel_dvd.repoRUN yum install --disablerepo=* --enablerepo=rhel-9.3-for-x86_64-baseos-rpms --enablerepo=rhel-9.3-for-x86_64-appstream-rpms -y python3CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;] &emsp;&emsp;以上容器文件使用utility.lab.example.com/ubi9/ubi:latest镜像作为基础镜像。容器文件而后将安装python36软件包，并运行sleep infinity bash命令来防止容器退出 &emsp;&emsp;通常，容器运行一个进程，然后在该进程完成后退出。sleep infinity命令可防止容器退出因为该进程永远不会完成，然后可以在容器内进行测试、开发和调试 &emsp;&emsp;在检查容器文件后，可以使用podman build命令来构建镜像。podman build命令的语法如下所示： 1234567891011121314$ podman build -t NAME:TAG DIR[student@servera ~]$ podman build -t rhel7:2.0 .[student@servera ~]$ podman imagesREPOSITORY TAG IMAGE ID CREATED SIZElocalhost/rhel7 2.0 98d0b6385a00 40 seconds ago 238 MButility.lab.example.com/ubi9/ubi latest 8d2a8803cfca 12 months ago 219 MButility.lab.example.com/ubi7/ubi latest 87dd8ec61bbc 4 years ago 215 MButility.lab.example.com/rhel8/mariadb-103 latest 11a47e0fbed0 4 years ago 572 MB# 以上输出的最后一行显示了容器镜像ID。大多数Podman命令使用容器镜像ID的前12个字符来指代容器镜像，可以将此短ID或者容器或容器镜像的名称，作为大多数Podman命令的参数# 注解:-t,--tag name 生成镜像的名称NAME:新镜像的名称标签:新镜像的标签。如果未指定标签，则镜像自动标记为latestDIR:工作目录路径。容器文件必须位于工作目录中。如果工作目录是当前目录，则可以用点(.)来指定它。使用-f标志指定与当前目录不同的目录 &emsp;&emsp;使用podman inspect命令来查看容器镜像的低级别信息，并验证其内容是否符合容器要求: 1[student@servera ~]$ podman inspect localhost/rhel7:2.0 &emsp;&emsp;podman inspect命令的输出显示reqistry.access.redhat.com/ubi8/ubi:latest基础镜像、用于安装python36 软件包的dnf命令，以及在运行时执行以防止容器退出的sleep infinity bash命令 4 运行容器&emsp;&emsp;现在已拥有所需的容器镜像，可以使用它们来运行容器。容器可以处于以下状态之一! &emsp;&emsp;&emsp;1.Created：已创建好但尚未启动的容器 &emsp;&emsp;&emsp;2.运行中：与其进程一起运行的容器 &emsp;&emsp;&emsp;3.已停止：其进程已停止的容器 &emsp;&emsp;&emsp;4.Paused：其进程已暂停的容器，不支持 Rootless容器 &emsp;&emsp;&emsp;5.Deleted：其进程处于已死状态的容器 &emsp;&emsp;podman ps命令列出系统上正在运行的容器 &emsp;&emsp;使用podman ps -a来命令查看计算机中的所有容器 (已创建、已停止、已暂停或正在运行) &emsp;&emsp;可使用podman create命令来创建容器，以便稍后运行。若要创建容器，请使用容器localhost/rhel7:2.0镜像的ID。也可以使用–name选项设置名称来标识容器。此命令的输出是容器的长ID，如果不指定–name选项，会自动生成一个容器名称 123456789$ podman create --name python36 dd6ca291f097# 使用podman ps和podman ps -a命令来验证容器是否已创建但尚未启动$ podman ps -a$ podman ps # 运行podman start命令。可以使用名称或容器ID来启动容器。此命令的输出是容器的名称$ podman start python36$ podman ps 4.1 从远程存储库运行容器&emsp;&emsp;可使用podman run命令，在一个步骤中创建并运行容器。podman run命令在容器内运行进程，此进程将启动新容器 &emsp;&emsp;可使用podman run命令的-d选项以分离模式运行容器，这将在后台运行容器，而不是在会话的前台运行 &emsp;&emsp;在python36容器的示例中，您不需要提供容器运行所提的命令，原因是为该容器创建镜像的容器文件中已提供了sleep infinity 命令 1234567891011121314151617181920212223242526# podman run -t：终端 -i：交互 -d：放在后台 --name：指定容器的名称，如果不指定，会自动产生名称[student@servera ~]$ podman run -it utility.lab.example.com/ubi7/ubi[student@servera ~]$ podman psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES537b1f1fbb6d utility.lab.example.com/ubi7/ubi:latest /bin/bash About a minute ago Up About a minute objective_antonelli#实验前可以提前下载镜像至本地 #podman search utility.lab.example.com/#podman pull utility.lab.example.com/ubi8/ubi#podman images[student@servera ~]$ podman run -it --name rhel9 utility.lab.example.com/ubi9/ubi[student@servera ~]$ podman run -di --name rhel9-1 utility.lab.example.com/ubi9/ubi[student@servera ~]$ podman psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES537b1f1fbb6d utility.lab.example.com/ubi7/ubi:latest /bin/bash 5 minutes ago Up 5 minutes objective_antonellifb45a06e2271 utility.lab.example.com/ubi9/ubi:latest /bin/bash 56 seconds ago Up 57 seconds rhel9-1# ctrl+d退出后再查看容器的状态[student@servera ~]$ podman exec -ti rhel9-1 /bin/bash[student@servera ~]$ podman run -d --name rhel9-2 utility.lab.example.com/ubi9/ubi sleep infinity[student@servera ~]$ podman psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfb45a06e2271 utility.lab.example.com/ubi9/ubi:latest /bin/bash 5 minutes ago Up 5 minutes rhel9-1c4f7f4d72747 utility.lab.example.com/ubi9/ubi:latest sleep infinity 34 seconds ago Up 34 seconds rhel9-2 4.2 容器中的环境隔离&emsp;&emsp;每个容器都有自己的文件系统、网络和进程。查看ps命令的输出，并在主机和运行中容器之间进行比较，就会注意到隔离功能 &emsp;&emsp;在本地计算机上运行ps -ax命令，该命令将返回具有许多进程的预期结果 123456789[student@servera ~]$ ps -ax[student@servera ~]$ podman run -di --name python36-db utility.lab.example.com/rhel8/mariadb-103f4c3d26df7bd3614e6b4954ae6ed485046128afc89a95cba20c834b2ba0327ff[student@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES850618efbece utility.lab.example.com/ubi9/ubi:latest /bin/bash 9 hours ago Exited (0) 9 hours ago rhel9fb45a06e2271 utility.lab.example.com/ubi9/ubi:latest /bin/bash 9 hours ago Up 9 hours rhel9-1c4f7f4d72747 utility.lab.example.com/ubi9/ubi:latest sleep infinity 9 hours ago Up 9 hours rhel9-2f4c3d26df7bd utility.lab.example.com/rhel8/mariadb-103:latest run-mysqld 22 minutes ago Exited (1) 22 minutes ago python36-db 4.3 容器内执行命令&emsp;&emsp;podman exec命令可在运行中的容器内执行命令，该命令取容器的名称或ID作为第一个参数，并将下列参数作为要在容器内运行的命令 &emsp;&emsp;使用podman exec命令查看rhel7容器中正在运行的进程 &emsp;&emsp;ps aux命令的输出看起来有所不同，因为它运行与本地计算机不同的进程 &emsp;&emsp;使用sh -c命令来封装要在容器中执行的命令 &emsp;&emsp;ps ax &gt; /tmp/process-data.log命令被解释为要在容器中执行的命令。如果不封装命令，则Podman可能会将大于号字符(&gt;)解释为podman命令的一部分，而不是podman exec选项的参数 123456789101112131415161718192021222324[student@servera ~]$ podman exec rhel7 ps -ax PID TTY STAT TIME COMMAND 1 ? Ss 0:00 /bin/bash 2 ? R 0:00 ps -ax[student@servera ~]$ podman exec rhel7 sh -c 'ps -a &gt; /tmp/process_data.log'[student@servera ~]$ podman exec rhel7 sh -c 'echo China &gt; /test.txt'[student@servera ~]$ podman exec -ti rhel7 /bin/bash[root@2d4b030f4141 /]# ls /bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys test.txt tmp usr var[root@2d4b030f4141 /]# cat test.txtChina[root@2d4b030f4141 /]# cat /tmp/process_data.log PID TTY TIME CMD[root@2d4b030f4141 /]# exitexit[student@servera ~]$ # 将主机系统上安装的python版本与容器上安装的python版本进行比较[student@servera ~]$ podman exec rhel9-1 python3 --versionPython 3.9.18[student@servera ~]$ python3 --versionPython 3.9.18[student@servera ~]$ podman exec python38 python3 --versionPython 3.9.18 4.4 容器中的文件系统隔离&emsp;&emsp;开发人员可以使用文件系统隔离功能，为不同版本的编程语言编写和测试应用，无需使用多个物理机或虚拟机 &emsp;&emsp;在终端上的/tmp目录中创建一个显示hello world的简单bash脚本 12345678[student@servera ~]$ echo &quot;echo Hello China!&quot; &gt; /tmp/hello.sh[student@servera tmp]$ cd /tmp;lltotal 8-rw-r--r--. 1 student student 18 Mar 3 21:24 hello.sh-rw-r--r--. 1 student student 30 Mar 3 21:01 process_data.logdrwx------. 3 root root 17 Mar 3 08:28 systemd-private-3f251aad6ce74edb86dddf89d56e8aed-chronyd.service-c0Zzhidrwx------. 3 root root 17 Mar 3 08:28 systemd-private-3f251aad6ce74edb86dddf89d56e8aed-dbus-broker.service-QaiP7Ndrwx------. 3 root root 17 Mar 3 08:28 systemd-private-3f251aad6ce74edb86dddf89d56e8aed-systemd-logind.service-iM6Zeq &emsp;&emsp;/tmp/hello.sh文件位于主机计算机上，而不存在于容器内的文件系统上。如果尝试使用podmanexec来执行脚本，则会出现错误，因为容器中不存在/tmp/hello.sh脚本 123456789101112[student@servera tmp]$ stat /tmp/hello.sh File: /tmp/hello.sh Size: 18 Blocks: 8 IO Block: 4096 regular fileDevice: fc04h/64516d Inode: 18159864 Links: 1Access: (0644/-rw-r--r--) Uid: ( 1000/ student) Gid: ( 1000/ student)Context: unconfined_u:object_r:user_tmp_t:s0Access: 2025-03-03 21:24:33.865950531 -0500Modify: 2025-03-03 21:24:33.865950531 -0500Change: 2025-03-03 21:24:33.865950531 -0500 Birth: 2025-03-03 21:24:33.865950531 -0500[student@servera tmp]$ podman exec rhel7 stat /tmp/hello.shstat: cannot stat '/tmp/hello.sh': No such file or directory &emsp;&emsp;podman cp命令在主机和容器文件系统之间复制文件和文件夹。可以使用podman cp 命令将/tmp/hello.sh文件复制到python38容器: 12345678910[student@servera ~]$ podman cp /tmp/hello.sh rhel7:/tmp/hello.sh[student@servera ~]$ podman exec rhel7 stat /tmp/hello.sh File: '/tmp/hello.sh' Size: 18 Blocks: 8 IO Block: 4096 regular fileDevice: 58h/88d Inode: 10261231 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2025-03-04 02:24:34.000000000 +0000Modify: 2025-03-04 02:24:34.000000000 +0000Change: 2025-03-04 02:28:24.720471184 +0000 Birth: - &emsp;&emsp;脚本复制到容器文件系统后，即可从容器内执行: 12[student@servera ~]$ podman exec rhel7 bash /tmp/hello.shHello China! 4.5 删除容器和镜像&emsp;&emsp;使用podman rm和podman rmi命令删除容器和镜像 &emsp;&emsp;删除容器镜像之前，必须先从该镜像移除任何现有的运行中容器 12345[student@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES850618efbece utility.lab.example.com/ubi9/ubi:latest /bin/bash 10 hours ago Exited (0) 10 hours ago rhel9student@servera ~]$ podman rmi utility.lab.example.com/ubi9/ubi:latestError: image used by 8e7870d50daa32c768c4301911364285a659a5383b4cdadc96b8d3b7ff411c2d: image is in use by a container: consider listing external containers and force-removing image &emsp;&emsp;必须先停止容器，然后才能删除它。若要停止容器，请使用podman stop命令 12[student@servera ~]$ podman stop python38python38 &emsp;&emsp;停止容器后，使用podman rm命令来删除容器 123[student@servera ~]$ podman rm --help[student@servera ~]$ podman rm python38python38 &emsp;&emsp;当容器不再存在时，可使用podman rmi命令删除对应的镜像: 12345678910111213[student@servera ~]$ podman imagesREPOSITORY TAG IMAGE ID CREATED SIZElocalhost/rhel7 2.0 98d0b6385a00 11 hours ago 238 MButility.lab.example.com/ubi9/ubi latest 8d2a8803cfca 12 months ago 219 MButility.lab.example.com/ubi7/ubi latest 87dd8ec61bbc 4 years ago 215 MButility.lab.example.com/rhel8/mariadb-103 latest 11a47e0fbed0 4 years ago 572 MB[student@servera ~]$ podman rmi 8d2a8803cfcaError: image used by c4f7f4d727471d590f6241cccf0be0b1ef2256cc43a710594642611fe6d0be47: image is in use by a container: consider listing external containers and force-removing image[student@servera ~]$ podman rmi 98d0b6385a00Untagged: localhost/rhel7:2.0Deleted: 98d0b6385a005e09cfcee59a393cfce2fc46b56f09af6c4f87bd874f00966ed2Deleted: 53c739e51f226903b6568038c9cf563de2007f756e0a0e86e5c00604cf474f3dDeleted: 92b83aa1157f23b209f53480c6bbdf780c39490b37337bc4f4fcb1061b7c978 5 容器存储和网络资源5.1 管理容器资源&emsp;&emsp;可以使用容器来运行简单的进程，然后退出。还可以配置容器以连续运行某一服务，如数据库服务器。如果持续运行服务，最终可能需要向容器添加更多资源，如持久存储或对其他网络的访问权限 &emsp;&emsp;可以使用不同的策略为容器配置持久存储: &emsp;&emsp;&emsp;1.对于红帽OpenShift等企业容器平台上的大型部署，可以使用复杂的存储解决方案为容器提供存储，而无需了解底层基础架构 &emsp;&emsp;&emsp;2.对于单个容器主机上且无需扩展的小型部署，可以通过在运行中的容器上创建要挂载的目录，从容器主机创建持久存储 &emsp;&emsp;当Web服务器或数据库服务器等容器为容器主机外部的客户端提供内容时，必须为这些客户端设置通信通道，以访问容器的内容 &emsp;&emsp;可以配置端口映射，以启用与容器的通信。通过端口映射，目的地为容器主机上端口的请求将被转发到容器内的端口 5.2 容器的环境变量&emsp;&emsp;容器镜像允许在创建时传递环境变量以自定义容器 &emsp;&emsp;可以使用环境变量为容器设置参数，以根据您的环境进行定制，无需创建自己的自定义镜像。通常不会修改容器镜像，因为这会向镜像添加层，或许更加难以维护 &emsp;&emsp;使用podman run -d –name db01 utility.lab.example.com/rhel8/mariadb-103命令运行容器化数据库，但发现容器无法启动 123456789101112131415[student@servera ~]$ podman search utility.lab.example.com/ [student@servera ~]$ podman pull utility.lab.example.com/rhel8/mariadb-103[student@servera ~]$ podman imagesREPOSITORY TAG IMAGE ID CREATED SIZEutility.lab.example.com/rhel8/mariadb-103 latest 11a47e0fbed0 4 years ago 572 MB[student@servera ~]$ podman run -d --name db01 utility.lab.example.com/rhel8/mariadb-10329decc6e48d62506e62e503a383943709138a8f789a32dd27d2fa1761bf3ea9f# 发现容器无法启动[student@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES29decc6e48d6 utility.lab.example.com/rhel8/mariadb-103:latest run-mysqld 2 minutes ago Exited (1) 2 minutes ago db01 &emsp;&emsp;使用podman container logs命令调查容器状态的原因 123456789101112131415[student@servera ~]$ podman container logs db01Warning: Can't detect memory limit from cgroupsWarning: Can't detect number of CPU cores from cgroupsWarning: Can't detect memory limit from cgroupsWarning: Can't detect number of CPU cores from cgroups=&gt; sourcing 20-validate-variables.sh ...You must either specify the following environment variables: MYSQL_USER (regex: '^[a-zA-Z0-9_]+$') MYSQL_PASSWORD (regex: '^[a-zA-Z0-9_~!@#$%^&amp;*()-=&lt;&gt;,.?;:|]+$') MYSQL_DATABASE (regex: '^[a-zA-Z0-9_]+$')Or the following environment variable: MYSQL_ROOT_PASSWORD (regex: '^[a-zA-Z0-9_~!@#$%^&amp;*()-=&lt;&gt;,.?;:|]+$')Or both.Optional Settings:...... &emsp;&emsp;容器信息展示 123456789101112# 输出中的usage 标签提供了如何运行镜像的示例。url标签指向红帽容器目录中的一个Web页面，其中记录了环境变量以及有关如何使用容器镜像的其他信息。# 此镜像的文档显示容器将3306端口用于数据库服务。文档中还显示了以下环境变量可用于配置数据库服务:[student@servera ~]$ podman imagesREPOSITORY TAG IMAGE ID CREATED SIZEutility.lab.example.com/ubi9/ubi latest 8d2a8803cfca 12 months ago 219 MButility.lab.example.com/ubi7/ubi latest 87dd8ec61bbc 4 years ago 215 MButility.lab.example.com/rhel8/mariadb-103 latest 11a47e0fbed0 4 years ago 572 MB[student@servera ~]$ podman inspect utility.lab.example.com/rhel8/mariadb-103 | grep usage &quot;usage&quot;: &quot;podman run -d -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db -p 3306:3306 rhel8/mariadb-103&quot;, &quot;usage&quot;: &quot;podman run -d -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db -p 3306:3306 rhel8/mariadb-103&quot;,$ skopeo inspect docker://utility.lab.example.com/rhel8/mariadb-105 | grep -B 1 Usage &emsp;&emsp;mariadb镜像的环境变量 变量 描述 MYSQL_USER 要创建的MySQL帐户的用户名 MYSQL_PASSWORD 用户帐户的密码 MYSQL_DATABASE 数据库名称 MYSQL_ROOT_PASSWORD root用户的密码 (可选) &emsp;&emsp;在检查了镜像的可用环境变量后，使用podman run命令-e选项将环境变量传递给容器，并使用podman ps命令来验证它是否正在运行 12345678910111213[student@servera ~]$ podman rm -af29decc6e48d62506e62e503a383943709138a8f789a32dd27d2fa1761bf3ea9f[student@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES[student@servera ~]$ podman run -d --name db01 -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db utility.lab.example.com/rhel8/mariadb-10341a62ff2efd7f268e52d6f6a0a9b503411824c5f4391b86dae5d3f01376cb896# 容器启动成功[student@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES41a62ff2efd7 utility.lab.example.com/rhel8/mariadb-103:latest run-mysqld 8 seconds ago Up 8 seconds db01 5.3 容器持久存储&emsp;&emsp;默认情况下运行容器时，所有内容都使用基于容器的镜像 &emsp;&emsp;鉴于容器镜像的寿命短，用户或应用写入的所有新数据在移除容器后都会丢失 &emsp;&emsp;若要持久保留数据，可以将容器中的主机文件系统内容与–volume(-v)选项搭配使用。在容器中使用此卷类型时，必须考虑文件系统级别的权限&emsp;&emsp;MariaDB容器镜像中，mysql用户必须拥有/var/lib/mysql目录，就如同MariaDB在主机上运行时一样 &emsp;&emsp;打算挂载到容器中的目录必须具有mysql作为用户和组所有者(或mysql用户的UID/GID，如果主机上没有安装MariaDB) &emsp;&emsp;如果以root用户身份运行容器，则主机上的UID和GID与容器内的UID和GID匹配 &emsp;&emsp;可使用podman unshare命令在用户命名空间内运行命令。要获取用户命名空间的UID映射，请使用podman unshare cat命令 123456[student@servera ~]$ podman unshare cat /proc/self/uid_map 0 1000 1 1 100000 65536[student@servera ~]$ podman unshare cat /proc/self/gid_map 0 1000 1 1 100000 65536 &emsp;&emsp;以上输出显示： &emsp;&emsp;&emsp;1.容器中的root用户 (UID和GID为0)映射到主机计算机上的用户(UID和GID为1000) &emsp;&emsp;&emsp;2.容器中的UID和GID1映射到主机计算机上的UID和GID 100000 &emsp;&emsp;&emsp;3.1后的每个UID和GID以1增量递增。例如，容器内的UID和GID30映射到主机计算机上的UID和GID100029 &emsp;&emsp;&emsp;4.可使用podman exec命令查看使用临时存储运行的容器内的mysql用户UID和GID: 12[student@servera ~]$ podman exec -it db01 grep mysql /etc/passwdmysql:x:27:27:MySQL Server:/var/lib/mysql:/sbin/nologin &emsp;&emsp;将/home/user/db_data目录挂载到db01容器中，以在容器的/var/lib/mysql目录中提供持久存储 &emsp;&emsp;创建/home/user/db_data目录，并使用podmanunshare命令将27的用户命名空间UID和GID设置为该目录的所有者 12345678910[student@servera ~]$ mkdir /home/student/db_data[student@servera ~]$ ll -d /home/student/db_datadrwxr-xr-x. 2 student student 6 Mar 4 00:43 /home/student/db_data[student@servera ~]$ ll -d -n /home/student/db_datadrwxr-xr-x. 2 1000 1000 6 Mar 4 00:43 /home/student/db_data[student@servera ~]$ podman unshare chown 27:27 /home/student/db_data/[student@servera ~]$ ll -d /home/student/db_datadrwxr-xr-x. 2 100026 100026 6 Mar 4 00:43 /home/student/db_data &emsp;&emsp;容器中的UID和GID 27映射到主机计算机上的UID和GID100026 &emsp;&emsp;可使用ll -d命令查看/home/student/db_data目录的所有权来验证映射 &emsp;&emsp;现在已设置了正确的文件系统级权限，可使用podman run 命令-v选项来挂载目录: 123456789101112131415161718[student@servera ~]$ podman rm -af41a62ff2efd7f268e52d6f6a0a9b503411824c5f4391b86dae5d3f01376cb896[student@servera ~]$ podman run -d --name db01 -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db -v /home/student/db_data/:/var/lib/mysql utility.lab.example.com/rhel8/mariadb-10354279d1fff7f3b679dd9d4efb7bfa28a626f41f7d07bea50a445ec2392c8cb02# db01容器未在运行[student@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES54279d1fff7f utility.lab.example.com/rhel8/mariadb-103:latest run-mysqld 6 minutes ago Exited (1) 6 minutes ago db01$ podman run -d --name db01 \\-e MYSQL_USER=user \\-e MYSQL_PASSWORD=pass \\-e MYSQL_DATABASE=db \\-e MYSQL_ROOT_PASSWORD=redhat \\-v /home/student/db_data/:/var/lib/mysql \\utility.lab.example.com/rhel8/mariadb-103 &emsp;&emsp;podman container logs命令显示/var/lib/mysql/data目录的权限错误： 12345678910111213141516171819[student@servera ~]$ podman container logs db01Warning: Can't detect memory limit from cgroupsWarning: Can't detect number of CPU cores from cgroupsWarning: Can't detect memory limit from cgroupsWarning: Can't detect number of CPU cores from cgroups=&gt; sourcing 20-validate-variables.sh ...=&gt; sourcing 25-validate-replication-variables.sh ...=&gt; sourcing 30-base-config.sh ...---&gt; 05:58:29 Processing basic MySQL configuration files ...=&gt; sourcing 60-replication-config.sh ...=&gt; sourcing 70-s2i-config.sh ...---&gt; 05:58:29 Processing additional arbitrary MySQL configuration provided by s2i ...=&gt; sourcing 40-paas.cnf ...=&gt; sourcing 50-my-tuning.cnf ...---&gt; 05:58:29 Initializing database ...---&gt; 05:58:29 Running mysql_install_db ...mkdir: cannot create directory '/var/lib/mysql/data': Permission deniedFatal error Can't create database directory '/var/lib/mysql/data'# 发生此错误的原因是，主机上/home/user/db\\_data目录中设置的SELinux上下文不正确 5.4 容器存储的SELinux上下文&emsp;&emsp;必须先设置container_file_t SELinux上下文类型，然后才能将该目录作为持久存储挂载到容器。如果目录没有container_file_t SELinux 上下文，则容器无法访问该目录 &emsp;&emsp;可以将Z选项附加到podman run命令-v选项的参数，以自动设置目录的SELinux上下文 &emsp;&emsp;/home/student/db_data挂载为/var/lib/mysql目录的持久存储时，可使用podman run -v /home/student/db_data:/var/lib/mysql:Z命令设置该目录的SELinux上下文 123456789101112[student@servera ~]$ podman run -d --name db01 -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db -v /home/student/db_data/:/var/lib/mysql:Z utility.lab.example.com/rhel8/mariadb-1036195fc399b1f6ae8f5b9d3f436c02bb5d6b093b597949dc968b2a64f6e7d023c[student@servera ~]$ ll -dZ /home/student/db_data/drwxr-xr-x. 3 100026 100026 system_u:object_r:container_file_t:s0:c428,c988 36 Mar 4 01:26 /home/student/db_data/$ podman run -d --name db01 \\-e MYSQL_USER=user \\-e MYSQL_PASSWORD=pass \\-e MYSQL_DATABASE=db \\-e MYSQL_ROOT_PASSWORD=redhat \\-v /home/student/db_data:/var/lib/mysql:Z utility.lab.example.com/rhel8/mariadb-103 5.5 分配端口映射到容器&emsp;&emsp;要提供对容器的网络访问权限，客户端必须连接到容器主机上的端口，这些端口将网络流量传递到容器中的端口 &emsp;&emsp;将容器主机上的网络端口映射到容器中的端口时，容器将接收发送到主机网络端口的网络流量 &emsp;&emsp;例如，可以将容器主机上的13306端口映射到容器上的3306端口，以便与MariaDB容器通信。因此，发送到容器主机端口13306的流量将由容器中运行的MariaDB接收 &emsp;&emsp;可以使用 podman run命令的-p选项设置从容器主机上13306端口到db01容器上3306端口的端口映射 123456789101112131415[student@servera ~]$ podman run -d --name db01 -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db -v /home/student/db_data/:/var/lib/mysql:Z -p 13306:3306 utility.lab.example.com/rhel8/mariadb-10328202973f22d60ea4ccd096b8a321b30c7d57156a2c41d2502c1a0456c794ced[student@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES28202973f22d utility.lab.example.com/rhel8/mariadb-103:latest run-mysqld 20 seconds ago Up 20 seconds 0.0.0.0:13306-&gt;3306/tcp db01$ podman run -d --name db01 \\-e MYSQL_USER=user \\-e MYSQL_PASSWORD=pass \\-e MYSQL_DATABASE=db \\-e MYSQL_ROOT_PASSWORD=redhat \\-p 13306:3306 \\-v /home/student/db_data:/var/lib/mysql:Z \\utility.lab.example.com/rhel8/mariadb-103 &emsp;&emsp;使用podman port命令的-a选项可显示正在使用的所有容器端口映射。还可以使用podman port db01命令显示 db01容器的映射端口 1234[student@servera ~]$ podman port -a28202973f22d 3306/tcp -&gt; 0.0.0.0:13306[student@servera ~]$ podman port db013306/tcp -&gt; 0.0.0.0:13306 &emsp;&emsp;可使用firewall-cmd命令允许端口13306流量传入容器主机，以便它可以重定向到容器: 1234567891011121314151617181920212223# Rootless(特权)容器无法打开主机上特权端口1024有 以下的端口。比如-p 80:8000 ，比必须使用root才可以对其进行调整[student@servera ~]$ sudo firewall-cmd --permanent --add-port=13306/tcp[sudo] password for student: success[student@servera ~]$ sudo firewall-cmd --reloadsuccess[student@servera ~]$ sudo firewall-cmd --list-allpublic (active) target: default icmp-block-inversion: no interfaces: eth0 sources: services: cockpit dhcpv6-client ssh ports: 13306/tcp protocols: forward: yes masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: # firewall-cmd --add-port=13306/tcp --permanent# firewall-cmd --reload 5.6 容器的网络后端&emsp;&emsp;Podmanv4.0支持两种容器网络后端，即Netavark和CNI &emsp;&emsp;自RHEL9起，系统默认使用Netavark。若要验证所用的网络后端，请运行以下podman info命令 &emsp;&emsp;将网络堆栈从 CNI 切换到 Netavark | Red Hat Product Documentation 12$ podman info --format {{.Host.NetworkBackend}}netavark &emsp;&emsp;主机上使用默认Podman网络的现有容器无法解析彼此的主机名，因为默认网络上未启用DNS &emsp;&emsp;使用podman network create命令创建一个支持DNS的网络： &emsp;&emsp;&emsp;使用podman network create命令创建名为db_net的网络，并将子网指定为10.87.0.0/16，网关指定为10.87.0.1 123$ podman network create --gateway 10.87.0.1 --subnet 10.87.0.0/16 db_net$ podman network ls # 列出容器网络 &emsp;&emsp;如果不指定–gateway 或–subnet 选项，则会使用默认值创建它们 &emsp;&emsp;podman network inspect 命令显示关于特定网络的信息： &emsp;&emsp;&emsp;可使用podman network inspect 命令验证网关和子网的设置是否正确，以及新的dbnet网络是否启用了DNS 1$ podman network inspect db_net &emsp;&emsp;podman run命令–network选项将启用DNS的db_net网络添加到新容器： &emsp;&emsp;&emsp;可使用podman run命令–network选项创建连接到db_net网络的db01和client01容器 123456789101112131415161718$ podman run -d --name db01 \\--network db_net \\-e MYSQL_USER=user \\-e MYSQL_PASSWORD=pass \\-e MYSQL_DATABASE=db \\-e MYSQL_ROOT_PASSWORD=redhat \\-v /home/student/db_data:/var/lib/mysql:Z \\-p 13306:3306 \\utility.lab.example.com/rhel8/mariadb-105$ podman run -d --name client01 \\--network db_net \\-v /etc/yum.repos.d:/etc/yum.repos.d/ \\utility.lab.example.com/ubi9-beta/ubi \\sleep infinity$ podman ps -a &emsp;&emsp;由于容器设计为仅具有所需的最少软件包，因此容器可能不具有测试通信所需的实用程序，如ping和ip命令。可以使用podman exec 命令在容器中安装这些实用程序 12$ podman exec client01 dnf install -y iputils iproute..output omitted.. &emsp;&emsp;容器现在可以通过容器名称互相ping &emsp;&emsp;可以使用podman exec命令来测试DNS解析，名称解析到为db_net网络手动设置的子网内的IP 123456$ podman exec client01 ping -c4 db01PING db01.dns.podman (10.87.0.2) 56(84) bytes of data.64 bytes from 10.87.0.2 (10.87.0.2): icmp_seq=1 ttl=64 time=1.08 ms64 bytes from 10.87.0.2 (10.87.0.2): icmp_seq=2 ttl=64 time=0.082 ms64 bytes from 10.87.0.2 (10.87.0.2): icmp_seq=3 ttl=64 time=0.063 ms64 bytes from 10.87.0.2 (10.87.0.2): icmp_seq=4 ttl=64 time=0.070 ms &emsp;&emsp;可以使用 podman exec命令验证每个容器中的IP地址是否与DNS解析匹配 12$ podman exec client01 ip a | grep 10.8 inet 10.87.0.3/16 brd 10.87.255.255 scope global eth0 5.7 多个网络连接到单个容器&emsp;&emsp;多个网络可以同时连接到一个容器，以帮助分隔不同类型的流量 &emsp;&emsp;可以使用podman network create命令创建backend网络 1$ podman network create backend &emsp;&emsp;使用podman network ls 命令查看所有Podman网络 12345$ podman network lsNETWORK ID NAME DRIVER a7fea510a6d1 backend bridgefe680efc5276 db01 bridge2f259bab93aa podman bridge &emsp;&emsp;没有通过podman network create命令的–gateway和–subnet选项指定子网和网关，使用podman network inspect命令获取backend网络的IP信息 1$ podman network inspect backend &emsp;&emsp;在容器运行时，可以使用podman network connect命令将其他网络连接到容器 &emsp;&emsp;可以使用podman network connect命令，将backend网络连接到db01和client01容器 12$ podman network connect backend db01$ podman network connect backend client01 &emsp;&emsp;可以使用podman inspect命令验证两个网络是否都已连接到各个容器并显示IP信息 1234$ podman inspect db01$ podman inspect db01 | grep -A 34 Networks &gt; db01$ cat db01 #查看db01的两个网络IP为10.89.0.2 ， 10.87.0.2 &emsp;&emsp;client01容器现在可以与两个网络上的db01容器通信，可以使用podman exec命令从cliento1容器pingdb01容器上的两个网络 123456$ podman exec -ti client01 ping -c4 10.89.0.2PING 10.89.0.2 (10.89.0.2) 56(84) bytes of data.64 bytes from 10.89.0.2: icmp_seq=1 ttl=64 time=0.352 ms$ podman exec -ti client01 ping -c4 10.87.0.2PING 10.87.0.2 (10.87.0.2) 56(84) bytes of data.64 bytes from 10.87.0.2: icmp_seq=1 ttl=64 time=0.594 ms &emsp;&emsp;容器内安装mariadb客户端访问容器数据库 123$ podman exec client01 dnf -y install mariadbpodman$ podman exec -ti client01 mysql -u user -ppass -h db01 6 系统服务管理容器6.1 systemd管理小型容器&emsp;&emsp;可以运行容器来完成系统任务、获取一系列命令的输出，可能还希望运行无限期运行服务的容器，如Web服务器或数据库 &emsp;&emsp;在传统环境中，特权用户通常将这些服务配置为在系统启动时运行，并使用systemctl命令进行管理 &emsp;&emsp;普通用户可以创建systemd单元来配置您的Rootless容器。利用此配置，可以通过systemctl命令将容器作为常规系统服务进行管理 &emsp;&emsp;基于systemd单元管理容器主要用于不需要扩展的基本和小型部署 &emsp;&emsp;对于许多基于容器的应用和服务的更复杂扩展和编排，可以使用基于Kubernetes的企业编排平台，如红帽OpenShift容器平台 6.2 systemd用户服务要求&emsp;&emsp;普通用户可以使用systemctl命令来启用服务，该服务在打开会话(图形界面、文本控制台或SSH)时启动，并在关闭最后一个会话时停止。 此行为与系统服务有所不同，系统服务是在系统启动时启动并在系统关机时停止 &emsp;&emsp;默认情况下，当使用useradd命令创建用户帐户时，系统将使用普通用户ID范围中的下一个可用ID &emsp;&emsp;系统在/etc/subuid文件中为用户的容器保留一系列ID &emsp;&emsp;如果使用useradd命令–system选项创建用户帐户，系统不会为用户容器保留范围。因此，无法使用系统帐户启动Rootless容器 &emsp;&emsp;创建一个专门的用户帐户来管理容器，使用useradd命令创建appdev-adm用户，并将redhat用作密码 12345678[student@servera ~]$ sudo useradd appdev-adm[sudo] password for student:student[student@servera ~]$ sudo passwd appdev-admChanging password for user appdev-adm.New password:BAD PASSWORD: The password is shorter than 8 charactersRetype new password:passwd: all authentication tokens updated successfully. &emsp;&emsp;使用su命令切换到appdev-adm用户，并使用podman命令来启动 1234567[student@servera ~]$ su appdev-admPassword:redhat[appdev-adm@servera student]$ podman infoERRO[0000] XDG_RUNTIME_DIR directory &quot;/run/user/1000&quot; is not owned by the current user[root@servera ~]# su appdev-adm[appdev-adm@servera root]$ podman infoERRO[0000] XDG_RUNTIME_DIR directory &quot;/run/user/0&quot; is not owned by the current user &emsp;&emsp;Podman是一款无状态实用程序，需要完整的登录会话 &emsp;&emsp;Podman必须在SSH会话中使用，不能在sudo或su shell中使用。因此，将退出su shell，并通过SSH登录计算机 &emsp;&emsp;无状态应用： &emsp;&emsp;&emsp;Stateless Application指不会在会话中保存下次会话中去要的客户端数据，每个会话都像首次执行一样,不会依赖之前的数据进行响应 1234567891011[appdev-adm@servera root]$ exitexit[root@servera ~]# ssh appdev-adm@localhostappdev-adm@localhost's password:Register this system with Red Hat Insights: insights-client --registerCreate an account or view all your systems at https://red.ht/insights-dashboardLast login: Tue Mar 4 09:06:27 2025[appdev-adm@servera ~]$ podman info[appdev-adm@servera ~]$ podman login -u admin -p redhat321 utility.lab.example.comLogin Succeeded![appdev-adm@servera ~]$ podman search utility.lab.example.com/ &emsp;&emsp;配置容器注册表并使用您的凭据进行身份验证，可以使用以下命令运行http容器 12345678910[appdev-adm@servera ~]$ mkdir /home/appdev-adm/nginx_web/[appdev-adm@servera ~]$ echo nginx_web_page &gt; /home/appdev-adm/nginx_web/index.html[appdev-adm@servera ~]$ cat /home/appdev-adm/nginx_web/index.htmlnginx_web_page[appdev-adm@servera ~]$ podman run -d --name nginx -v /home/appdev-adm/nginx_web/:/usr/share/nginx/html/:Z -p 8080:80 utility.lab.example.com/library/nginx[appdev-adm@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6f72fe3fc6d7 utility.lab.example.com/library/nginx:latest nginx -g daemon o... 2 minutes ago Up 2 minutes 0.0.0.0:8080-&gt;80/tcp nginx[appdev-adm@servera ~]$ curl localhost:8080nginx_web_page 6.2.1 创建systemd用户文件&emsp;&emsp;在~/.confiq/svstemd/user/目录中手动定义systemd服务 &emsp;&emsp;用户服务的文件语法与系统服务文件的相同，详细信息可查看systemd.unit(5)和systemd.service(5)man手册 &emsp;&emsp;podman generate systemd命令为现有容器生成systemd服务文件，podman generate systemd命令使用容器作为模型创建配置文件 &emsp;&emsp;podman generate systemd命令的–new选项指示podman实用程序对systemd服务进行配置，以便在该服务启动时创建容器并在该服务停止时删除容器 &emsp;&emsp;使用podman generate systemd命令和–name选项来显示为nginx容器建模的systemd服务文件 1234567891011121314151617181920212223242526272829303132[appdev-adm@servera ~]$ man systemd.unit | grep config.*user ~/.config/systemd/user.control/* ~/.config/systemd/user/* │~/.config/systemd/user.control │ using the dbus API ($XDG_CONFIG_HOME is used if │ │$HOME/.config/systemd/user │ set, ~/.config otherwise) │[appdev-adm@servera ~]$ man systemd.unit[appdev-adm@servera ~]$ mkdir -p ~/.config/systemd/user/[appdev-adm@servera ~]$ cd ~/.config/systemd/user/[appdev-adm@servera ~]$ podman stop nginx # 生成单元文件之前先停止容器nginx[appdev-adm@servera user]$ podman generate systemd -n nginx -f/home/appdev-adm/.config/systemd/user/container-nginx.service[appdev-adm@servera user]$ lscontainer-nginx.service[appdev-adm@servera user]$ systemctl --user enable --now container-nginx.serviceCreated symlink /home/appdev-adm/.config/systemd/user/default.target.wants/container-nginx.service → /home/appdev-adm/.config/systemd/user/container-nginx.service.[appdev-adm@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6f72fe3fc6d7 utility.lab.example.com/library/nginx:latest nginx -g daemon o... 26 minutes ago Up About a minute 0.0.0.0:8080-&gt;80/tcp nginx[appdev-adm@servera user]$ systemctl --user stop container-nginx.service[appdev-adm@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6f72fe3fc6d7 utility.lab.example.com/library/nginx:latest nginx -g daemon o... 27 minutes ago Exited (0) 16 seconds ago 0.0.0.0:8080-&gt;80/tcp nginx[appdev-adm@servera user]$ systemctl --user restart container-nginx.service$ podman generate systemd -n nginxExecStart=/usr/bin/podman start nginxExecStop=/usr/bin/podman stop -t 10 nginx*$ podman generate systemd -n nginx -f$ lscontainer-nginx.service &emsp;&emsp;启动时，systemd守护进程执行podman start命令来启动现有容器 &emsp;&emsp;停止时，systemd守护进程执行podman stop命令来停止容器，systemd守护进程不会删除该容器 &emsp;&emsp;使用上一命令并加上–new选项来比较systemd配置 1234567891011$ podman generate systemd -n nginx --newExecStartPre=/bin/rm -f %t/%n.ctr-idExecStart=/usr/bin/podman run --cidfile=%t/%n.ctr-id --cgroups=no-conmon --rm --sdnotify=conmon --replace -d --name nginx -p 8080:80 -v /home/appdev-adm/nginx_web/:/usr/share/nginx/html/:Z utility.lab.example.com/library/nginxExecStop=/usr/bin/podman stop --ignore --cidfile=%t/%n.ctr-idExecStopPost=/usr/bin/podman rm -f --ignore --cidfile=%t/%n.ctr-id#通过--new选项创建用户单元文件的方法，仅供参考$ cd ~/.config/systemd/user/$ podman generate systemd -n nginx --new -f/home/appdev-adm/.config/systemd/user/container-nginx.service &emsp;&emsp;启动时，systemd守护进程执行podman run命令创建并启动新容器。此操作使用podman run命令的–rm选项，将在停止时删除容器 &emsp;&emsp;停止时，systemd 执行podman stop命令以停止容器 &emsp;&emsp;在systemd停止容器后，systemd将使用podman rm -f命令将其移除 &emsp;&emsp;验证podman generate systemd命令的输出，并使用–files选项运行上一命令，以在当前目录中创建systemd用户文件。由于nginx容器使用持久存储，因此选择使用带有–new选项的podman generate systemd命令。然后创建~/config/systemd/user/目录并将文件移到此位置上 6.2.2 管理systemd用户文件&emsp;&emsp;现在已创建了systemd用户文件，可以使用systemctl命令的–user选项来管理nginx容器 &emsp;&emsp;首先，重新加载systemd守护进程，使systemctl命令知道新的用户文件： &emsp;&emsp;&emsp;使用systemctl–user start命令启动nginx容器 &emsp;&emsp;&emsp;使用为容器生成的systemd用户文件的名称 123456$ systemctl --user enable --now container-nginx.service#其他的管理方法，仅供参考$ systemctl --user status container-nginx.service$ systemctl --user stop container-nginx.service#建议重启验证容器是否可以 开机自启动 &emsp;&emsp;systemd 系统和用户服务之间使用的不同目录和命令： 12345678910111213141.存储自定义单元文件 系统服务 /etc/systemd/system/unit.service 用户服务 \\~/.config/systemd/user/unit.service 2.重新加载单元文件 系统服务 \\# systemctl daemon-reload 用户服务 \\$ systemctl --user daemon-reload 3.启动和停止服务 系统服务 \\# systemctl start UNIT \\# systemctl stop UNIT 用户服务 \\$ systemctl --user start UNIT \\$ systemctl --user stop UNIT 4.在计算机启动时启动服务 系统服务 \\# systemctl enable UNIT 用户服务 \\$ loginctl enable-linger \\$ systemctl --user enable UNIT 6.2.3 配置为系统引导时启动&emsp;&emsp;此时systemd服务配置已就绪，可以为给定的用户运行容器。但是，如果用户从系统注销，systemd服务会在特定时间后停止容器。出现此行为的原因是，systemd服务单元是使用.user选项创建的，它在用户登录时启动服务，并在用户注销时停止服务 &emsp;&emsp;可以通过运行loginctl enable-linger命令来更改此默认行为，并强制已启用的服务在服务器启动时启动，并在服务器关闭期间停止。使用loginctl命令将systemd用户服务配置为在所配置服务的最后一个用户会话关闭后保留 &emsp;&emsp;使用 loginctl show-user 命令验证配置是否成功。 123456789101112[appdev-adm@servera ~]$ loginctl show-user appdev-admLinger=no[appdev-adm@servera ~]$ loginctl enable-linger[appdev-adm@servera ~]$ loginctl show-user appdev-admLinger=yes# 重启后进行测试[kiosk@foundation0 ~]$ ssh root@servera[root@servera ~]# ssh appdev-adm@localhost[appdev-adm@servera ~]$ podman ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6f72fe3fc6d7 utility.lab.example.com/library/nginx:latest nginx -g daemon o... 40 minutes ago Up About a minute 0.0.0.0:8080-&gt;80/tcp nginx 6.2.4 Root使用Systemd管理容器&emsp;&emsp;将容器配置为以root身份运行，并使用systemd服务文件进行管理的优势是可以将这些服务文件配置为像常见systemd单元文件那样工作，而不是以特定用户身份来运行 &emsp;&emsp;将服务文件设置为root的过程与前面概述的Rootless容器过程类似，但以下例外: &emsp;&emsp;&emsp;1.不要创建专门的用户来管理容器 &emsp;&emsp;&emsp;2.服务文件必须在/etc/systemd/system目录中，而不是在~/config/systemd/user目录中 &emsp;&emsp;&emsp;3.使用systemctl命令管理容器，但不使用–user选项，不要以root用户身份运行loginctl enable-linger命令","link":"/2025/04/22/%E5%AE%B9%E5%99%A8/podman%E5%AE%B9%E5%99%A8/"},{"title":"CKA笔记","text":"1 准备DNS解析 1234567# 这一步需要在所有机器上完成cat &gt;&gt; /etc/hosts &lt;&lt;EOF192.168.8.3 k8s-master192.168.8.4 k8s-worker1192.168.8.5 k8s-worker2192.168.30.133 registry.xiaohui.cnEOF 2 Docker CE 部署2.1 添加Docker仓库&emsp;&emsp;为了节约网络流量和时间，这一步只在k8s-master这一台机器上完成 &emsp;&emsp;如需练习worker节点加入到k8s集群的操作，在k8s-master上初始化好k8s集群后，再来其他节点完成这个步骤 12345678910111213141516# 安装依赖sudo apt-get updatesudo apt-get install -y ca-certificates curl gnupg lsb-release# 添加公钥到系统sudo mkdir -p /etc/apt/keyringscurl -fsSL https://mirrors.nju.edu.cn/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg# 添加仓库到系统echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.nju.edu.cn/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null# 判断仓库是否已做好sudo apt-get updateroot@k8s-master:~# apt updateHit:1 https://mirrors.nju.edu.cn/docker-ce/linux/ubuntu noble InRelease... 2.2 安装Docker CE&emsp;&emsp;为了节约网络流量和时间，这一步只在k8s-master这一台机器上完成 &emsp;&emsp;如需练习worker节点加入到k8s集群的操作，在k8s-master上初始化好k8s集群后，再来其他节点完成这个步骤 12sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin# 部署完Docker CE之后，还需要cri-docker shim才可以和Kubernetes集成 2.3 CRI-Docker 部署&emsp;&emsp;为了节约网络流量和时间，这一步只在k8s-master这一台机器上完成 &emsp;&emsp;如需练习worker节点加入到k8s集群的操作，在k8s-master上初始化好k8s集群后，再来其他节点完成这个步骤 12345# 下载cri-dockerwget http://hub.gitmirror.com/https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.17/cri-dockerd_0.3.17.3-0.ubuntu-jammy_amd64.deb# 安装cri-dockerdpkg -i cri-dockerd_0.3.17.3-0.ubuntu-jammy_amd64.deb 2.3.1 将镜像指引到国内1234567root@k8s-master:~# cp /lib/systemd/system/cri-docker.service /etc/systemd/system/cri-docker.serviceroot@k8s-master:~# sed -i 's/ExecStart=.*/ExecStart=\\/usr\\/bin\\/cri-dockerd --container-runtime-endpoint fd:\\/\\/ --network-plugin=cni --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com\\/google_containers\\/pause:3.10/' /etc/systemd/system/cri-docker.serviceroot@k8s-master:~# systemctl daemon-reloadroot@k8s-master:~# systemctl restart cri-docker.serviceroot@k8s-master:~# systemctl enable cri-docker.service 3 Containerd 部署3.1 安装Containerd12wget https://github.com/containerd/nerdctl/releases/download/v1.7.7/nerdctl-full-1.7.7-linux-amd64.tar.gztar Cxzvvf /usr/local nerdctl-full-1.7.7-linux-amd64.tar.gz 3.2 生成配置文件12345678mkdir /etc/containerdcontainerd config default &gt; /etc/containerd/config.toml#使用systemdsed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml#沙盒镜像改为国内sed -i 's|sandbox_image = &quot;registry.k8s.io/pause:3.8&quot;|sandbox_image = &quot;registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10&quot;|' /etc/containerd/config.toml#添加加速器地址sed -i '/\\[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry\\]/{n;s|config_path = &quot;&quot;|config_path = &quot;/etc/containerd/certs.d&quot;|}' /etc/containerd/config.toml 3.3 使用镜像加速器123456mkdir /etc/containerd/certs.d/docker.io -pcat &gt; /etc/containerd/certs.d/docker.io/hosts.toml &lt;&lt;-'EOF'server = &quot;https://xxx.xxx.xxx&quot;[host.&quot;https://xxx.xxx.xxx&quot;] capabilities = [&quot;pull&quot;, &quot;resolve&quot;, &quot;push&quot;]EOF 3.4 启动Containerd服务123systemctl daemon-reloadsystemctl enable --now containerdsystemctl enable --now buildkit 3.5 nerdctl命令自动补齐12nerdctl completion bash &gt; /etc/bash_completion.d/nerdctlsource /etc/bash_completion.d/nerdctl 4 创建第一个容器4.1 运行容器12345678910docker run -d -p 8000:80 --name container1 class-docker.myk8s.cn/library/nginxdocker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeea8ed66990c class-docker.myk8s.cn/library/nginx:latest &quot;/docker-entrypoint.…&quot; 7 seconds ago Up 0.0.0.0:8000-&gt;80/tcp container1 # 如果用的是containerd，运行容器的命令就是下面这样的nerdctl run -d -p 8000:80 --name container1 class-docker.myk8s.cn/library/nginxnerdctl psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1353d09a9df3 class-docker.myk8s.cn/library/nginx:latest &quot;/docker-entrypoint.…&quot; 21 seconds ago Up 0.0.0.0:8000-&gt;80/tcp container1 &emsp;&emsp;参数解释： &emsp;&emsp;&emsp;-d 是指后台运行 &emsp;&emsp;&emsp;-p 是端口映射，此处是将宿主机的8000端口和容器内的80端口映射到一起 &emsp;&emsp;&emsp;–name 是指容器的名字 &emsp;&emsp;&emsp;nginx 是指本次使用的镜像名字 4.2 进入容器123456789docker exec -it container1 /bin/bashroot@eea8ed66990c:/# echo hello lixiaohui &gt; /usr/share/nginx/html/index.htmlroot@eea8ed66990c:/# exit# 如果用的是containerd，进入容器的命令就是下面这样的# exec -it 是指通过交互式进入terminalnerdctl exec -it container1 /bin/bashroot@1353d09a9df3:/# echo hello lixiaohui &gt; /usr/share/nginx/html/index.htmlroot@1353d09a9df3:/# exit 4.3 访问容器内容12curl http://127.0.0.1:8000hello lixiaohui 5 镜像相关操作5.1 Commit 构建1.将上述实验中的container1容器生成一个新的镜像：nginx:v1 12docker commit container1 nginx:v1docker images 2.如果用的是containerd，commit方法构建容器镜像的命令就是下面这样的 12nerdctl commit container1 nginx:v1nerdctl images 3.输出 123REPOSITORY TAG IMAGE ID CREATED PLATFORM SIZE BLOB SIZEnginx latest 0d17b565c37b 13 minutes ago linux/amd64 149.1 MiB 54.1 MiBnginx v1 edc2905109d8 5 seconds ago linux/amd64 149.2 MiB 54.1 MiB 5.2 使用Commit镜像1.使用nginx:v1镜像在本机的3000端口提供一个名为lixiaohuicommit的容器 1234docker run -d -p 3000:80 --name lixiaohuicommit nginx:v1curl http://127.0.0.1:3000hello lixiaohui 2.如果用的是containerd，使用容器镜像的命令就是下面这样的 1234nerdctl run -d -p 3000:80 --name lixiaohuicommit nginx:v1curl http://127.0.0.1:3000hello lixiaohui 5.3 Dockerfile构建1234567cat &gt; dockerfile &lt;&lt;EOFFROM class-docker.myk8s.cn/library/httpdMAINTAINER 939958092@qq.comRUN echo hello lixiaohui dockerfile container &gt; /usr/local/apache2/htdocs/index.htmlEXPOSE 80WORKDIR /usr/local/apache2/htdocs/EOF 12docker build -t httpd:v1 -f dockerfile .docker images 如果用的是containerd，dockerfile方式构建容器镜像的命令就是下面这样的 12nerdctl build -t httpd:v1 -f dockerfile .nerdctl images 1234REPOSITORY TAG IMAGE ID CREATED PLATFORM SIZE BLOB SIZEhttpd v1 494736083f8f About a minute ago linux/amd64 150.2 MiB 53.8 MiBnginx latest 2d17cc4981bf 4 minutes ago linux/amd64 149.1 MiB 54.1 MiBnginx v1 fc81b1ce4076 3 minutes ago linux/amd64 149.2 MiB 54.1 MiB &emsp;&emsp;docker build -t httpd:v1 -f dockerfile .&emsp;&emsp;不管是docker还是nerdctl，这个命令后面还有一个英文的句号.是指当前目录 5.4 使用Dockerfile镜像1.用httpd:v1的镜像在本机4000端口上提供一个名为lixiaohuidockerfile的容器 12docker run -d -p 4000:80 --name lixiaohuidockerfile httpd:v1docker ps 2.如果用的是containerd，dockerfile方式构建容器镜像的使用命令就是下面这样的 12nerdctl run -d -p 4000:80 --name lixiaohuidockerfile httpd:v1nerdctl ps 1234CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES534323e724a7 docker.io/library/nginx:latest &quot;/docker-entrypoint.…&quot; 5 minutes ago Up 0.0.0.0:8000-&gt;80/tcp container1 7ee887b78a75 docker.io/library/httpd:v1 &quot;httpd-foreground&quot; 3 seconds ago Up 0.0.0.0:4000-&gt;80/tcp lixiaohuidockerfile a41ef87ba51f docker.io/library/nginx:v1 &quot;/docker-entrypoint.…&quot; 3 minutes ago Up 0.0.0.0:3000-&gt;80/tcp lixiaohuicommit 1curl http://127.0.0.1:4000 1hello lixiaohui dockerfile container 5.5 删除容器1docker rm -f container1 lixiaohuidockerfile lixiaohuicommit 6 构建私有仓库&emsp;&emsp;构建私有仓库请使用另外一台单独的机器，将IP设置为192.168.30.133，并确保在本文档最开始的地方在所有节点之间执行了添加/etc/hosts文件操作 6.1 生成root证书信息1234openssl genrsa -out /etc/ssl/private/selfsignroot.key 4096openssl req -x509 -new -nodes -sha512 -days 3650 -subj &quot;/C=CN/ST=Shanghai/L=Shanghai/O=Company/OU=SH/CN=Root&quot; \\-key /etc/ssl/private/selfsignroot.key \\-out /usr/local/share/ca-certificates/selfsignroot.crt 6.2 生成服务器私钥以及证书请求文件12345openssl genrsa -out /etc/ssl/private/registry.key 4096openssl req -sha512 -new \\-subj &quot;/C=CN/ST=Shanghai/L=Shanghai/O=Company/OU=SH/CN=xiaohui.cn&quot; \\-key /etc/ssl/private/registry.key \\-out registry.csr 6.3 生成openssl cnf扩展文件12345678910111213cat &gt; certs.cnf &lt;&lt; EOF[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = registry.xiaohui.cnEOF 6.4 签发证书123456openssl x509 -req -in registry.csr \\-CA /usr/local/share/ca-certificates/selfsignroot.crt \\-CAkey /etc/ssl/private/selfsignroot.key -CAcreateserial \\-out /etc/ssl/certs/registry.crt \\-days 3650 -extensions v3_req -extfile certs.cnf 6.5 信任根证书1update-ca-certificates 6.6 部署Harbor仓库6.6.1 部署Docker CE123456789101112131415161718sudo apt-get updatesudo apt-get install -y \\ ca-certificates \\ curl \\ gnupg \\ lsb-releasesudo mkdir -p /etc/apt/keyringscurl -fsSL https://mirror.nju.edu.cn/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpgecho \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirror.nju.edu.cn/docker-ce/linux/ubuntu \\ $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get updatesudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin 6.6.2 添加Docker镜像加速器12345678# 只限在国内部署时才需要加速，在国外这样加速反而缓慢sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ &quot;registry-mirrors&quot;: [&quot;https://xxx.xxx.xxx&quot;]}EOF 6.6.3 Compose支持12345# 添加Compose支持，并启动Docker服务curl -L &quot;http://hub.gitmirror.com/https://github.com/docker/compose/releases/download/v2.34.0/docker-compose-linux-x86_64&quot; -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composesudo systemctl daemon-reloadsudo systemctl restart docker 1234wget http://hub.gitmirror.com/https://github.com/goharbor/harbor/releases/download/v2.12.2/harbor-offline-installer-v2.12.2.tgztar xf harbor-offline-installer-v2.12.2.tgz -C /usr/local/bincd /usr/local/bin/harbordocker load -i harbor.v2.12.2.tar.gz 6.6.4 修改harbor.yml123456# 在harbor.yml中，修改以下参数，定义了网址、证书、密码vim harbor.yml# 修改hostname为registry.xiaohui.cn# 修改https处的certificate为/etc/ssl/certs/registry.crt# 修改https处的private_key为/etc/ssl/private/registry.key# 修改harbor_admin_password为admin 12./prepare./install.sh 6.7 生成服务文件1234567891011121314151617cat &gt; /etc/systemd/system/harbor.service &lt;&lt;EOF[Unit]Description=HarborAfter=docker.service systemd-networkd.service systemd-resolved.serviceRequires=docker.serviceDocumentation=http://github.com/vmware/harbor[Service]Type=simpleRestart=on-failureRestartSec=5ExecStart=/usr/local/bin/docker-compose -f /usr/local/bin/harbor/docker-compose.yml upExecStop=/usr/local/bin/docker-compose -f /usr/local/bin/harbor/docker-compose.yml down[Install]WantedBy=multi-user.targetEOFsudo systemctl daemon-reloadsystemctl enable harbor --now 6.8 上传镜像到本地仓库&emsp;&emsp;在所有的机器上，将registry.xiaohui.cn以及其对应的IP添加到/etc/hosts，然后将上述实验中的httpd:v1镜像，改名为带上IP:PORT形式，尝试上传我们的镜像到本地仓库 123docker login registry.xiaohui.cndocker tag httpd:v1 registry.xiaohui.cn/library/httpd:v1docker push registry.xiaohui.cn/library/httpd:v1 7 Kubernetes部署7.1 关闭swap分区&emsp;&emsp;为了节约网络流量和时间，这一步只在k8s-master这一台机器上完成 &emsp;&emsp;如需练习worker节点加入到k8s集群的操作，在k8s-master上初始化好k8s集群后，再来其他节点完成这个步骤 1234# 实时关闭swapoff -a# 永久关闭sed -i 's/.*swap.*/#&amp;/' /etc/fstab 7.2 允许iptables检查桥接流量&emsp;&emsp;为了节约网络流量和时间，这一步只在k8s-master这一台机器上完成 &emsp;&emsp;如需练习worker节点加入到k8s集群的操作，在k8s-master上初始化好k8s集群后，再来其他节点完成这个步骤 1234567891011root@k8s-master:~# cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confbr_netfilterEOFmodprobe br_netfiltercat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOFroot@k8s-master:~# sudo sysctl --system 7.3 安装kubeadm&emsp;&emsp;为了节约网络流量和时间，这一步只在k8s-master这一台机器上完成 &emsp;&emsp;如需练习worker节点加入到k8s集群的操作，在k8s-master上初始化好k8s集群后，再来其他节点完成这个步骤 12345678910111213141516171819# 安装依赖root@k8s-master:~# apt-get update &amp;&amp; apt-get install -y apt-transport-https curl# 安装K8S软件包仓库-阿里云cat &gt; /etc/apt/sources.list.d/k8s.list &lt;&lt;EOFdeb https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/deb /EOF# 安装软件包仓库的公钥curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/deb/Release.key | apt-key add -# 更新软件包的仓库索引apt-get update# 开始安装apt-get install -y kubelet kubeadm kubectl# 操作系统所有软件包升级时将忽略kubelet、kubeadm、kubectlapt-mark hold kubelet kubeadm kubectl 7.4 添加命令自动补齐1234root@k8s-master:~# kubectl completion bash &gt; /etc/bash_completion.d/kubectlroot@k8s-master:~# kubeadm completion bash &gt; /etc/bash_completion.d/kubeadmroot@k8s-master:~# source /etc/bash_completion.d/kubectlroot@k8s-master:~# source /etc/bash_completion.d/kubeadm 7.5 集成CRI-Docker&emsp;&emsp;为了节约网络流量和时间，这一步只在k8s-master这一台机器上完成 &emsp;&emsp;如需练习worker节点加入到k8s集群的操作，在k8s-master上初始化好k8s集群后，再来其他节点完成这个步骤 1234567# 保证K8S和docker进行通信root@k8s-master:~# crictl config runtime-endpoint unix:///run/cri-dockerd.sockWARN[0000] Config &quot;/etc/crictl.yaml&quot; does not exist, trying next: &quot;/usr/bin/crictl.yaml&quot;INFO[0000] No --get, --set or --list provided, setting key &quot;runtime-endpoint&quot; to value &quot;unix:///run/cri-dockerd.sock&quot;root@k8s-master:~# crictl imagesIMAGE TAG IMAGE ID SIZE 7.6 集成containerd12crictl config runtime-endpoint unix:///run/containerd/containerd.sockcrictl images 7.7 集群部署&emsp;&emsp;kubeadm.yaml中name字段必须在网络中可被解析，也可以将解析记录添加到集群中所有机器的/etc/hosts中 &emsp;&emsp;初始化集群部署的操作只能在k8s-master上执行 7.7.1 初始化配置修改123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 初始化配置kubeadm config print init-defaults &gt; kubeadm.yamlsed -i 's/.*advert.*/ advertiseAddress: 192.168.8.3/g' kubeadm.yamlsed -i 's/.*name.*/ name: k8s-master/g' kubeadm.yamlsed -i 's|imageRepo.*|imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers|g' kubeadm.yamlsed -i &quot;/^\\\\s*networking:/a\\\\ podSubnet: 172.16.0.0/16&quot; kubeadm.yaml# 注意下面的替换，只有在集成的是CRI-Docker时才需要执行，Containerd不需要sed -i 's/ criSocket.*/ criSocket: unix:\\/\\/\\/run\\/cri-dockerd.sock/' kubeadm.yaml# 确认配置root@k8s-master:~# vim kubeadm.yamlapiVersion: kubeadm.k8s.io/v1beta4bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.8.3 bindPort: 6443nodeRegistration: criSocket: unix:///run/cri-dockerd.sock imagePullPolicy: IfNotPresent imagePullSerial: true name: k8s-master taints: nulltimeouts: controlPlaneComponentHealthCheck: 4m0s discovery: 5m0s etcdAPICall: 2m0s kubeletHealthCheck: 4m0s kubernetesAPICall: 1m0s tlsBootstrap: 5m0s upgradeManifests: 5m0s---apiServer: {}apiVersion: kubeadm.k8s.io/v1beta4caCertificateValidityPeriod: 87600h0m0scertificateValidityPeriod: 8760h0m0scertificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: {}dns: {}encryptionAlgorithm: RSA-2048etcd: local: dataDir: /var/lib/etcdimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: 1.32.0networking: podSubnet: 172.16.0.0/16 dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12proxy: {}scheduler: {} 7.7.2 集群初始化1234567891011121314151617181920212223242526# 模块加载root@k8s-master:~# modprobe br_netfilter # 集群初始化root@k8s-master:~# kubeadm init --config kubeadm.yaml...Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.8.3:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:0c3038d57ed67ce8da15c8f192f775a9489b340e95cd7f1fec89ab38ef7bef14 7.7.3 授权管理权限123root@k8s-master:~# mkdir -p $HOME/.kuberoot@k8s-master:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configroot@k8s-master:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config 7.7.4 查看集群状态123root@k8s-master:~# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master NotReady control-plane 30m v1.32.4 7.8 部署Calico网络插件&emsp;&emsp;Calico网络插件部署的操作只能在k8s-master上执行 7.8.1 安装calico组件1234561.使用operator安装calico组件# 以下为github的地址，可能会失败root@k8s-master:~# kubectl create -f https://raw.gitmirror.com/projectcalico/calico/refs/tags/v3.29.3/manifests/tigera-operator.yaml# 使用下面的地址执行kubectl create -f https://www.linuxcenter.cn/files/cka/cka-yaml/tigera-operator-calico-3.29.3.yaml 7.8.2 设置calico在集群的网段1234562.使用下面的自定义资源设置一下calico在集群中的网段# 以下为github的地址，可能会失败root@k8s-master:~# wget https://raw.gitmirror.com/projectcalico/calico/refs/tags/v3.29.3/manifests/custom-resources.yaml# 使用下面的地址执行wget https://www.linuxcenter.cn/files/cka/cka-yaml/custom-resources-calico-3.29.3.yaml 7.8.3 确认资源的地址12345678910111213141516171819202122232425262728293031root@k8s-master:~# vim custom-resources.yamlapiVersion: operator.tigera.io/v1kind: Installationspec: calicoNetwork: ipPools: - name: default-ipv4-ippool cidr: 192.168.0.0/16 #换成上面规定好的172.16.0.0/16 root@k8s-master:~# vim custom-resources-calico-3.29.3.yaml...spec: # Configures Calico networking. calicoNetwork: ipPools: - name: default-ipv4-ippool blockSize: 26 cidr: 172.16.0.0/16 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all()---# This section configures the Calico API server.# For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServerapiVersion: operator.tigera.io/v1kind: APIServermetadata: name: defaultspec: {} 7.8.4 自定义资源发布到集群12root@k8s-master:~# kubectl apply -f custom-resources-calico-3.29.3.yamlkubectl apply -f custom-resources.yaml 7.8.5查询集群组件状态123456# 查询集群组件是否工作正常，正常应该都处于runningkubectl get pod -A# 解决工作中pod一直处于pending的方法# 对于某个podkubectl get pod calico-apiserver-644d4cf9f4-vkxj6 -n calico-system -o yaml | grep image: 7.9 加入Worker节点&emsp;&emsp;加入节点操作需在所有的worker节点完成，这里要注意，Worker节点需要完成以下先决条件才能执行kubeadm join &emsp;&emsp;&emsp;1.Docker、CRI-Docker 部署 &emsp;&emsp;&emsp;2.Swap分区关闭 &emsp;&emsp;&emsp;3.iptables桥接流量的允许 &emsp;&emsp;&emsp;4.安装kubeadm等软件 &emsp;&emsp;&emsp;5.集成CRI-Docker &emsp;&emsp;&emsp;6.所有节点的/etc/hosts中互相添加对方的解析 &emsp;&emsp;如果时间长忘记了join参数，可以在master节点上用以下方法重新生成 1kubeadm token create --print-join-command &emsp;&emsp;如果有多个CRI对象，在worker节点上执行以下命令加入节点时，指定CRI对象，案例如下： 123kubeadm join 192.168.8.3:6443 --token m0uywc.81wx2xlrzzfe4he0 \\--discovery-token-ca-cert-hash sha256:5a24296d9c8f5ace4dede7ed46ee2ecf5ed51c0877e5c1650fe2204c09458274 \\--cri-socket=unix:///var/run/cri-dockerd.sock &emsp;&emsp;注意上描述命令最后的–cri-socket参数，在系统中部署了docker和cri-docker时，必须明确指明此参数，并将此参数指向我们的cri-docker，不然命令会报告有两个重复的CRI的错误 &emsp;&emsp;在k8s-master机器上执行以下内容给节点打上角色标签，k8s-worker1 k8s-worker2打上了worker标签 12kubectl label nodes k8s-worker1 k8s-worker2 node-role.kubernetes.io/worker=kubectl get nodes 7.10 重置集群&emsp;&emsp;如果在安装好集群的情况下，想重复练习初始化集群，或者包括初始化集群报错在内的任何原因，想重新初始化集群时，可以用下面的方法重置集群，重置后，集群就会被删除，可以用于重新部署，一般来说，这个命令仅用于k8s-master这个节点 12345678910111213141516171819root@k8s-master:~# kubeadm reset --cri-socket=unix:///var/run/cri-dockerd.sock...[reset] Are you sure you want to proceed? [y/N]: y...The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.dThe reset process does not reset or clean up iptables rules or IPVS tables.If you wish to reset iptables, you must do so manually by using the &quot;iptables&quot; command.If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)to reset your system's IPVS tables.The reset process does not clean your kubeconfig files and you must remove them manually.Please, check the contents of the $HOME/.kube/config file.# 根据提示，手工完成文件和规则的清理 清理后就可以重新部署集群了root@k8s-master:~# rm -rf /etc/cni/net.droot@k8s-master:~# iptables -Froot@k8s-master:~# rm -rf $HOME/.kube/config 8 Namespace8.1 命令行创建12kubectl create namespace lixiaohuikubectl get namespaces 8.2 YAML文件创建123456789cat &gt; namespace.yml &lt;&lt;EOFapiVersion: v1kind: Namespacemetadata: name: zhangsanEOFkubectl create -f namespace.yml kubectl get namespaces 创建带有namespace属性的资源 12kubectl run nginx --image=class-docker.myk8s.cn/library/nginx --namespace=lixiaohuikubectl get pod -n lixiaohui 每次查询和创建资源都需要带–namespace=lixiaohui挺麻烦，可以设置默认值 123kubectl config set-context --current --namespace=lixiaohuikubectl config view | grep namespace:kubectl get pod 删除namespace会删除其下所有资源，但是如果要删除已经切换为默认值的namespace时，可能会卡住，所以我们要先把默认值切换为其他，然后再删除 12kubectl config set-context --current --namespace=defaultkubectl delete namespaces lixiaohui zhangsan CRD自定义资源CRD 介绍K8S资源类型不止有namespace，还有很多，不过那都是系统自带的，现在我们来看看怎么自定义k8s中的资源 什么是 CRD？ CRD（Custom Resource Definition） 是 Kubernetes 提供的一种机制，允许用户定义自己的资源类型。这些自定义资源可以像 Kubernetes 原生资源（如 Pod、Service、Deployment 等）一样被管理。 为什么需要 CRD？ 扩展 Kubernetes API：Kubernetes 的原生资源可能无法满足所有用户的需求。CRD 允许用户定义自己的资源类型，从而扩展 Kubernetes 的功能。 管理复杂应用：有些应用可能需要管理一些特定的资源，这些资源不属于 Kubernetes 原生支持的范围。通过 CRD，你可以将这些资源纳入 Kubernetes 的管理范围，实现统一的资源管理。 CRD 的作用 定义资源结构：CRD 允许你定义资源的结构，包括其字段和数据类型。 管理资源生命周期：Kubernetes 将为你管理这些自定义资源的生命周期，包括创建、更新、删除等操作。 集成 Kubernetes 生态系统：CRD 可以与 Kubernetes 的其他组件（如控制器、操作符等）集成，实现更复杂的业务逻辑。 在 Kubernetes 的自定义资源定义（CRD）中，CRD 本身只定义了资源的结构和 API，但它不会直接执行任何创建、更新或删除操作。这些操作需要通过一个控制器（Controller）来实现。控制器是一个独立的程序，它监听 CRD 的变化，并根据这些变化执行实际的操作。 查询CRD以及API资源先看看系统中的api资源都有哪些，然后我们自己来创建一个 12345root@k8s-master:~# kubectl api-resourcesNAME SHORTNAMES APIVERSION NAMESPACED KINDbindings v1 true Bindingcomponentstatuses cs v1 false ComponentStatus... 再来看看现在都有哪些自定义资源 123456789101112131415161718192021222324252627root@k8s-master:~# kubectl get crdNAME CREATED ATadminnetworkpolicies.policy.networking.k8s.io 2025-03-08T05:07:24Zapiservers.operator.tigera.io 2025-03-08T05:07:24Zbgpconfigurations.crd.projectcalico.org 2025-03-08T05:07:23Zbgpfilters.crd.projectcalico.org 2025-03-08T05:07:23Zbgppeers.crd.projectcalico.org 2025-03-08T05:07:23Zblockaffinities.crd.projectcalico.org 2025-03-08T05:07:23Zcaliconodestatuses.crd.projectcalico.org 2025-03-08T05:07:23Zclusterinformations.crd.projectcalico.org 2025-03-08T05:07:23Zcrontabs.stable.example.com 2025-04-10T02:51:46Zfelixconfigurations.crd.projectcalico.org 2025-03-08T05:07:23Zglobalnetworkpolicies.crd.projectcalico.org 2025-03-08T05:07:23Zglobalnetworksets.crd.projectcalico.org 2025-03-08T05:07:24Zhostendpoints.crd.projectcalico.org 2025-03-08T05:07:24Zimagesets.operator.tigera.io 2025-03-08T05:07:24Zinstallations.operator.tigera.io 2025-03-08T05:07:24Zipamblocks.crd.projectcalico.org 2025-03-08T05:07:24Zipamconfigs.crd.projectcalico.org 2025-03-08T05:07:24Zipamhandles.crd.projectcalico.org 2025-03-08T05:07:24Zippools.crd.projectcalico.org 2025-03-08T05:07:24Zipreservations.crd.projectcalico.org 2025-03-08T05:07:24Zkubecontrollersconfigurations.crd.projectcalico.org 2025-03-08T05:07:24Znetworkpolicies.crd.projectcalico.org 2025-03-08T05:07:24Znetworksets.crd.projectcalico.org 2025-03-08T05:07:24Ztiers.crd.projectcalico.org 2025-03-08T05:07:24Ztigerastatuses.operator.tigera.io 2025-03-08T05:07:24Z 创建CRD以及API资源ok，我们来创建一个自己的crd，crd将注册为api资源 123456789101112131415161718192021222324252627282930313233343536373839404142cat &gt; crd.yaml &lt;&lt;-'EOF'apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: # 名字必需与下面的 spec 字段匹配，并且格式为 '&lt;名称的复数形式&gt;.&lt;组名&gt;' name: crontabs.stable.example.comspec: # 组名称，用于 REST API：/apis/&lt;组&gt;/&lt;版本&gt; group: stable.example.com # 列举此 CustomResourceDefinition 所支持的版本 versions: - name: v1 # 每个版本都可以通过 served 标志来独立启用或禁止 served: true # 其中一个且只有一个版本必需被标记为存储版本 storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer # 可以是 Namespaced 或 Cluster scope: Namespaced names: # 名称的复数形式，用于 URL：/apis/&lt;组&gt;/&lt;版本&gt;/&lt;名称的复数形式&gt; plural: crontabs # 名称的单数形式，作为命令行使用时和显示时的别名 singular: crontab # kind 通常是单数形式的驼峰命名（CamelCased）形式。你的资源清单会使用这一形式。 kind: CronTab # shortNames 允许你在命令行使用较短的字符串来匹配资源 shortNames: - ctEOF 12root@k8s-master:~# kubectl apply -f crd.yamlcustomresourcedefinition.apiextensions.k8s.io/crontabs.stable.example.com created 此时再看就会有我们自己的crd资源和api资源了 1234root@k8s-master:~# kubectl get crdNAME CREATED AT...crontabs.stable.example.com 2025-03-10T03:07:46Z 1234root@k8s-master:~# kubectl api-resources...NAME SHORTNAMES APIVERSION NAMESPACED KINDcrontabs ct stable.example.com/v1 true CronTab 查询API资源结构与参数我们既然已经注册为api资源，来看看能否explain字段？ 12345678910111213root@k8s-master:~# kubectl explain crontabsGROUP: stable.example.comKIND: CronTabVERSION: v1DESCRIPTION: &lt;empty&gt;FIELDS: apiVersion &lt;string&gt; kind &lt;string&gt; metadata &lt;ObjectMeta&gt; spec &lt;Object&gt;... 看看都有哪些spec 12345678910111213141516171819root@k8s-master:~# kubectl explain crontabs.specGROUP: stable.example.comKIND: CronTabVERSION: v1FIELD: spec &lt;Object&gt;DESCRIPTION: &lt;empty&gt;FIELDS: cronSpec &lt;string&gt; &lt;no description&gt; image &lt;string&gt; &lt;no description&gt; replicas &lt;integer&gt; &lt;no description&gt; 一切正常，看来我们已经创建了自定义资源，接下来就是等开发人员通过编程等方式创建operator等控制器，来使用我们的资源了 PodPod创建一个Pod中只有一个业务容器 12345678910111213cat &gt; pod.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: lixiaohuipodspec: containers: - name: hello image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo &quot;Hello, lixiaohui!&quot; &amp;&amp; sleep 3600'] restartPolicy: OnFailureEOF 123kubectl create -f pod.yml kubectl get podkubectl logs lixiaohuipod 一个Pod中有多个业务容器 12345678910111213141516171819cat &gt; multicontainer.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: podspec: containers: - name: hello image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo &quot;Hello, lixiaohui!&quot; &amp;&amp; sleep 3600'] - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent ports: - name: web containerPort: 80 restartPolicy: OnFailureEOF 123kubectl create -f multicontainer.ymlkubectl get podkubectl get -f multicontainer.yml -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod 2/2 Running 0 66s 172.16.200.199 k8s-worker1 &lt;none&gt; &lt;none&gt; 12root@k8s-master:~# curl 172.16.200.199&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 修改Pod直接修改yaml文件，然后执行以下命令 1kubectl apply -f pod.yml 进入容器并修改其内容 12345kubectl exec -it pod -c httpd -- /bin/bashroot@pod:/usr/local/apache2# echo lixiaohuitest &gt; htdocs/index.html root@pod:/usr/local/apache2# exitcurl http://172.16.200.199 Init类型容器根据安排，myapp-container的容器将等待两个init结束之后才会启动，也就是40秒之后才会启动 1234567891011121314151617181920212223cat &gt; init.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: initpd labels: app: myappspec: containers: - name: myapp-container image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600'] initContainers: - name: init-myservice image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', &quot;sleep 20&quot;] - name: init-mydb image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', &quot;sleep 20&quot;]EOF 12kubectl create -f init.yml kubectl get pod -w Sidecar类型容器两个容器挂载了同一个目录，一个容器负责写入数据，一个容器负责对外展示 12345678910111213141516171819202122232425cat &gt; sidecar.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: sidecarpodspec: containers: - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/local/apache2/htdocs/ name: lixiaohuivolume - name: busybox image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo &quot;Hello sidecar&quot; &gt; /usr/local/apache2/htdocs/index.html &amp;&amp; sleep 3600'] volumeMounts: - mountPath: /usr/local/apache2/htdocs/ name: lixiaohuivolume restartPolicy: OnFailure volumes: - name: lixiaohuivolume emptyDir: {}EOF 12kubectl create -f sidecar.yml kubectl get -f sidecar.yml -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESsidecarpod 2/2 Running 0 3m54s 172.17.245.1 k8s-worker2 &lt;none&gt; &lt;none&gt; 12curl http://172.17.245.1Hello sidecar Static Pod运行中的 kubelet 会定期扫描配置的目录中的变化， 并且根据文件中出现/消失的 Pod 来添加/删除 Pod。 1234systemctl status kubelet...Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf 1234tail /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf...[Service]Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot; 12grep -i static /var/lib/kubelet/config.yaml staticPodPath: /etc/kubernetes/manifests 编写静态pod yaml 12345678910111213cat &gt; static.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: staticpodspec: containers: - name: hello image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo &quot;Hello, lixiaohui!&quot; &amp;&amp; sleep 3600'] restartPolicy: OnFailureEOF 把这个yaml文件复制到/etc/kubernetes/manifests，然后观察pod列表，然后把yaml文件移出此文件夹，再观察pod列表 12cp static.yml /etc/kubernetes/manifests/kubectl get pod 12NAME READY STATUS RESTARTS AGEstaticpod-k8s-master 1/1 Running 0 74s 12rm -rf /etc/kubernetes/manifests/static.yml kubectl get pod 1No resources found in default namespace. Pod 删除kubectl delete pod –all会删除所有pod 1kubectl delete pod --all kubernetes 控制器Replica Set使用nginx镜像创建具有3个pod的RS,并分配合适的标签 1234567891011121314151617181920212223242526cat &gt; rs.yml &lt;&lt;EOFapiVersion: apps/v1kind: ReplicaSetmetadata: name: nginxrstest labels: app: nginxrstestspec: replicas: 3 selector: matchLabels: app: nginxrstest template: metadata: labels: app: nginxrstest spec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 imagePullPolicy: IfNotPresentEOF 12kubectl create -f rs.yml kubectl get replicasets.apps,pods -o wide 1234567NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORreplicaset.apps/nginxrstest 3 3 3 2m4s nginx nginx app=nginxrstestNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/nginxrstest-chtkc 1/1 Running 0 62s 172.17.93.196 k8s-worker1 &lt;none&gt; &lt;none&gt;pod/nginxrstest-scvhv 1/1 Running 0 62s 172.17.245.4 k8s-worker2 &lt;none&gt; &lt;none&gt;pod/nginxrstest-zqllq 1/1 Running 0 62s 172.17.193.2 k8s-master &lt;none&gt; &lt;none&gt; 123curl http://172.17.93.196...&lt;title&gt;Welcome to nginx!&lt;/title&gt; 1kubectl delete replicasets nginxrstest Deployment使用nginx镜像创建具有3个副本的Deployment，并分配合适的属性 123456789101112131415161718192021222324cat &gt; deployment.yml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80EOF 我们发现deployment管理了一个RS，而RS又实现了3个pod 12kubectl create -f deployment.ymlkubectl get deployments.apps,replicasets.apps,pods -l app=nginx 12345678910NAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-deployment 3/3 3 3 13sNAME DESIRED CURRENT READY AGEreplicaset.apps/nginx-deployment-69795dd799 3 3 3 13sNAME READY STATUS RESTARTS AGEpod/nginx-deployment-69795dd799-7cgwp 1/1 Running 0 13spod/nginx-deployment-69795dd799-vm5p4 1/1 Running 0 13spod/nginx-deployment-69795dd799-zx9g9 1/1 Running 0 13s 更新Deployment将deployment的镜像更改一次 1234kubectl set image deployments/nginx-deployment nginx=class-docker.myk8s.cn/library/nginx:1.16.1 --record查看更新进度kubectl rollout status deployment/nginx-deployment 123456789Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 old replicas are pending termination...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 old replicas are pending termination...deployment &quot;nginx-deployment&quot; successfully rolled out 更新过程是多了一个replicaset 1kubectl get deployments.apps,replicasets.apps,pods -l app=nginx 1234567891011NAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-deployment 3/3 3 3 2m55sNAME DESIRED CURRENT READY AGEreplicaset.apps/nginx-deployment-66b957f9d 3 3 3 2m16sreplicaset.apps/nginx-deployment-69795dd799 0 0 0 2m55sNAME READY STATUS RESTARTS AGEpod/nginx-deployment-66b957f9d-9cwgq 1/1 Running 0 116spod/nginx-deployment-66b957f9d-9zn4p 1/1 Running 0 98spod/nginx-deployment-66b957f9d-zvgzd 1/1 Running 0 2m16s 回滚 Deployment故意将镜像名称命名设置为 nginx:1.161 而不是nginx:1.16.1，发现永远无法更新成功，此时就需要回退 12kubectl set image deployments/nginx-deployment nginx=nginx:1.161 --recordkubectl rollout status deployment/nginx-deployment 1Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated... 查看历史版本 1kubectl rollout history deployments/nginx-deployment 12345deployment.apps/nginx-deployment REVISION CHANGE-CAUSE1 &lt;none&gt;2 kubectl set image deployments/nginx-deployment nginx=class-docker.myk8s.cn/library/nginx:1.16.1 --record=true3 kubectl set image deployments/nginx-deployment nginx=nginx:1.161 --record=true 1kubectl rollout history deployment.v1.apps/nginx-deployment --revision=3 12345678910111213deployment.apps/nginx-deployment with revision #3Pod Template: Labels: app=nginx pod-template-hash=64bd4564c8 Annotations: kubernetes.io/change-cause: kubectl set image deployments/nginx-deployment nginx=nginx:1.161 --record=true Containers: nginx: Image: nginx:1.161 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; 回退到版本2 12kubectl rollout undo deployments/nginx-deployment --to-revision=2kubectl rollout status deployment/nginx-deployment 伸缩 Deployment将指定的deployment副本更改为5 12kubectl scale deployments/nginx-deployment --replicas=5kubectl get deployments.apps,replicasets.apps -l app=nginx 1234567NAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-deployment 5/5 5 5 5m6sNAME DESIRED CURRENT READY AGEreplicaset.apps/nginx-deployment-64bd4564c8 0 0 0 113sreplicaset.apps/nginx-deployment-66b957f9d 5 5 5 4m27sreplicaset.apps/nginx-deployment-69795dd799 0 0 0 5m6s 1kubectl delete deployments.apps nginx-deployment DaemonSet使用busybox镜像，在每一个节点上都运行一个pod 12345678910111213141516171819202122cat &gt; daemonset.yml &lt;&lt;EOFapiVersion: apps/v1kind: DaemonSetmetadata: name: lixiaohui labels: daemonset: testspec: selector: matchLabels: name: testpod template: metadata: labels: name: testpod spec: containers: - name: hello image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'sleep 3600']EOF 12kubectl create -f daemonset.ymlkubectl get daemonsets.apps 12NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGElixiaohui 2 2 2 2 2 &lt;none&gt; 24s 1kubectl delete -f daemonset.yml StatefulSet使用nginx镜像，创建一个副本数为3的有状态应用，并挂载本地目录到容器中 123456789101112131415161718192021222324252627282930cat &gt; statefulset.yml &lt;&lt;EOFapiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: selector: matchLabels: app: nginx serviceName: &quot;nginx&quot; replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumes: - name: www emptyDir: {}EOF 发现创建的过程是有次序的，这也验证了有状态应用的启动顺序 12kubectl create -f statefulset.ymlkubectl get pods -w 123456789101112NAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 41sweb-1 0/1 Pending 0 0sweb-1 0/1 Pending 0 0sweb-1 0/1 ContainerCreating 0 0sweb-1 0/1 ContainerCreating 0 0sweb-1 1/1 Running 0 4sweb-2 0/1 Pending 0 0sweb-2 0/1 Pending 0 0sweb-2 0/1 ContainerCreating 0 0sweb-2 0/1 ContainerCreating 0 0sweb-2 1/1 Running 0 4s 12345kubectl get podNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 2m47sweb-1 1/1 Running 0 2m6sweb-2 1/1 Running 0 2m2s 1kubectl delete -f statefulset.yml Job与CronJobJob不断打印CKA JOB字符串，失败最多重试4次 12345678910111213141516cat &gt; job.yml &lt;&lt;EOFapiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: [&quot;sh&quot;, &quot;-c&quot;, &quot;while true;do echo CKA JOB;done&quot;] restartPolicy: Never backoffLimit: 4EOF 12kubectl create -f job.ymlkubectl get jobs,pods 12345NAME COMPLETIONS DURATION AGEjob.batch/pi 0/1 82s 82sNAME READY STATUS RESTARTS AGEpod/pi-66qbm 1/1 Running 0 82s 1kubectl logs pi-66qbm 1234CKA JOBCKA JOBCKA JOB 1kubectl delete -f job.yml CronJob每分钟打印一次指定字符串 123456789101112131415161718192021cat &gt; cronjob.yml &lt;&lt;EOFapiVersion: batch/v1kind: CronJobmetadata: name: cronjobtestspec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: hello image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailureEOF 123kubectl create -f cronjob.yml# 这里需要等待一分钟再去getkubectl get cronjobs,pod 1234NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEcronjob.batch/cronjobtest */1 * * * * False 0 23s 83sNAME READY STATUS RESTARTS AGEpod/cronjobtest-27444239-kqcjc 0/1 Completed 0 23s 1kubectl logs cronjobtest-27444239-kqcjc 12Wed Mar 2 11:45:00 UTC 2022Hello from the Kubernetes cluster 1kubectl delete -f crobjob.yml Service 服务发现用nginx镜像准备一个3副本的deployment作为后端，并开放80端口 123456789101112131415161718192021222324cat &gt; deployment-service.yml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment-servicetest labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80EOF 然后用kubectl expose的命令创建一个针对deployment的服务，并查询endpoint是否准备就绪 123kubectl create -f deployment-service.ymlkubectl expose deployment nginx-deployment-servicetest --port=9000 --name=lxhservice --target-port=80 --type=NodePortkubectl get service,endpoints 1234567NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 27mservice/lxhservice NodePort 10.96.213.26 &lt;none&gt; 9000:31919/TCP 28sNAME ENDPOINTS AGEendpoints/kubernetes 192.168.8.3:6443 27mendpoints/lxhservice 172.16.152.69:80,172.16.152.72:80,172.16.152.73:80 + 8 more... 28s 1curl http://192.168.8.3:31919 12...&lt;title&gt;Welcome to nginx!&lt;/title&gt; 1kubectl delete service lxhservice ClusterIP类型的ServiceClusterIP是默认的Service类型，对外提供8000端口，并把流量引流到具有app: nginx的后端80端口上 12345678910111213cat &gt; clusterip.yml &lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: nginx ports: - protocol: TCP port: 8000 targetPort: 80EOF 12kubectl create -f clusterip.ymlkubectl get service 12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmy-service ClusterIP 10.102.224.203 &lt;none&gt; 8000/TCP 88s 1curl http://10.102.224.203:8000 12...&lt;title&gt;Welcome to nginx!&lt;/title&gt; 1kubectl delete -f clusterip.yml NodePort类型的ServiceType: NodePort将会在节点的特定端口上开通服务，本实验中，我们指定了端口为31788 123456789101112131415cat &gt; nodeport.yml &lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: nodeservicespec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 8000 targetPort: 80 nodePort: 31788EOF 12kubectl create -f nodeport.ymlkubectl get service 12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnodeservice NodePort 10.100.234.83 &lt;none&gt; 8000:31788/TCP 11s 12# 因为是nodeport，所以用节点IPcurl http://192.168.8.3:31788 12...&lt;title&gt;Welcome to nginx!&lt;/title&gt; 1kubectl delete -f nodeport.yml Headless类型的Service在此类型的Service中，将不会只返回Service IP，会直接返回众多Pod 的IP地址，所以需要进入pod中用集群内DNS进行测试 1234567891011121314cat &gt; headless.yml &lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: headlessspec: clusterIP: None selector: app: nginx ports: - protocol: TCP port: 8000 targetPort: 80EOF 12kubectl create -f headless.ymlkubectl get service 12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheadless ClusterIP None &lt;none&gt; 8000/TCP 4s 1kubectl run --rm --image=class-docker.myk8s.cn/library/busybox:1.28 -it testpod 123456789If you don't see a command prompt, try pressing enter./ # nslookup headlessServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: headlessAddress 1: 172.16.127.16 172-16-127-16.headless.lixiaohui.svc.cluster.localAddress 2: 172.16.125.86 172-16-125-86.headless.lixiaohui.svc.cluster.localAddress 3: 172.16.125.85 172-16-125-85.headless.lixiaohui.svc.cluster.local 1kubectl delete -f headless.yml LoadBalancer类型的Service部署metallb负载均衡先部署一个metallb controller和Speaker metallb controller用于负责监听 Kubernetes Service 的变化，当服务类型被设置为 LoadBalancer 时，Controller 会从一个预先配置的 IP 地址池中分配一个 IP 地址给该服务，并管理这个 IP 地址的生命周期。 Speaker负责将服务的 IP 地址通过标准的路由协议广播到网络中，确保外部流量能够正确路由到集群中的服务。 1kubectl apply -f https://www.linuxcenter.cn/files/cka/cka-yaml/metallb-native.yaml 定义一组由负载均衡对外分配的IP地址范围 12345678910cat &gt; ippool.yml &lt;&lt;-EOFapiVersion: metallb.io/v1beta1kind: IPAddressPoolmetadata: name: lxh-ip-pool-192-168-8-10-100 namespace: metallb-systemspec: addresses: - 192.168.8.10-192.168.8.100EOF 1kubectl apply -f ippool.yml 在 Layer 2 模式下用于控制如何通过 ARP（Address Resolution Protocol）或 NDP（Neighbor Discovery Protocol）协议宣告服务的 IP 地址，使得这些 IP 地址在本地网络中可解析 12345678910cat &gt; l2Advertisement.yml &lt;&lt;-EOFapiVersion: metallb.io/v1beta1kind: L2Advertisementmetadata: name: l2-myippool namespace: metallb-systemspec: ipAddressPools: - lxh-ip-pool-192-168-8-10-100EOF 1kubectl apply -f l2Advertisement.yml 部署LoadBalancer 服务负载均衡准备好之后，创建LoadBalancer类型的服务 1234567891011121314cat &gt; loadbalancer.yml &lt;&lt;-EOFapiVersion: v1kind: Servicemetadata: name: loadbalance-servicespec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: LoadBalancerEOF 1kubectl apply -f loadbalancer.yml 获取服务看看是否分配到了负载均衡IP 1kubectl get service 从输出上看，分配到了192.168.8.10 12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEloadbalance-service LoadBalancer 10.110.113.122 192.168.8.10 80:30214/TCP 4s 用负载均衡IP访问一下试试 1curl 192.168.8.10 1&lt;title&gt;Welcome to nginx!&lt;/title&gt; 删除service资源 1kubectl delete -f loadbalancer.yml IngressIngress 需要Ingress控制器支持，先部署控制器 1kubectl apply -f https://www.linuxcenter.cn/files/cka/cka-yaml/ingressdeploy.yaml 1kubectl get pod -n ingress-nginx 123456NAME READY STATUS RESTARTS AGENAME READY STATUS RESTARTS AGEingress-nginx-admission-create-cv22n 0/1 Completed 0 92singress-nginx-admission-patch-lbr2v 0/1 Completed 1 92singress-nginx-controller-tdpb2 1/1 Running 0 92singress-nginx-controller-w2q4g 1/1 Running 0 92s 用nginx镜像生成一个3副本的Pod，并通过Service提供服务，然后再用ingress，以特定域名的方式对外暴露 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354cat &gt; ingress.yml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment-ingress labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: ingressservicespec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: lixiaohuispec: ingressClassName: nginx rules: - host: www.lixiaohui.com http: paths: - pathType: Prefix path: &quot;/&quot; backend: service: name: ingressservice port: number: 80EOF 12kubectl create -f ingress.ymlkubectl get deployments,service,ingress 12345678NAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment-ingress 3/3 3 3 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingressservice ClusterIP 10.110.117.37 &lt;none&gt; 80/TCP 2m7NAME CLASS HOSTS ADDRESS PORTS AGElixiaohui nginx www.lixiaohui.com 192.168.8.4,192.168.8.5 80 2m26s 12345把上述ADDRESS部分的IP和域名绑定解析echo 192.168.8.4 www.lixiaohui.com &gt;&gt; /etc/hostscurl http://www.lixiaohui.com 1kubectl delete -f ingress.yml Gateway APIGateway API 基本介绍Kubernetes Gateway API 是一种新的 API 规范，旨在提供一种在 Kubernetes 中管理网关和负载均衡器的标准方法。它被设计为 Ingress API 的替代方案，提供更丰富的功能和更好的扩展性,Gateway API 的核心思想是通过使用可扩展的、角色导向的、协议感知的配置机制来提供网络服务。 核心组件： Gateway API 包括几个核心组件： GatewayClass：定义一组具有配置相同的网关，由实现该类的控制器管理。 Gateway：定义流量处理基础设施（例如云负载均衡器）的一个实例。 Route：描述了特定协议的规则，用于将请求从 Gateway 映射到 Kubernetes 服务。目前，HTTPRoute 是比较稳定的版本，而 TCPRoute、UDPRoute、GRPCRoute、TLSRoute 等也在开发中。 GatewayClass 类似于阿里云负载均衡中的负载均衡实例类型选择。在阿里云中，用户可以选择应用型负载均衡（ALB）、网络型负载均衡（NLB）或传统型负载均衡（CLB），每种类型都有其特定的配置和功能。在 Kubernetes 中，GatewayClass 定义了一组共享通用配置和行为的 Gateway 集合，类似于选择一种负载均衡的实现类型 Gateway 类似于阿里云负载均衡中的具体负载均衡实例。在阿里云中，用户创建一个负载均衡实例后，可以配置监听端口、协议等。在 Kubernetes 中，Gateway 描述了流量被分配到集群中服务的方式，是 GatewayClass 的具体实现 Route 类似于阿里云负载均衡中的转发规则。在阿里云中，用户可以配置基于域名、路径、HTTP Header 等条件的转发规则。在 Kubernetes 中，Route 描述了通过网关的流量如何映射到服务，例如 HTTPRoute 可以基于请求路径、主机名等条件将流量转发到不同的后端服务。 以下是使用 Gateway 和 HTTPRoute 将 HTTP 流量路由到服务的简单示例： 在此示例中，实现为反向代理的 Gateway 的请求数据流如下： 客户端开始准备 URL 为 http://test.lixiaohui.com 的 HTTP 请求 客户端的 DNS 解析器查询目标名称并了解与 Gateway 关联的一个或多个 IP 地址的映射。 客户端向 Gateway IP 地址发送请求；反向代理接收 HTTP 请求并使用 Host: 标头来匹配基于 Gateway 和附加的 HTTPRoute 所获得的配置。 可选的，反向代理可以根据 HTTPRoute 的匹配规则进行请求头和（或）路径匹配。 可选地，反向代理可以修改请求；例如，根据 HTTPRoute 的过滤规则添加或删除标头。 最后，反向代理将请求转发到一个或多个后端。 Gateway API实验 需要先做metallb，由metalb给service提供外部负载均衡IP 部署nginx Fabric，为GatewayAPI做后端流量处理组件 创建一个基于Fabric的gatewayClass 创建一个gateway，并监听在80端口，并关联刚创建的gatewayClass 创建一个httpRoute，此处定义客户端访问的域名和路径 实验效果： 外部客户端可以用浏览器打开http://test.lixiaohui.com 并返回我们的nginx业务网站内容 部署 Gateway API CRD 这一步用于扩展K8S功能，以便于支持Gateway API 1kubectl apply -f https://www.linuxcenter.cn/files/cka/gatewayapi/experimental-install.yaml 部署Fabric自定义资源 1kubectl apply -f https://www.linuxcenter.cn/files/cka/gatewayapi/nginx-fabric-crds.yaml 部署Fabric 这一步部署的Fabric用于处理后端流量处理 1kubectl apply -f https://www.linuxcenter.cn/files/cka/gatewayapi/nginx-fabric-deploy.yaml 等它部署好之后，看看pod起来没 123root@k8s-master:~# kubectl get pods -n nginx-gatewayNAME READY STATUS RESTARTS AGEnginx-gateway-5c8f44c856-2zt76 2/2 Running 0 32s 这里将会自动创建基于nginx的GatewayClass 1234root@k8s-master:~# kubectl get gatewayclasses.gateway.networking.k8s.ioNAME CONTROLLER ACCEPTED AGEnginx gateway.nginx.org/nginx-gateway-controller True 45s 部署应用，这里的应用是模拟公司的常规业务，稍后用于对外提供服务 123456789101112131415161718192021222324cat &gt; deployment-service.yml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: name: k8sgateway-lxhtest labels: app: nginxspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80EOF 为了稳定pod的访问，这里用service的方式做了一个内部暴露 12kubectl create -f deployment-service.ymlkubectl expose deployment k8sgateway-lxhtest --port=9000 --name=lxhservice --target-port=80 创建一个名为lxh-gateway的gateway并关联了一个名为nginx的gatewayClass，这个gateway提供了一个监听在80端口的http协议的监听器，这个监听器接收来自任何namespace以lixiaohui.com为后缀的所有请求。 创建一个名为lxh-http的httpRoute，并关联我们的gateway，本次httpRoute提供了test.lixiaohui.com的域名根目录的请求入口，并将流量导入到一个名为lxhservice的9000端口 123456789101112131415161718192021222324252627282930313233cat &gt; gatewayandhttproute.yml &lt;&lt;-EOFapiVersion: gateway.networking.k8s.io/v1kind: Gatewaymetadata: name: lxh-gatewayspec: gatewayClassName: nginx listeners: - name: default hostname: &quot;*.lixiaohui.com&quot; port: 80 protocol: HTTP allowedRoutes: namespaces: from: All---apiVersion: gateway.networking.k8s.io/v1kind: HTTPRoutemetadata: name: lxh-httpspec: parentRefs: - name: lxh-gateway hostnames: [&quot;test.lixiaohui.com&quot;] rules: - matches: - path: type: PathPrefix value: / backendRefs: - name: lxhservice port: 9000EOF 1kubectl apply -f gatewayandhttproute.yml 可以看到，我们的gateway，已经从负载均衡中，拿到了外部IP地址 1234root@k8s-master:~# kubectl get service -n nginx-gatewayNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-gateway LoadBalancer 10.96.18.47 192.168.8.10 80:31752/TCP,443:32200/TCP 7m1s 服务后端也有endpoint 123root@k8s-master:~# kubectl get endpoints -n nginx-gatewayNAME ENDPOINTS AGEnginx-gateway 172.16.194.67:443,172.16.194.67:80 8m24s 查看我们的gateway和httproute 1234567root@k8s-master:~# kubectl get gatewaysNAME CLASS ADDRESS PROGRAMMED AGElxh-gateway nginx 192.168.8.10 True 5m19sroot@k8s-master:~# kubectl get httprouteNAME HOSTNAMES AGElxh-http [&quot;test.lixiaohui.com&quot;] 5m25s 访问测试 12echo 192.168.8.10 test.lixiaohui.com &gt;&gt; /etc/hostscurl http://test.lixiaohui.com 1&lt;title&gt;Welcome to nginx!&lt;/title&gt; 健康检查Liveness Probes文件存活检测创建一个名为liveness的容器，并在其中执行文件的创建，休眠，然后再删除文件的操作，然后用livenessProbe来检测 123456789101112131415161718192021222324cat &gt; liveness.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: labels: test: liveness name: liveness-execspec: containers: - name: liveness image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5EOF 1.periodSeconds 字段指定了 kubelet 应该每 5 秒执行一次存活探测。 2.initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。 3.kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测。 4.如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。 5.如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。 这个容器生命周期的前 30 秒， /tmp/healthy 文件是存在的。 所以在这最开始的 30 秒内，执行命令 cat /tmp/healthy 会返回成功代码。 30 秒之后，执行命令 cat /tmp/healthy 就会返回失败代码。 12kubectl create -f liveness.yml kubectl describe pod liveness-exec 123456789101112Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m16s default-scheduler Successfully assigned lixiaohui/liveness-exec to k8s-worker1 Normal Pulled 3m11s kubelet Successfully pulled image &quot;busybox&quot; in 3.821847462s Normal Pulled 117s kubelet Successfully pulled image &quot;busybox&quot; in 3.615149362s Normal Pulling 45s (x3 over 3m15s) kubelet Pulling image &quot;busybox&quot; Normal Created 43s (x3 over 3m11s) kubelet Created container liveness Normal Started 43s (x3 over 3m11s) kubelet Started container liveness Normal Pulled 43s kubelet Successfully pulled image &quot;busybox&quot; in 2.73613205s Warning Unhealthy 0s (x9 over 2m40s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 0s (x3 over 2m30s) kubelet Container liveness failed liveness probe, will be restarted 每30秒在pod事件中就会显示存活探测器失败了，下方信息显示这个容器被杀死并且被重建了3次 1kubectl get pods 12NAME READY STATUS RESTARTS AGEliveness-exec 1/1 Running 3 (63s ago) 4m49s 1kubectl delete -f liveness.yml HTTP存活检测以httpget的形式访问容器中的/lixiaohui页面，根据返回代码来判断是否正常 123456789101112131415161718192021cat &gt; httpget.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: httpspec: containers: - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent livenessProbe: httpGet: path: /lixiaohui port: 80 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 restartPolicy: OnFailureEOF kubelet 会向容器内运行的服务发送一个 HTTP GET 请求来执行探测。 如果服务器上 /lixiaohui路径下的处理程序返回成功代码，则 kubelet 认为容器是健康存活的。 如果处理程序返回失败代码，则 kubelet 会杀死这个容器并且重新启动它。 任何大于或等于 200 并且小于 400 的返回代码标示成功，其它返回代码都标示失败。 12kubectl create -f httpget.yml kubectl get pods 12NAME READY STATUS RESTARTS AGEhttp 0/1 Completed 3 3m6s 1kubectl describe pod http 1234567891011Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m28s default-scheduler Successfully assigned lixiaohui/http to k8s-worker1 Normal Pulled 3m23s kubelet Successfully pulled image &quot;httpd&quot; in 4.365187879s Normal Pulled 3m9s kubelet Successfully pulled image &quot;httpd&quot; in 2.794325055s Normal Created 2m54s (x3 over 3m23s) kubelet Created container httpd Normal Started 2m54s (x3 over 3m23s) kubelet Started container httpd Normal Pulled 2m54s kubelet Successfully pulled image &quot;httpd&quot; in 2.497771235s Warning Unhealthy 2m43s (x9 over 3m19s) kubelet Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 2m43s (x3 over 3m13s) kubelet Container httpd failed liveness probe, will be restarted 1kubectl delete -f httpget.yml ReadinessProbeTCP存活检测kubelet 会在容器启动 5 秒后发送第一个就绪探测。 这会尝试连接容器的 800 端口。如果探测成功，这个 Pod 会被标记为就绪状态，kubelet 将继续每隔 10 秒运行一次检测。 除了就绪探测，这个配置包括了一个存活探测。 kubelet 会在容器启动 15 秒后进行第一次存活探测。 与就绪探测类似，会尝试连接 器的 800 端口。 如果存活探测失败，这个容器会被重新启动。 1234567891011121314151617181920212223242526cat &gt; readiness.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: tcpcheckspec: containers: - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent ports: - name: webport protocol: TCP containerPort: 80 readinessProbe: tcpSocket: port: 800 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 800 initialDelaySeconds: 15 periodSeconds: 20 restartPolicy: OnFailureEOF 12kubectl create -f readiness.yml kubectl get pods 12NAME READY STATUS RESTARTS AGEtcpcheck 0/1 Running 1 (6s ago) 67s 1kubectl describe pod tcpcheck 123456789101112...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 60s default-scheduler Successfully assigned lixiaohui/tcpcheck to k8s-worker1 Normal Pulling 60s kubelet Pulling image &quot;httpd&quot; Normal Pulled 57s kubelet Successfully pulled image &quot;httpd&quot; in 2.730390747s Normal Created 57s kubelet Created container httpd Normal Started 57s kubelet Started container httpd Warning Unhealthy 0s (x7 over 50s) kubelet Readiness probe failed: dial tcp 172.16.125.74:800: connect: connection refused Warning Unhealthy 0s (x3 over 40s) kubelet Liveness probe failed: dial tcp 172.16.125.74:800: connect: connection refused Normal Killing 0s kubelet Container httpd failed liveness probe, will be restarted 可以看到，我们的pod对外提供了80端口，但是我们一直在检测800端口，所以这个pod的检测是失败的 1kubectl delete -f readiness.yml StartupProbe启动探测器有时候，会有一些现有的应用程序在启动时需要较多的初始化时间。 要不影响对引起探测死锁的快速响应，这种情况下，设置存活探测参数是要技巧的。 技巧就是使用一个命令来设置启动探测，针对HTTP 或者 TCP 检测，可以通过设置 failureThreshold * periodSeconds 参数来保证有足够长的时间应对糟糕情况下的启动时间。应用程序将会有最多 30秒(3 * 10 = 30s) 的时间来完成它的启动。 一旦启动探测成功一次，存活探测任务就会接管对容器的探测，对容器死锁可以快速响应。 如果启动探测一直没有成功，容器会在 30 秒后被杀死，并且根据 restartPolicy 来设置 Pod 状态。 12345678910111213141516171819202122232425262728cat &gt; startup.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: startprobespec: containers: - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent ports: - name: webport protocol: TCP containerPort: 80 readinessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 periodSeconds: 10 startupProbe: httpGet: path: / port: 800 initialDelaySeconds: 5 failureThreshold: 3 periodSeconds: 10 restartPolicy: OnFailureEOF Probe参数 Probe 有很多配置字段，可以使用这些字段精确的控制存活和就绪检测的行为： 1.initialDelaySeconds：容器启动后要等待多少秒后存活和就绪探测器才被初始化，默认是 0 秒，最小值是 0。 2.periodSeconds：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。 3.timeoutSeconds：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。 4.successThreshold：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。 存活和启动探测的这个值必须是 1。最小值是 1。 5.failureThreshold：当探测失败时，Kubernetes 的重试次数。 存活探测情况下的放弃就意味着重新启动容器。 就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。 12kubectl create -f startup.ymlkubectl describe -f startup.yml 12345678Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 24s default-scheduler Successfully assigned default/startprobe to k8s-worker2 Normal Pulled 23s kubelet Container image &quot;httpd&quot; already present on machine Normal Created 23s kubelet Created container httpd Normal Started 23s kubelet Started container httpd Warning Unhealthy 3s (x2 over 13s) kubelet Startup probe failed: Get &quot;http://192.168.20.20:800/&quot;: dial tcp 192.168.20.20:800: connect: connection refused 可以发现由于我们故意写成了800端口，检测失败，容器一直无法就绪 1kubectl delete -f startup.yml Kubernetes 探针检测顺序与优先级在 Kubernetes 中，startupProbe、livenessProbe 和 readinessProbe 是用于监控和管理容器健康状况的探针，每种探针在容器生命周期中的不同阶段发挥不同的作用。以下是这三种探针的检测顺序和优先级： 1. startupProbe 检测顺序：startupProbe 是在容器启动时首先执行的探针。它用于判断应用是否已成功启动，并且只在启动期间运行。 优先级：如果配置了 startupProbe，Kubernetes 会忽略 livenessProbe 和 readinessProbe 直到 startupProbe 成功。startupProbe 成功后，livenessProbe 和 readinessProbe 才会开始运行。 目的：用于处理启动时间较长的应用程序，确保应用在完全启动之前不会因 livenessProbe 的失败而被重启。 2. livenessProbe 检测顺序：在 startupProbe 成功之后，livenessProbe 开始执行。它定期检查容器是否处于健康状态。 优先级：如果配置了 startupProbe，livenessProbe 只有在 startupProbe 成功之后才开始运行。如果未配置 startupProbe，livenessProbe 在容器启动后立即开始运行。 目的：用于检测容器是否仍然处于健康状态。如果 livenessProbe 失败，Kubernetes 会重启该容器。 3. readinessProbe 检测顺序：在 startupProbe 成功之后，readinessProbe 开始执行。它定期检查容器是否已准备好接收流量。 优先级：如果配置了 startupProbe，readinessProbe 只有在 startupProbe 成功之后才开始运行。如果未配置 startupProbe，readinessProbe 在容器启动后立即开始运行。 目的：用于判断容器是否可以接收请求。如果 readinessProbe 失败，容器将从服务的端点列表中移除，不再接收新的流量。 总结 顺序：startupProbe -&gt; livenessProbe -&gt; readinessProbe 优先级： startupProbe 优先于其他两个探针。如果配置了 startupProbe，必须先通过 startupProbe 检测，livenessProbe 和 readinessProbe 才会启动。 livenessProbe 和 readinessProbe 在 startupProbe 成功后同时开始运行，没有严格的优先级区分，但它们的作用不同，livenessProbe 用于重启失败的容器，readinessProbe 用于控制流量。 优雅关闭从 Kubernetes 1.22 开始，terminationGracePeriodSeconds 特性被开启，在杀死容器时，Pod停止获得新的流量。但在Pod中运行的容器不会受到影响。直到超时发生。可以在Pod级别或者容器下具体的探针级别设定，探针会优先和覆盖Pod级别 下面的例子中，容器将在收到结束需求是沉睡2分钟来代表业务的正常关闭，然后整个pod最多等待200秒，超过200秒，就会强制删除 123456789101112131415161718192021cat &gt; grace.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: httpgracespec: terminationGracePeriodSeconds: 200 containers: - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent ports: - name: webport protocol: TCP containerPort: 80 lifecycle: preStop: exec: command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 2m&quot;] restartPolicy: OnFailureEOF 12kubectl create -f grace.yml kubectl get -f grace.yml 12NAME READY STATUS RESTARTS AGEhttpgrace 1/1 Running 0 7s 12kubectl delete -f grace.yml &amp;kubectl get pod 12NAME READY STATUS RESTARTS AGEhttpgrace 1/1 Terminating 0 18s 配置存储卷emptyDir当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的 123456789101112131415161718cat &gt; emptydir.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: emptydirspec: containers: - image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent name: test-container volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: sizeLimit: 2GEOF 此时sizeLimit要注意内容量的单位，如果是单位是M，那就是以1000为进制，如果是Mi就是1024进制 12kubectl create -f emptydir.yml kubectl get -f emptydir.yml -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESemptydir 1/1 Running 0 29s 172.16.200.228 k8s-worker1 &lt;none&gt; &lt;none&gt; 12# 根据上面的提示，在指定的机器上完成这个步骤crictl ps | grep -i test-container 1d27c066d7acc3 faed93b288591 2 minutes ago Running test-container 0 6f045542048c9 1crictl inspect d27c066d7acc3 | grep cache 123456&quot;containerPath&quot;: &quot;/cache&quot;,&quot;hostPath&quot;: &quot;/var/lib/kubelet/pods/baa08250-5800-4828-a3cd-bfcd0789af37/volumes/kubernetes.io~empty-dir/cache-volume&quot;, &quot;container_path&quot;: &quot;/cache&quot;, &quot;host_path&quot;: &quot;/var/lib/kubelet/pods/baa08250-5800-4828-a3cd-bfcd0789af37/volumes/kubernetes.io~empty-dir/cache-volume&quot; &quot;destination&quot;: &quot;/cache&quot;, &quot;source&quot;: &quot;/var/lib/kubelet/pods/baa08250-5800-4828-a3cd-bfcd0789af37/volumes/kubernetes.io~empty-dir/cache-volume&quot;, 可以看到我们的数据卷被创建到了/var/lib/kubelet/pods/baa08250-5800-4828-a3cd-bfcd0789af37/volumes/kubernetes.io~empty-dir/cache-volume 1kubectl delete -f emptydir.yml HostPathhostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中，但要注意的是要尽可能避免使用这个类型的卷，会限制pod的迁移性 下面的例子中，我们挂载了一个目录到容器中，并通过nginx对外展示其中的index.html 12345678910111213141516171819202122cat &gt; hostpath.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: hostpathtestspec: containers: - image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent name: hostpathpod ports: - name: web containerPort: 80 volumeMounts: - mountPath: /usr/share/nginx/html name: test-volume volumes: - name: test-volume hostPath: path: /data type: DirectoryOrCreateEOF 取值 行为 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。 DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 Directory 在给定路径上必须存在的目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 File 在给定路径上必须存在的文件。 Socket 在给定路径上必须存在的 UNIX 套接字。 CharDevice 在给定路径上必须存在的字符设备。 BlockDevice 在给定路径上必须存在的块设备。 12kubectl create -f hostpath.yml kubectl get -f hostpath.yml -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEShostpathtest 1/1 Running 0 3s 172.16.200.223 k8s-worker2 &lt;none&gt; &lt;none&gt; 1234# 根据提示，在work02上完成这个步骤echo hostwrite &gt; /data/index.htmlcurl http://172.16.200.223 1hostwrite 1kubectl delete -f hostpath.yml 持久卷概述持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者 使用存储类（Storage Class）来动态供应。 持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，也是使用 卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期持久卷申请（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。 Pod 会耗用节点资源，而 PVC 申领会耗用 PV 资源 构建简易NFS服务器在master上模拟一个nfs服务器，将本地的/nfsshare共享出来给所有人使用 123456apt install nfs-kernel-server -ymkdir /nfssharechmod 777 /nfsshare -Recho /nfsshare *(rw) &gt;&gt; /etc/exportssystemctl enable nfs-server --nowexportfs -rav PV创建一个名为nfspv大小为5Gi卷，并以ReadWriteOnce的方式申明，且策略为Recycle 12345678910111213141516171819202122cat &gt; pv.yml &lt;&lt;EOFapiVersion: v1kind: PersistentVolumemetadata: name: nfspv labels: pvname: nfspvspec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /nfsshare server: 192.168.8.3EOF Kubernetes 支持两种卷模式（volumeModes）：Filesystem（文件系统） 和 Block（块），volumeMode 属性设置为 Filesystem 的卷会被 Pod 挂载（Mount） 到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前 在设备上创建文件系统。 访问模式 描述 ReadWriteOnce 卷可以被一个节点以读写方式挂载。 ReadWriteOnce 访问模式也允许运行在同一节点上的多个 Pod 访问卷。 ReadOnlyMany 卷可以被多个节点以只读方式挂载。 ReadWriteMany 卷可以被多个节点以读写方式挂载。 ReadWriteOncePod 卷可以被单个 Pod 以读写方式挂载。 如果你想确保整个集群中只有一个 Pod 可以读取或写入该 PVC， 请使用ReadWriteOncePod 访问模式。 在创建pv前，需要确保在3个节点上都安装了nfs客户端 1apt install nfs-common -y 12kubectl create -f pv.yml kubectl get pv 12NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfspv 5Gi RWO Recycle Available slow 4s 回收策略 描述 Retain 手动回收 Recycle 基本擦除 (rm -rf /thevolume/*) Delete 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除，目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。 PV卷状态 描述 Available（可用） 卷是一个空闲资源，尚未绑定到任何申领； Bound（已绑定） 该卷已经绑定到某申领； Released（已释放） 所绑定的申领已被删除，但是资源尚未被集群回收； Failed（失败） 卷的自动回收操作失败。 PVC1234567891011121314151617cat &gt; pvc.yml &lt;&lt;EOFapiVersion: v1kind: PersistentVolumeClaimmetadata: name: myclaimspec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 1Gi storageClassName: slow selector: matchLabels: pvname: &quot;nfspv&quot;EOF 12kubectl create -f pvc.yml kubectl get pvc 12NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmyclaim Bound nfspv 5Gi RWO slow 8s 参数 描述 accessModes 申领在请求具有特定访问模式的存储时，使用与卷相同的访问模式约定。 volumeMode 申领使用与卷相同的约定来表明是将卷作为文件系统还是块设备来使用。 resources 申领和 Pod 一样，也可以请求特定数量的资源。 selector 申领可以设置标签选择算符 来进一步过滤卷集合。只有标签与选择算符相匹配的卷能够绑定到申领上。 selector 参数选择： matchLabels - 卷必须包含带有此值的标签matchExpressions - 通过设定键（key）、值列表和操作符（operator） 来构造的需求。合法的操作符有 In、NotIn、Exists 和 DoesNotExist。来自 matchLabels 和 matchExpressions 的所有需求都按逻辑与的方式组合在一起。 这些需求都必须被满足才被视为匹配。 使用PV和PVC创建一个pod并尝试使用PVC 12345678910111213141516171819202122apt install nfs-common -ycat &gt; pvcuse.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: myfrontend image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent ports: - name: web containerPort: 80 volumeMounts: - mountPath: &quot;/usr/local/apache2/htdocs&quot; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaimEOF 12kubectl create -f pvcuse.yml kubectl get -f pvcuse.yml -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmypod 1/1 Running 0 8s 172.16.200.225 k8s-worker2 &lt;none&gt; &lt;none&gt; 1234567# 在NFS服务器上(192.168.8.3)创建出index.html网页echo pvctest &gt; /nfsshare/index.html# 这里要看一下调度到哪个机器，这个机器必须执行apt install nfs-common -ycurl http://172.16.200.225 1pvctest 1kubectl delete -f pvcuse.yml 配置存储类NFS外部供应下载外部供应代码12git clone https://gitee.com/cnlxh/nfs-subdir-external-provisioner.gitcd nfs-subdir-external-provisioner 本次采用default命名空间，如果需要别的命名空间，请执行以下替换 123NS=$(kubectl config get-contexts|grep -e &quot;^\\*&quot; |awk '{print $5}')NAMESPACE=${NS:-default}sed -i'' &quot;s/namespace:.*/namespace: $NAMESPACE/g&quot; ./deploy/rbac.yaml ./deploy/deployment.yaml 如果集群采用了RBAC，请授权一下 1kubectl create -f deploy/rbac.yaml 配置NFS外部供应根据实际情况在deploy/deployment中修改镜像、名称、nfs地址和挂载 1kubectl create -f deploy/deployment.yaml 部署存储类要支持nfs挂载，所有节点都需要安装nfs-common安装包 1apt install nfs-common -y 123456789101112cat &gt; storageclassdeploy.yml &lt;&lt;'EOF'apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-clientprovisioner: cnlxh/nfs-storageallowVolumeExpansion: truevolumeBindingMode: WaitForFirstConsumerparameters: pathPattern: &quot;${.PVC.namespace}-${.PVC.name}&quot; onDelete: deleteEOF 指定存储卷的绑定模式为 WaitForFirstConsumer，这意味着 PV 的创建和绑定会在 PVC 被 Pod 引用时才进行，而不是在 PVC 创建时立即绑定。这种模式特别适用于需要考虑 Pod 调度约束的场景，例如确保 PV 创建在与 Pod 同一节点上（对于本地存储）或同一可用区（对于云存储）。 1kubectl create -f storageclassdeploy.yml 标记默认存储类1kubectl get storageclass 12NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEnfs-client cnlxh/nfs-storage Delete Immediate false 22m 1kubectl patch storageclass nfs-client -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}' 使用存储类只需要在pvc.spec中执行storageClassName: nfs-client就可以了 1kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml 打开test-pod.yaml就会发现，它向我们的pvc也就是nfs服务器写入了名为SUCCESS文件，在nfs服务器上执行： 1ls /nfsshare/default-test-claim/ 1SUCCESS 删除pod和pvc，会删除我们的资源，测试一下，执行后，会删除pod、pvc、pv，再去nfs服务器查看，数据就没了 1kubectl delete -f deploy/test-pod.yaml -f deploy/test-claim.yaml Pod调度nodeSelector给k8s-worker2节点打一个标签name=lixiaohui 1kubectl label nodes k8s-worker2 name=lixiaohui 如果需要删除标签可以用： 1kubectl label nodes k8s-worker2 name- 将pod仅调度到具有name=lixiaohui标签的节点上 12345678910111213cat &gt; assignpod.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: cnlxhtestspec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent nodeSelector: name: lixiaohuiEOF 12kubectl create -f assignpod.yml kubectl get pod cnlxhtest -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScnlxhtest 1/1 Running 0 10s 172.16.125.120 k8s-worker2 &lt;none&gt; &lt;none&gt; 1kubectl delete -f assignpod.yml nodeName将Pod仅调度到具有特定名称的节点上，例如仅调度到k8s-worker1上 12345678910111213cat &gt; nodename.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: lxhnodenamespec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent nodeName: k8s-worker1EOF 12kubectl create -f nodename.yml kubectl get pod lxhnodename -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESlxhnodename 1/1 Running 0 9s 172.16.127.5 k8s-worker1 &lt;none&gt; &lt;none&gt; 1kubectl delete -f nodename.yml tolerationsmaster节点默认不参与调度的原因就是因为其上有taint，而toleration就是容忍度 1kubectl describe nodes k8s-master | grep -i taint 1node-role.kubernetes.io/control-plane:NoSchedule 添加一个磁盘类型为hdd就不调度的污点 1kubectl taint node k8s-worker2 disktype=hdd:NoSchedule 如需删除以上污点，可用以下命令实现： 1kubectl taint node k8s-worker2 disktype- 创建一个可以容忍具有master taint的pod 123456789101112131415cat &gt; tolerations.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: tolerationsspec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent tolerations: - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot;EOF 12kubectl create -f tolerations.yml kubectl get pod tolerations -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStolerations 1/1 Running 0 7s 172.16.125.65 k8s-worker1 &lt;none&gt; &lt;none&gt; 此时我们发现，并没有调度到k8s-master上，由此我们得出来一个结果，容忍不代表必须，如果必须要调度到k8s-master，需要用以下例子 1234567891011121314151617cat &gt; mustassign.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: tolerationsmustspec: containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent tolerations: - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; nodeSelector: node-role.kubernetes.io/control-plane: &quot;&quot; EOF 12kubectl create -f mustassign.yml kubectl get -f mustassign.yml -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStolerations 1/1 Running 0 3s 172.16.119.16 k8s-master &lt;none&gt; &lt;none&gt; 12kubectl delete -f tolerations.ymlkubectl delete -f mustassign.yml affinity本实验展示在 Kubernetes 集群中，如何使用节点亲和性把 Kubernetes Pod 分配到特定节点 首先需要给节点打上一个合适的标签 1kubectl label nodes k8s-worker2 disktype=ssd 查看节点是否具有标签 1kubectl get nodes --show-labels | grep -i disktype 删除k8s-worker2上的污点，避免干扰 1kubectl taint node k8s-worker2 disktype- 强制调度到具有特定标签的节点上 1234567891011121314151617181920cat &gt; required.yml &lt;&lt;'EOF'apiVersion: v1kind: Podmetadata: name: requirespec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresentEOF 可以看到的确调度到了k8s-worker2 12345kubectl create -f required.ymlkubectl get -f required.yml -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESrequire 1/1 Running 0 57s 172.16.126.6 k8s-worker2 &lt;none&gt; &lt;none&gt; 再来试试优先但不强制的调度 123456789101112131415161718192021cat &gt; preferred.yml &lt;&lt;'EOF'apiVersion: v1kind: Podmetadata: name: nginxspec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresentEOF 可以看到依旧被调度到k8s-worker2上 1234kubectl create -f preferred.ymlkubectl get -f preferred.yml -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx 1/1 Running 0 10s 172.16.126.7 k8s-worker2 &lt;none&gt; &lt;none&gt; 你可以测试一下把k8s-worker2机器关机，再从yaml中把pod改个名防止冲突，在关机后重新创建，会看到也可以调度到其他节点 PriorityClass本实验主要是学习如何查询和创建PriorityClass，具有更高数字的PriorityClass在资源紧张时会优先获得资源，具有更低数字的PriorityClass在资源紧张时会被驱逐 在 Kubernetes 中，PriorityClass 的优先级范围是一个整数值，用于定义 Pod 的相对优先级。优先级值越高，表示 Pod 的优先级越高。Kubernetes 使用这些优先级值来决定在资源紧张时哪些 Pod 可以被抢占，以及在调度时哪些 Pod 应该优先调度。 优先级范围的意义 调度优先级： 当多个 Pod 等待调度时，调度器会优先调度优先级更高的 Pod。 如果资源不足，调度器会尝试抢占优先级较低的 Pod，以满足高优先级 Pod 的资源需求。 抢占行为： 如果高优先级的 Pod 无法找到足够的资源，调度器会尝试抢占优先级较低的 Pod。 被抢占的 Pod 会被终止，并重新调度到其他节点（如果资源允许）。 PriorityClass 还有两个可选字段： globalDefault 字段表示这个 PriorityClass 的值应该用于没有 priorityClassName 的 Pod。 系统中只能存在一个 globalDefault 设置为 true 的 PriorityClass。 如果不存在设置了 globalDefault 的 PriorityClass， 则没有 priorityClassName 的 Pod 的优先级为零。 description 字段是一个任意字符串。 它用来告诉集群用户何时应该使用此 PriorityClass。 查询系统中现有的PriorityClass 1234root@k8s-master:~# kubectl get priorityclassesNAME VALUE GLOBAL-DEFAULT AGE PREEMPTIONPOLICYsystem-cluster-critical 2000000000 false 45h PreemptLowerPrioritysystem-node-critical 2000001000 false 45h PreemptLowerPriority 123456789cat &gt; pc.yml &lt;&lt;-'EOF'apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata: name: high-priorityvalue: 1000000globalDefault: falsedescription: &quot;此优先级类应仅用于 XYZ 服务 Pod。&quot;EOF 创建并再次查看 12345678root@k8s-master:~# kubectl apply -f pc.ymlpriorityclass.scheduling.k8s.io/high-priority createdroot@k8s-master:~# kubectl get priorityclasses.scheduling.k8s.ioNAME VALUE GLOBAL-DEFAULT AGE PREEMPTIONPOLICYhigh-priority 1000000 false 10s PreemptLowerPrioritysystem-cluster-critical 2000000000 false 45h PreemptLowerPrioritysystem-node-critical 2000001000 false 45h PreemptLowerPriority 创建后可以将其应用到pod或deployment上其作用 1234567891011121314151617181920212223cat &gt; deployment-pc.yml &lt;&lt;-'EOF'apiVersion: apps/v1kind: Deploymentmetadata: name: high-priority-testspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: priorityClassName: high-priority containers: - name: nginx image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80EOF 创建此deployment，并观察deployment产生的pod是否具有特定的值 12root@k8s-master:~# kubectl apply -f deployment-pc.ymldeployment.apps/high-priority-test created 查看有没有这个值 12root@k8s-master:~# kubectl get deployments.apps high-priority-test -o yaml | grep priorityClassName: priorityClassName: high-priority 123456789root@k8s-master:~# kubectl get podNAME READY STATUS RESTARTS AGEhigh-priority-test-5ddcb8559c-pskbm 1/1 Running 0 2m59shigh-priority-test-5ddcb8559c-vlsw7 1/1 Running 0 2m59shigh-priority-test-5ddcb8559c-wmxrl 1/1 Running 0 2m59sroot@k8s-master:~# kubectl get pod high-priority-test-5ddcb8559c-pskbm -o yaml | grep priorityClassName: priorityClassName: high-priority ok，已经成功应用了特定的priorityClass ConfigMapsYAML文件创建在Data节点创建了一些键值 123456789101112cat &gt; cmyaml.yml &lt;&lt;EOFapiVersion: v1kind: ConfigMapmetadata: name: game-demodata: player_initial_lives: &quot;3&quot; ui_properties_file_name: &quot;lixiaohui&quot; game.properties: | enemy.types=aliens,monsters player.maximum-lives=5EOF 12kubectl create -f cmyaml.yml kubectl get configmaps 123NAME DATA AGEgame-demo 3 18skube-root-ca.crt 1 49d 1kubectl describe configmaps game-demo 1234567891011121314151617181920212223Name: game-demoNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====game.properties:----enemy.types=aliens,monstersplayer.maximum-lives=5player_initial_lives:----3ui_properties_file_name:----lixiaohuiBinaryData====Events: &lt;none&gt; 命令行创建创建了一个名为lixiaohui的键值 12kubectl create configmap lixiaohui --from-literal=username=lixiaohui --from-literal=age=18kubectl describe configmaps lixiaohui 123456789101112131415161718Name: lixiaohuiNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====age:----18username:----lixiaohuiBinaryData====Events: &lt;none&gt; 创建了一个index.html文件，然后用–from-file来引用 123echo hello world &gt; index.htmlkubectl create configmap indexcontent --from-file=index.htmlkubectl describe configmaps indexcontent 12345678910111213141516Name: indexcontentNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====index.html:----hello worldBinaryData====Events: &lt;none&gt; Volume 挂载ConfigMap创建一个Pod，其挂载的内容，将来自于我们的configmap 1234567891011121314151617181920cat &gt; cmvolume.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: configmapvolume labels: app: configmaptestspec: containers: - name: test image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent volumeMounts: - name: index mountPath: /usr/local/apache2/htdocs volumes: - name: index configMap: name: indexcontentEOF 12kubectl create -f cmvolume.yml kubectl get -f cmvolume.yml -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESconfigmapvolume 1/1 Running 0 63s 172.16.200.237 k8s-worker1 &lt;none&gt; &lt;none&gt; 12curl http://172.16.200.237hello world 1kubectl delete -f cmvolume.yml 环境变量ConfigMap创建一个名为mysqlpass且包含password=ABCabc123的configmap 1kubectl create configmap mysqlpass --from-literal=password=ABCabc123 1234567891011121314151617cat &gt; cmenv.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: mysqlspec: containers: - name: mysqlname image: class-docker.myk8s.cn/library/mysql imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD valueFrom: configMapKeyRef: name: mysqlpass key: passwordEOF 我们用环境变量的方法引用了configmap 12kubectl create -f cmenv.yml kubectl exec -it mysql -- mysql -uroot -pABCabc123 123456789101112131415mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 9Server version: 8.0.28 MySQL Community Server - GPLCopyright (c) 2000, 2022, Oracle and/or its affiliates.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; exitBye 此时我们要注意，用configMap来引用密码是不太靠谱的，通常用于配置文件等明文场景，密文应该使用下一个实验的secret，因为configMap是明文的，见如下示例 1kubectl describe configmaps mysqlpass 123456789101112131415Name: mysqlpassNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====password:----ABCabc123BinaryData====Events: &lt;none&gt; 1kubectl delete -f cmenv.yml Secrets刚才我们说过，用configMaps不方便存储密码类的敏感信息，此时我们可以改用Secret 命令行创建1kubectl create secret generic mysqlpass --from-literal=password=ABCabc123 查看时，会发现已经加密 1kubectl describe secrets mysqlpass 12345678910Name: mysqlpassNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: OpaqueData====password: 9 bytes 环境变量Secret使用刚才创建的密码，创建Pod并进行尝试 1234567891011121314151617cat &gt; stenv.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: mysql-secretspec: containers: - name: mysqlname image: class-docker.myk8s.cn/library/mysql imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysqlpass key: passwordEOF 1kubectl create -f stenv.yml 1kubectl exec -it mysql-secret -- mysql -uroot -pABCabc123 123456789101112131415mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.28 MySQL Community Server - GPLCopyright (c) 2000, 2022, Oracle and/or its affiliates.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; exit;Bye 1kubectl delete -f stenv.yml 资源配额Pod 资源配额内存申请64Mi，CPU申请100m，上限为内存128Mi，CPU100m 1234567891011121314151617181920cat &gt; quota.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: frontend labels: name: frontendspec: containers: - name: app image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent resources: requests: memory: &quot;64Mi&quot; cpu: &quot;100m&quot; limits: memory: &quot;128Mi&quot; cpu: &quot;100m&quot;EOF 1个逻辑CPU等于1000m，如果不带单位就是核心数，可以带小数点，例如0.1。内存的单位：100M等于100 * 1000，100Mi等于100 * 1024 12kubectl create -f quota.ymlkubectl describe -f quota.yml | grep -A 5 Limits 123456Limits: cpu: 100m memory: 128MiRequests: cpu: 100m memory: 64Mi 1kubectl delete -f quota.yml NameSpace 资源配额新建namespace 1kubectl create namespace test 在ResourceQuota中，requests.cpu: “1” 就代表1000m，requests.memory后面，如果只是一个数字，没有Mi等单位时，默认使用的是字节单位 1234567891011121314cat &gt; nmquota.yml &lt;&lt;EOFapiVersion: v1kind: ResourceQuotametadata: name: lixiaohuiquota namespace: testspec: hard: pods: &quot;1&quot; requests.cpu: &quot;1&quot; requests.memory: &quot;1&quot; limits.cpu: &quot;2&quot; limits.memory: &quot;2Gi&quot;EOF 1kubectl create -f nmquota.yml 新建一个Pod尝试申请资源 12345678910111213141516171819cat &gt; nmpod.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: frontend namespace: testspec: containers: - name: app image: class-docker.myk8s.cn/library/nginx imagePullPolicy: IfNotPresent resources: requests: memory: &quot;64Mi&quot; cpu: &quot;15000m&quot; limits: memory: &quot;128Mi&quot; cpu: &quot;15000m&quot;EOF 我们发现由于限制无法申请成功 1kubectl create -f nmpod.yml 1Error from server (Forbidden): error when creating &quot;nmpod.yml&quot;: pods &quot;frontend&quot; is forbidden: exceeded quota: lixiaohuiquota, requested: limits.cpu=15,requests.cpu=15,requests.memory=64Mi, used: limits.cpu=0,requests.cpu=0,requests.memory=0, limited: limits.cpu=2,requests.cpu=1,requests.memory=1 访问控制ServiceAaccount在一个名为test的namespace中，创建一个名为lixiaohui的ServiceAccount 123kubectl create namespace testkubectl -n test create serviceaccount lixiaohuikubectl -n test get serviceaccounts lixiaohui 12NAME SECRETS AGElixiaohui 0 63s 1kubectl -n test describe serviceaccounts lixiaohui 12345678Name: lixiaohuiNamespace: testLabels: &lt;none&gt;Annotations: &lt;none&gt;Image pull secrets: &lt;none&gt;Mountable secrets: &lt;none&gt;Tokens: &lt;none&gt;Events: &lt;none&gt; Role和ClusterRole在名为test的namespace中创建一个名为test-role的角色，以及创建一个名为test-clusterrole的集群角色 创建一个名为test-role仅有查看pod的角色 命令行方法 1kubectl -n test create role --resource=pod --verb=get test-role 1kubectl -n test delete role test-role YAML 方法 1234567891011121314cat &gt; role.yml &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: test-role namespace: testrules:- apiGroups: - &quot;&quot; resources: - pods verbs: - getEOF 12kubectl create -f role.ymlkubectl describe role -n test test-role 1234567Name: test-roleLabels: &lt;none&gt;Annotations: &lt;none&gt;PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get] 将上述创建的lixiaohui服务账号和本role绑定 12kubectl -n test create rolebinding --role=test-role --serviceaccount=test:lixiaohui lixiaohui-bindingkubectl -n test describe rolebinding lixiaohui-binding 12345678910Name: lixiaohui-bindingLabels: &lt;none&gt;Annotations: &lt;none&gt;Role: Kind: Role Name: test-roleSubjects: Kind Name Namespace ---- ---- --------- ServiceAccount lixiaohui test 测试权限 1234kubectl -n test auth can-i create pods --as=system:serviceaccount:test:lixiaohuinokubectl -n test auth can-i get pods --as=system:serviceaccount:test:lixiaohuiyes ClusterRole 创建一个名为test-clusterrole仅有创建pod和deployment的角色 命令行创建 12kubectl create clusterrole --resource=pod,deployment --verb=create test-clusterrolekubectl describe clusterrole test-clusterrole 12345678Name: test-clusterroleLabels: &lt;none&gt;Annotations: &lt;none&gt;PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [create get] deployments.apps [] [] [create get] 1kubectl delete clusterrole test-clusterrole YAML 文件创建 123456789101112131415161718192021cat &gt; clusterrole.yml &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: test-clusterrolerules:- apiGroups: - &quot;&quot; resources: - pods verbs: - create - get- apiGroups: - apps resources: - deployments verbs: - create - getEOF 12kubectl create -f clusterrole.ymlkubectl describe clusterrole test-clusterrole 12345678Name: test-clusterroleLabels: &lt;none&gt;Annotations: &lt;none&gt;PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [create get] deployments.apps [] [] [create get] 将lixiaohui用户和clusterrole绑定，并测试权限 12kubectl create clusterrolebinding --clusterrole=test-clusterrole --serviceaccount=test:lixiaohui lixiaohui-clusterbindkubectl describe clusterrolebinding lixiaohui-clusterbind 12345678910Name: lixiaohui-clusterbindLabels: &lt;none&gt;Annotations: &lt;none&gt;Role: Kind: ClusterRole Name: test-clusterroleSubjects: Kind Name Namespace ---- ---- --------- ServiceAccount lixiaohui test 1234567891011121314kubectl auth can-i get pods --as=system:serviceaccount:test:lixiaohuiyeskubectl auth can-i create pods --as=system:serviceaccount:test:lixiaohuiyeskubectl auth can-i create deployments --as=system:serviceaccount:test:lixiaohuiyeskubectl auth can-i create secret --as=system:serviceaccount:test:lixiaohuinokubectl auth can-i create service --as=system:serviceaccount:test:lixiaohuino 网络策略在名为zhangsan的namespace中，创建一个仅允许来自名为lixiaohui的namespace连接的网络策略 创建两个namesapce 12kubectl create namespace zhangsankubectl create namespace lixiaohui 在zhangsan的namespace中，新建一个pod 123456789101112131415161718cat &gt; nppod.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: pod namespace: zhangsan labels: app: httpdspec: containers: - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent ports: - name: web containerPort: 80 restartPolicy: OnFailureEOF 12kubectl create -f nppod.ymlkubectl get pod -n zhangsan -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod 1/1 Running 0 83s 172.16.152.73 k8s-worker2 &lt;none&gt; &lt;none&gt; 在lixiaohui的namespace中，新建一个pod 123456789101112131415161718cat &gt; nppod1.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: pod1 namespace: lixiaohui labels: app: nginxspec: containers: - name: httpd image: class-docker.myk8s.cn/library/httpd imagePullPolicy: IfNotPresent ports: - name: web containerPort: 80 restartPolicy: OnFailureEOF 12kubectl create -f nppod1.ymlkubectl get pod -n lixiaohui -o wide 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod1 1/1 Running 0 14s 172.16.152.74 k8s-worker2 &lt;none&gt; &lt;none&gt; 新建网络策略 12345678910111213141516171819cat &gt; np.yml &lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-from-namesapce-lixiaohui namespace: zhangsanspec: podSelector: {} policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: lixiaohui ports: - protocol: TCP port: 80EOF 1kubectl create -f np.yml 测试效果 1234567891011121314151617cat &gt; nptest.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: pod-defaultspec: containers: - name: busybox image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: - /bin/sh - -c - &quot;sleep 10m&quot; restartPolicy: OnFailureEOFkubectl create -f nptest.yml 从default namespace中访问没有被网络策略选中的pod，发现成功访问 1kubectl exec -it pod-default -- wget 172.16.152.74 1234Connecting to 172.16.152.74 (172.16.152.74:80)saving to 'index.html'index.html 100% |********************************| 45 0:00:00 ETA'index.html' saved 从default namespace中访问被网络策略选中的pod，发现无法访问 1kubectl exec -it pod-default -- wget 172.16.152.73 1Connecting to 172.16.152.73 (172.16.152.73:80) 新建一个lixiaohui namespace的pod，测试是否可以访问被隔离的pod，由于网络策略的原因，一定是可以访问的 123456789101112131415161718cat &gt; nplixiaohuitest.yml &lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: pod-lixiaohui-test namespace: lixiaohuispec: containers: - name: busybox image: class-docker.myk8s.cn/library/busybox imagePullPolicy: IfNotPresent command: - /bin/sh - -c - &quot;sleep 10m&quot; restartPolicy: OnFailureEOFkubectl create -f nplixiaohuitest.yml 以下测试中，发现可以正常访问zhangsan namesapce中的pod 1kubectl -n lixiaohui exec -it pod-lixiaohui-test -- wget 172.16.152.73 123saving to 'index.html'index.html 100% |********************************| 45 0:00:00 ETA'index.html' saved Helm 概念与实践Helm基本概念Helm 是 Kubernetes 的包管理工具，它能够简化 Kubernetes 应用程序的部署和管理。在 Kubernetes 中，一个应用程序可能由多个资源组成，比如 Deployment、Service、ConfigMap、Ingress 等。Helm 可以将这些资源打包成一个 Helm Chart（包），方便用户进行安装、升级、回滚和删除等操作，就和系统里的YUM/DNF/APT非常的类似。 主要组件 Helm Client 这是用户与 Helm 交互的客户端工具。用户通过在本地机器上运行 Helm 命令来操作 Helm Chart，例如安装、升级、查询等操作。它会直接与 Kubernetes API 服务器交互，客户端可以直接从它的官方GitHub下载，地址是：https://github.com/helm/helm/releases Helm Chart Helm Chart 是一个打包的 Kubernetes 应用程序，包含所有必要的资源定义和配置文件。它类似于软件包管理工具中的“包”，用于简化 Kubernetes 应用的部署和管理。一个 Helm Chart 包含以下部分： Chart.yaml： 定义：这是 Helm Chart 的元数据文件，包含 Chart 的基本信息。大概内容： name：Chart 的名称。 version：Chart 的版本号（遵循语义化版本规范）。 description：Chart 的简要描述。 dependencies：依赖的其他 Chart（可选）。 values.yaml：它是 Chart 的默认配置文件。用户可以通过修改这个文件或者在安装时覆盖其中的值来定制 Chart 的行为。比如，它可能包含应用的副本数量、镜像版本、资源限制等配置项。 templates/：包含 Kubernetes 资源的模板文件，这些模板文件在安装或升级时会被渲染成实际的 Kubernetes 资源，渲染的时候，将用values.yaml里的值替换掉templates模板里的变量，例如，一个 Deployment 的模板文件可以根据 values.yaml 中定义的副本数量和镜像版本来生成对应的 Deployment 资源，具体渲染的命令是：helm template，可以用–help等方式获取语法，不过需要注意的是，这会渲染出实际的 Kubernetes 资源文件，但不会实际部署到集群中。 charts/：如果当前 Chart 依赖其他 Chart，这些依赖的 Chart 会存放在这个目录下。这样可以实现 Chart 之间的组合和复用。 Helm Repository（仓库） 它是一个存储 Helm Chart 的仓库，可以是本地的，也可以是远程的。远程仓库通常是一个 HTTP 服务器，用户可以从仓库中搜索、下载和更新 Helm Chart。例如，Helm 官方维护了一个默认的公共仓库，里面包含了许多常用的 Helm Chart，用户可以通过 Helm 命令将这些 Chart 添加到本地的 Chart 仓库列表中，然后进行安装等操作。 官方网址 http://helm.sh Helm安装下载安装Helm 1234wget https://get.helm.sh/helm-v3.17.3-linux-amd64.tar.gztar xf helm-v3.17.3-linux-amd64.tar.gzmv linux-amd64/helm /usr/local/bin/helmhelm completion bash &gt; /etc/bash_completion.d/helm 默认情况下，helm内置了一个hub，用于软件搜索和安装，搜索软件是否可被安装，用以下格式命令： 1helm search hub Packages 命令行显示有点奇怪，也不够丰富，可以考虑用浏览器打开搜索：https://hub.helm.sh 添加仓库官方仓库网速慢，可以考虑一下以下仓库或自己部署仓库 1http://mirror.azure.cn/kubernetes/charts/ 添加方式 1helm repo add azurerepo http://mirror.azure.cn/kubernetes/charts/ Helm 实验案例wordpress安装手工定制安装本次安装一个wordpress 1helm search repo wordpress 12NAME CHART VERSION APP VERSION DESCRIPTIONazurerepo/wordpress 9.0.3 5.3.2 DEPRECATED Web publishing platform for building... ok，发现一个wordpress包，那这个包里有什么？把包下出来研究研究 123root@k8s-master:~# helm pull azurerepo/wordpressroot@k8s-master:~# lswordpress-9.0.3.tgz 下完是一个压缩包，解压看看 123456root@k8s-master:~# tar xf wordpress-9.0.3.tgzroot@k8s-master:~# lswordpress wordpress-9.0.3.tgzroot@k8s-master:~# cd wordpress/root@k8s-master:~/wordpress# lscharts Chart.yaml README.md requirements.lock requirements.yaml templates values.schema.json values.yaml 果然看到了Chart.yaml templates values.yaml 看看都有哪些模板 12root@k8s-master:~/wordpress# ls templates/deployment.yaml externaldb-secrets.yaml _helpers.tpl ingress.yaml NOTES.txt pvc.yaml secrets.yaml servicemonitor.yaml svc.yaml tests tls-secrets.yaml 模板里面都是各种变量，稍后需要用values.yml来填充 1234567891011root@k8s-master:~/wordpress# head templates/deployment.yamlapiVersion: {{ template &quot;wordpress.deployment.apiVersion&quot; . }}kind: Deploymentmetadata: name: {{ template &quot;wordpress.fullname&quot; . }} labels: {{- include &quot;wordpress.labels&quot; . | nindent 4 }}spec: selector: matchLabels: {{- include &quot;wordpress.matchLabels&quot; . | nindent 6 }} {{- if .Values.updateStrategy }} strategy: {{ toYaml .Values.updateStrategy | nindent 4 }} ok，模板看到了，我们试试用values.yml里的变量来渲染模板，生成最终的yaml文件 1root@k8s-master:~/wordpress# helm template --version 9.0.3 azurerepo/wordpress 从仓库中读出values.yml，将templates目录里的模板渲染成功 1234567891011121314151617181920---# Source: wordpress/charts/mariadb/templates/secrets.yamlapiVersion: v1kind: Secretmetadata: name: release-name-mariadb labels: app: &quot;mariadb&quot; chart: &quot;mariadb-7.3.12&quot; release: &quot;release-name&quot; heritage: &quot;Helm&quot;type: Opaquedata: mariadb-root-password: &quot;QmhiblBvMXJmZQ==&quot; mariadb-password: &quot;VWZxWm5CbnVicA==&quot;---# Source: wordpress/templates/secrets.yamlapiVersion: v1kind: Secret 如果有一个值不满意怎么办？可以手工指定某一个参数，或者干脆用本地的values.yaml 先试试手工指定，我们看了一下仓库里的value，要用这个镜像，那我来随便改一个看看 123456789101112root@k8s-master:~/wordpress# helm show values azurerepo/wordpress | grep ^image: -A 10image: registry: docker.io repository: bitnami/wordpress tag: 5.3.2-debian-10-r32 ## Specify a imagePullPolicy ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent' ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. 渲染时手工指定某个参数，比如替换一个镜像的registry 我通过–set参数指定了镜像的registry，并且生成了模板 12345root@k8s-master:~/wordpress# helm template --version 9.0.3 azurerepo/wordpress --set image.registry=linuxcenter.cn &gt; custom.ymlWARNING: This chart is deprecatedroot@k8s-master:~/wordpress# grep linuxcenter custom.yml image: linuxcenter.cn/bitnami/wordpress:5.3.2-debian-10-r32 image: linuxcenter.cn/bitnami/wordpress:5.3.2-debian-10-r32 当然，我上面的set是瞎写的，镜像仓库不存在，只是为了演示参数而已 我们重新用默认参数渲染出最终的yaml，然后向集群部署 这一步将安装mariadb和wordpress，我们这里用的是存储章节设置的默认存储类，所以会自动创建pv以及pvc，你要是还没有默认存储类，往上翻，重新做一下存储类并标记为默认即可 1234567891011121314root@k8s-master:~/wordpress# helm template --version 9.0.3 azurerepo/wordpress &gt; lixiaohui.ymlWARNING: This chart is deprecatedroot@k8s-master:~/wordpress# kubectl apply -f lixiaohui.ymlsecret/release-name-mariadb createdsecret/release-name-wordpress createdconfigmap/release-name-mariadb createdconfigmap/release-name-mariadb-tests createdpersistentvolumeclaim/release-name-wordpress createdservice/release-name-mariadb createdservice/release-name-wordpress createddeployment.apps/release-name-wordpress createdstatefulset.apps/release-name-mariadb createdpod/release-name-mariadb-test-8u7uw createdpod/release-name-credentials-test created 不需要看到pod工作正常，我们只研究helm自身，如果你需要它工作正常，需要搞定镜像、机器内存需要再加一下，不然数据库起不来 以上就是研究的过程，其实一般来说，不用这么麻烦，比如我们最后实际上就是用的默认value，如果默认value你就满意，可以用下面的方法直接让helm来管理，我们不用改东西，如果你并不是用的helm install，后面就不受helm管理，因为当你使用 helm template 命令生成 Kubernetes 资源清单文件并手动将其部署到集群中时，这些资源不会被 Helm 的生命周期管理所跟踪。因此，当你运行 helm list 命令时，不会看到任何相关的 Helm Release，因为 Helm 的 Release 管理机制没有被触发。 helm仓库直接安装这一步将安装mariadb和wordpress，我们这里用的是存储章节设置的默认存储类，所以会自动创建pv以及pvc，你要是还没有默认存储类，往上翻，重新做一下存储类并标记为默认即可 helm install 也是只是--set参数的，不一定非要用默认的值 1234567root@k8s-master:~/wordpress# helm install wordpress azurerepo/wordpressWARNING: This chart is deprecatedNAME: wordpressLAST DEPLOYED: Thu Apr 24 13:10:09 2025NAMESPACE: defaultSTATUS: deployedREVISION: 1 123root@k8s-master:~/wordpress# helm listNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONwordpress default 1 2025-04-24 13:10:09.708968227 +0800 CST deployed wordpress-9.0.3 5.3.2 ok，安装完毕，现在来看看这个服务怎么样了 查询服务端口 1kubectl get service 123NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEwordpress LoadBalancer 10.102.156.226 &lt;pending&gt; 80:31194/TCP,443:30386/TCP 3m42swordpress-mariadb ClusterIP 10.106.8.85 &lt;none&gt; 3306/TCP 3m42s 查询pod所在节点 1root@k8s-master:~# kubectl get pod -o wide 不需要看到pod工作正常，我们只研究helm自身，如果你需要它工作正常，需要搞定镜像、机器内存需要再加一下，不然数据库起不来 k8s Dashboard安装这是可选实验，不管你这个实验做的怎么样，为了安全，都不要在任何生产环境中使用dashboard 添加helm仓库 12cdhelm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ 不过你够呛能连上，用我的仓库加个速吧 12cdhelm repo add kubernetes-dashboard https://oss.linuxcenter.cn/files/cka/helm 部署dashboard 1helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard 输出 123456789101112131415161718192021222324Release &quot;kubernetes-dashboard&quot; does not exist. Installing it now.NAME: kubernetes-dashboardLAST DEPLOYED: Tue Apr 22 17:06:51 2025NAMESPACE: kubernetes-dashboardSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:**************************************************************************************************** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ****************************************************************************************************Congratulations! You have just installed Kubernetes Dashboard in your cluster.To access Dashboard run: kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443NOTE: In case port-forward command does not work, make sure that kong service name is correct. Check the services in Kubernetes Dashboard namespace using: kubectl -n kubernetes-dashboard get svcDashboard will be available at: https://localhost:8443 看看都部署了什么东西？ 1234567891011121314151617181920212223242526272829root@k8s-master:~# kubectl get all -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/kubernetes-dashboard-api-57fff658db-v78ls 0/1 ContainerCreating 0 14spod/kubernetes-dashboard-auth-6d58f47489-dt82g 0/1 ContainerCreating 0 14spod/kubernetes-dashboard-kong-79867c9c48-hhxjs 0/1 Init:0/1 0 14spod/kubernetes-dashboard-metrics-scraper-76df4956c4-5jhhw 0/1 ContainerCreating 0 14spod/kubernetes-dashboard-web-56df7655d9-zgd5m 0/1 ContainerCreating 0 14sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes-dashboard-api ClusterIP 10.96.72.242 &lt;none&gt; 8000/TCP 14sservice/kubernetes-dashboard-auth ClusterIP 10.110.166.42 &lt;none&gt; 8000/TCP 14sservice/kubernetes-dashboard-kong-proxy ClusterIP 10.105.29.218 &lt;none&gt; 443/TCP 14sservice/kubernetes-dashboard-metrics-scraper ClusterIP 10.100.78.233 &lt;none&gt; 8000/TCP 14sservice/kubernetes-dashboard-web ClusterIP 10.103.183.178 &lt;none&gt; 8000/TCP 14sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/kubernetes-dashboard-api 0/1 1 0 14sdeployment.apps/kubernetes-dashboard-auth 0/1 1 0 14sdeployment.apps/kubernetes-dashboard-kong 0/1 1 0 14sdeployment.apps/kubernetes-dashboard-metrics-scraper 0/1 1 0 14sdeployment.apps/kubernetes-dashboard-web 0/1 1 0 14sNAME DESIRED CURRENT READY AGEreplicaset.apps/kubernetes-dashboard-api-57fff658db 1 1 0 14sreplicaset.apps/kubernetes-dashboard-auth-6d58f47489 1 1 0 14sreplicaset.apps/kubernetes-dashboard-kong-79867c9c48 1 1 0 14sreplicaset.apps/kubernetes-dashboard-metrics-scraper-76df4956c4 1 1 0 14sreplicaset.apps/kubernetes-dashboard-web-56df7655d9 1 1 0 14s 好好好，在中国又遇到了镜像无法下载的坑，看看都有哪些镜像？ 1234567root@k8s-master:~# kubectl get deployments.apps -n kubernetes-dashboard -o yaml | grep image: image: docker.io/kubernetesui/dashboard-api:1.12.0 image: docker.io/kubernetesui/dashboard-auth:1.2.4 image: kong:3.8 image: kong:3.8 image: docker.io/kubernetesui/dashboard-metrics-scraper:1.2.2 image: docker.io/kubernetesui/dashboard-web:1.6.2 在所有的节点上都准备一下镜像 123456789101112131415161718docker pull class-docker.myk8s.cn/kubernetesui/dashboard-api:1.12.0docker pull class-docker.myk8s.cn/kubernetesui/dashboard-auth:1.2.4docker pull class-docker.myk8s.cn/library/kong:3.8docker pull class-docker.myk8s.cn/kubernetesui/dashboard-metrics-scraper:1.2.2docker pull class-docker.myk8s.cn/kubernetesui/dashboard-web:1.6.2docker tag class-docker.myk8s.cn/kubernetesui/dashboard-api:1.12.0 docker.io/kubernetesui/dashboard-api:1.12.0docker tag class-docker.myk8s.cn/kubernetesui/dashboard-auth:1.2.4 docker.io/kubernetesui/dashboard-auth:1.2.4docker tag class-docker.myk8s.cn/library/kong:3.8 kong:3.8docker tag class-docker.myk8s.cn/kubernetesui/dashboard-metrics-scraper:1.2.2 docker.io/kubernetesui/dashboard-metrics-scraper:1.2.2docker tag class-docker.myk8s.cn/kubernetesui/dashboard-web:1.6.2 docker.io/kubernetesui/dashboard-web:1.6.2 docker rmi class-docker.myk8s.cn/kubernetesui/dashboard-api:1.12.0docker rmi class-docker.myk8s.cn/kubernetesui/dashboard-auth:1.2.4docker rmi class-docker.myk8s.cn/library/kong:3.8docker rmi class-docker.myk8s.cn/kubernetesui/dashboard-metrics-scraper:1.2.2docker rmi class-docker.myk8s.cn/kubernetesui/dashboard-web:1.6.2 重启deployment 123456root@k8s-master:~# kubectl get deployments.apps -n kubernetes-dashboard -o name | xargs kubectl rollout restart -n kubernetes-dashboarddeployment.apps/kubernetes-dashboard-api restarteddeployment.apps/kubernetes-dashboard-auth restarteddeployment.apps/kubernetes-dashboard-kong restarteddeployment.apps/kubernetes-dashboard-metrics-scraper restarteddeployment.apps/kubernetes-dashboard-web restarted 看看pod状态 1234567root@k8s-master:~# kubectl get pod -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEkubernetes-dashboard-api-55bdb6859-s6fzw 1/1 Running 0 9m38skubernetes-dashboard-auth-5ffbf7cb46-bqj6c 1/1 Running 0 9m38skubernetes-dashboard-kong-5bd569b997-74klx 1/1 Running 0 9m37skubernetes-dashboard-metrics-scraper-7cfccb7644-5t9rt 1/1 Running 0 9m37skubernetes-dashboard-web-6c67987c9f-c7ssl 1/1 Running 0 9m37s 看到kong-proxy是ClusterIP，不方便查看，直接改为nodeport，就可以用任何节点的ip打开查看了 1234root@k8s-master:~# kubectl edit service -n kubernetes-dashboard kubernetes-dashboard-kong-proxy...spec: type: NodePort 看看我们的端口多少？ 1234567root@k8s-master:~# kubectl get service -n kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard-api ClusterIP 10.96.72.242 &lt;none&gt; 8000/TCP 17mkubernetes-dashboard-auth ClusterIP 10.110.166.42 &lt;none&gt; 8000/TCP 17mkubernetes-dashboard-kong-proxy NodePort 10.105.29.218 &lt;none&gt; 443:31000/TCP 17mkubernetes-dashboard-metrics-scraper ClusterIP 10.100.78.233 &lt;none&gt; 8000/TCP 17mkubernetes-dashboard-web ClusterIP 10.103.183.178 &lt;none&gt; 8000/TCP 17m 比如我在浏览器输入https://192.168.8.3:31000 创建一个用户登录的超级用户，并创建长期有效的token 12345678910111213141516171819202122232425262728293031cat &gt; create-dash-user.yml &lt;&lt;-'EOF'---apiVersion: v1kind: ServiceAccountmetadata: name: lxh-dash namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: lxh-dash-cluster-adminroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: lxh-dash namespace: kubernetes-dashboard---apiVersion: v1kind: Secretmetadata: name: lxh-dash-secret namespace: kubernetes-dashboard annotations: kubernetes.io/service-account.name: &quot;lxh-dash&quot;type: kubernetes.io/service-account-tokenEOF 1kubectl create -f create-dash-user.yml 获取登录token 1kubectl get secret lxh-dash-secret -n kubernetes-dashboard -o jsonpath={&quot;.data.token&quot;} | base64 -d 除了bash的root@master:~#提示符之外，把所有的内容复制一下，然后粘贴到登录框里并点击登录 监控与升级部署Metrics1kubectl apply -f https://www.linuxcenter.cn/files/cka/cka-yaml/metrics-components.yaml 也需要解决镜像的问题，记得在所有机器上都准备一下 12kubectl get deployments.apps -n kube-system metrics-server -o yaml | grep image: image: registry.k8s.io/metrics-server/metrics-server:v0.7.2 123docker pull class-docker.myk8s.cn/metrics-server/metrics-server:v0.7.2docker tag class-docker.myk8s.cn/metrics-server/metrics-server:v0.7.2 registry.k8s.io/metrics-server/metrics-server:v0.7.2docker rmi class-docker.myk8s.cn/metrics-server/metrics-server:v0.7.2 重启deployment触发部署 12root@k8s-master:~# kubectl rollout restart deployment -n kube-system metrics-serverdeployment.apps/metrics-server restarted 部署好之后，执行kubectl top 命令时就会返回结果了 1kubectl top nodes 1234NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master 142m 3% 1725Mi 45% k8s-worker1 150m 3% 1024Mi 27% k8s-worker2 53m 1% 995Mi 26% 1kubectl top pod 1234NAME CPU(cores) MEMORY(bytes) lixiaohui-mq4sp 0m 0Mi lixiaohui-qjlwt 0m 0Mi lixiaohui-sm6pp 0m 0Mi HPA 自动扩容在部署了metrics server后，可以给HPA提供自动扩容的资源用量指标，帮助HPA快速识别用量，并完成自动扩容 先创建一个deployment以及上层的访问service 1234567891011121314151617181920212223242526272829303132333435363738cat &gt; hpatest.yml &lt;&lt;-EOFapiVersion: apps/v1kind: Deploymentmetadata: name: php-apachespec: selector: matchLabels: run: php-apache template: metadata: labels: run: php-apache spec: containers: - name: php-apache image: class-docker.myk8s.cn/hpa-example imagePullPolicy: IfNotPresent ports: - containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m---apiVersion: v1kind: Servicemetadata: name: php-apache labels: run: php-apachespec: ports: - port: 80 selector: run: php-apacheEOF 根据以上代码，deployment创建了一个pod副本，service提供了一个名为php-apache的访问点 先在所有机器上下载一下镜像 1docker pull class-docker.myk8s.cn/hpa-example 创建后，来测试一下pod和service本身是否可以访问 1kubectl create -f hpatest.yml 1kubectl get deployments.apps 输出 12NAME READY UP-TO-DATE AVAILABLE AGEphp-apache 1/1 1 1 21s 1kubectl get service 输出 123NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 25dphp-apache ClusterIP 10.110.249.4 &lt;none&gt; 80/TCP 53m 1kubectl get pod -o wide 输出 12NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESphp-apache-5db7b77785-6j9lj 1/1 Running 0 53m 172.16.194.66 k8s-worker1 &lt;none&gt; &lt;none&gt; 12curl 172.16.194.66OK! 1curl 10.110.249.4 输出 1OK! 经过测试，pod和service都可以访问，现在来做一个CPU利用率大于50，就扩容的例子 需要注意的是，我们这里为了规避瞬时高峰的波动，给扩容和缩容设置了稳定的时间窗口 扩容场景： 当 CPU 使用率超过目标值（50%）时，HPA 会检测到需要扩容。 但因为设置了 stabilizationWindowSeconds: 10，HPA 会等待 10 秒，观察这段时间内的 CPU 使用率是否持续高于目标值。 如果在这 10 秒内，CPU 使用率一直高于 50%，HPA 会执行扩容操作。 如果在这 10 秒内，CPU 使用率恢复正常（低于 50%），HPA 则不会执行扩容操作。 缩容场景： 当 CPU 使用率低于目标值（50%）时，HPA 会检测到需要缩容。 但因为设置了 stabilizationWindowSeconds: 30，HPA 会等待 30 秒，观察这段时间内的 CPU 使用率是否持续低于目标值。 如果在这 30 秒内，CPU 使用率一直低于 50%，HPA 会执行缩容操作。 如果在这 30 秒内，CPU 使用率恢复正常（高于 50%），HPA 则不会执行缩容操作。 1234567891011121314151617181920212223242526cat &gt; hpa.yml &lt;&lt;-'EOF'apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata: name: php-apache namespace: defaultspec: maxReplicas: 10 metrics: - resource: name: cpu target: averageUtilization: 50 type: Utilization type: Resource minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache behavior: scaleUp: stabilizationWindowSeconds: 10 scaleDown: stabilizationWindowSeconds: 30EOF 1root@k8s-master:~# kubectl apply -f hpa.yml 稍等一分钟，执行以下命令查看实时CPU利用率和目标的差距，并观察副本数 1kubectl get hpa 输出 12NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEphp-apache Deployment/php-apache cpu: 0%/50% 1 10 1 31s 启动一个不同的 Pod 作为客户端。 客户端 Pod 中的容器在无限循环中运行，向 php-apache 服务发送查询。 不断的访问，将产生压力，压力大了，hpa就会扩容 需要注意的是，需要先在所有节点上下载busybox镜像 1kubectl run -i --tty load-generator --rm --image=class-docker.myk8s.cn/library/busybox:1.28 --restart=Never -- /bin/sh -c &quot;while sleep 0.01; do wget -q -O- http://php-apache; done&quot; 运行后会不断的输出ok，不要结束输出，开一个新的ssh会话，运行以下命令，观察hpa收集到的信息以及pod的扩容情况 1kubectl get hpa php-apache --watch 输出 123NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEphp-apache Deployment/php-apache cpu: 231%/50% 1 10 1 10mphp-apache Deployment/php-apache cpu: 250%/50% 1 10 4 10m 此时按下CTRL C结束ok的输出，再次观察target的利用率下降 1kubectl get hpa php-apache --watch 输出 123456789NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEphp-apache Deployment/php-apache cpu: 231%/50% 1 10 1 10mphp-apache Deployment/php-apache cpu: 250%/50% 1 10 4 10mphp-apache Deployment/php-apache cpu: 144%/50% 1 10 5 10mphp-apache Deployment/php-apache cpu: 72%/50% 1 10 5 11mphp-apache Deployment/php-apache cpu: 68%/50% 1 10 5 11mphp-apache Deployment/php-apache cpu: 67%/50% 1 10 7 11mphp-apache Deployment/php-apache cpu: 30%/50% 1 10 7 11mphp-apache Deployment/php-apache cpu: 0%/50% 1 10 7 12m 部署Prometheus手工部署较为复杂，我们采用operator进行部署，先克隆它的operator，本次采用的是0.14.0版本 1234wget https://www.linuxcenter.cn/files/cka/kube-prometheus-0.14.0.tartar xf kube-prometheus-0.14.0.tarcd kube-prometheus-0.14.0kubectl create -f manifests/setup/ 测试是否符合条件，如果上一步全部成功，这里会显示全部符合 1kubectl wait --for condition=Established --all CustomResourceDefinition --namespace=monitoring 在master上自动给所有节点准备容器镜像，此处会产生大量网络流量，需要较长时间，请耐心等待 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116cat &gt; dockerimage &lt;&lt;'EOF'#!/bin/bashfunction sshcmd { sshpass -p vagrant ssh root@$1 $2}echoecho Pulling images on $(hostname)echodocker pull class-docker.myk8s.cn/prometheus/alertmanager:v0.27.0docker pull class-docker.myk8s.cn/prometheus/blackbox-exporter:v0.25.0docker pull class-docker.myk8s.cn/jimmidyson/configmap-reload:v0.13.1docker pull class-docker.myk8s.cn/brancz/kube-rbac-proxy:v0.18.1docker pull class-docker.myk8s.cn/grafana/grafana:11.2.0docker pull class-docker.myk8s.cn/kube-state-metrics/kube-state-metrics:v2.13.0docker pull class-docker.myk8s.cn/prometheus/node-exporter:v1.8.2docker pull class-docker.myk8s.cn/prometheus-adapter/prometheus-adapter:v0.12.0docker pull class-docker.myk8s.cn/prometheus-operator/prometheus-operator:v0.76.2docker pull class-docker.myk8s.cn/prometheus/prometheus:v2.54.1docker tag class-docker.myk8s.cn/prometheus/alertmanager:v0.27.0 quay.io/prometheus/alertmanager:v0.27.0docker tag class-docker.myk8s.cn/prometheus/blackbox-exporter:v0.25.0 quay.io/prometheus/blackbox-exporter:v0.25.0docker tag class-docker.myk8s.cn/jimmidyson/configmap-reload:v0.13.1 ghcr.io/jimmidyson/configmap-reload:v0.13.1docker tag class-docker.myk8s.cn/brancz/kube-rbac-proxy:v0.18.1 quay.io/brancz/kube-rbac-proxy:v0.18.1docker tag class-docker.myk8s.cn/grafana/grafana:11.2.0 grafana/grafana:11.2.0docker tag class-docker.myk8s.cn/kube-state-metrics/kube-state-metrics:v2.13.0 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.13.0docker tag class-docker.myk8s.cn/prometheus/node-exporter:v1.8.2 quay.io/prometheus/node-exporter:v1.8.2docker tag class-docker.myk8s.cn/prometheus-adapter/prometheus-adapter:v0.12.0 registry.k8s.io/prometheus-adapter/prometheus-adapter:v0.12.0docker tag class-docker.myk8s.cn/prometheus-operator/prometheus-operator:v0.76.2 quay.io/prometheus-operator/prometheus-operator:v0.76.2docker tag class-docker.myk8s.cn/prometheus/prometheus:v2.54.1 quay.io/prometheus/prometheus:v2.54.1docker rmi class-docker.myk8s.cn/prometheus/alertmanager:v0.27.0docker rmi class-docker.myk8s.cn/prometheus/blackbox-exporter:v0.25.0docker rmi class-docker.myk8s.cn/jimmidyson/configmap-reload:v0.13.1docker rmi class-docker.myk8s.cn/brancz/kube-rbac-proxy:v0.18.1docker rmi class-docker.myk8s.cn/grafana/grafana:11.2.0docker rmi class-docker.myk8s.cn/kube-state-metrics/kube-state-metrics:v2.13.0docker rmi class-docker.myk8s.cn/prometheus/node-exporter:v1.8.2docker rmi class-docker.myk8s.cn/prometheus-adapter/prometheus-adapter:v0.12.0docker rmi class-docker.myk8s.cn/prometheus-operator/prometheus-operator:v0.76.2docker rmi class-docker.myk8s.cn/prometheus/prometheus:v2.54.1echoecho Saving images to fileechodocker save -o alertmanager.tar quay.io/prometheus/alertmanager:v0.27.0docker save -o blackbox-exporter.tar quay.io/prometheus/blackbox-exporter:v0.25.0docker save -o configmap-reload.tar ghcr.io/jimmidyson/configmap-reload:v0.13.1docker save -o kube-rbac-proxy.tar quay.io/brancz/kube-rbac-proxy:v0.18.1docker save -o grafana.tar grafana/grafana:11.2.0docker save -o kube-state-metrics.tar registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.13.0docker save -o node-exporter.tar quay.io/prometheus/node-exporter:v1.8.2docker save -o prometheus-adapter.tar registry.k8s.io/prometheus-adapter/prometheus-adapter:v0.12.0docker save -o prometheus-operator.tar quay.io/prometheus-operator/prometheus-operator:v0.76.2docker save -o prometheus.tar quay.io/prometheus/prometheus:v2.54.1echoecho Copying images file to k8s-worker1 and import itechoscp alertmanager.tar root@k8s-worker1:/rootscp blackbox-exporter.tar root@k8s-worker1:/rootscp configmap-reload.tar root@k8s-worker1:/rootscp kube-rbac-proxy.tar root@k8s-worker1:/rootscp grafana.tar root@k8s-worker1:/rootscp kube-state-metrics.tar root@k8s-worker1:/rootscp node-exporter.tar root@k8s-worker1:/rootscp prometheus-adapter.tar root@k8s-worker1:/rootscp prometheus-operator.tar root@k8s-worker1:/rootscp prometheus.tar root@k8s-worker1:/rootsshcmd k8s-worker1 'docker load -i /root/alertmanager.tar'sshcmd k8s-worker1 'docker load -i /root/blackbox-exporter.tar'sshcmd k8s-worker1 'docker load -i /root/configmap-reload.tar'sshcmd k8s-worker1 'docker load -i /root/kube-rbac-proxy.tar'sshcmd k8s-worker1 'docker load -i /root/grafana.tar'sshcmd k8s-worker1 'docker load -i /root/kube-state-metrics.tar'sshcmd k8s-worker1 'docker load -i /root/node-exporter.tar'sshcmd k8s-worker1 'docker load -i /root/prometheus-adapter.tar'sshcmd k8s-worker1 'docker load -i /root/prometheus-operator.tar'sshcmd k8s-worker1 'docker load -i /root/prometheus.tar'echoecho Copying images file to k8s-worker2 and import itechoscp alertmanager.tar root@k8s-worker2:/rootscp blackbox-exporter.tar root@k8s-worker2:/rootscp configmap-reload.tar root@k8s-worker2:/rootscp kube-rbac-proxy.tar root@k8s-worker2:/rootscp grafana.tar root@k8s-worker2:/rootscp kube-state-metrics.tar root@k8s-worker2:/rootscp node-exporter.tar root@k8s-worker2:/rootscp prometheus-adapter.tar root@k8s-worker2:/rootscp prometheus-operator.tar root@k8s-worker2:/rootscp prometheus.tar root@k8s-worker2:/rootsshcmd k8s-worker2 'docker load -i /root/alertmanager.tar'sshcmd k8s-worker2 'docker load -i /root/blackbox-exporter.tar'sshcmd k8s-worker2 'docker load -i /root/configmap-reload.tar'sshcmd k8s-worker2 'docker load -i /root/kube-rbac-proxy.tar'sshcmd k8s-worker2 'docker load -i /root/grafana.tar'sshcmd k8s-worker2 'docker load -i /root/kube-state-metrics.tar'sshcmd k8s-worker2 'docker load -i /root/node-exporter.tar'sshcmd k8s-worker2 'docker load -i /root/prometheus-adapter.tar'sshcmd k8s-worker2 'docker load -i /root/prometheus-operator.tar'sshcmd k8s-worker2 'docker load -i /root/prometheus.tar'EOFbash dockerimage 默认情况下，grafana等各个组件都提供了网络策略，无法被外部访问，我们先删除grafana的策略，并修改它的服务暴露方式为NodePort，因为我们要从外部访问它的图表面板 12345678910111213rm -rf manifests/grafana-networkPolicy.yamlsed -i '/spec:/a\\ type: NodePort' manifests/grafana-service.yamlsed -i '/targetPort/a\\ nodePort: 32000' manifests/grafana-service.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/nodeExporter-daemonset.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/blackboxExporter-deployment.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/grafana-deployment.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/kubeStateMetrics-deployment.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/prometheusAdapter-deployment.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/prometheusOperator-deployment.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/alertmanager-alertmanager.yamlsed -i '/ image: /a\\ imagePullPolicy: IfNotPresent' manifests/prometheus-prometheus.yamlkubectl apply -f manifests/ 确定grafana已经定义了32000端口，直接在浏览器打开任意节点的IP地址访问 1kubectl get service -n monitoring grafana 12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgrafana NodePort 10.99.168.64 &lt;none&gt; 3000:32000/TCP 30m 配置好解析后，在浏览器中打开 http://k8s-worker1:32000 ，或者不配置解析用IP也可以 用户名和密码都是admin，点击login之后会让你修改新密码，进行修改后点击submit或直接点击skip不修改 点击左侧或右上角的+号，选择import导入k8s模板 点击 [模板链接](Dashboards | Grafana Labs) 选一个模板，把ID填进来，点击load即可 category选择Docker，Data Source选择Prometheus可以更好的筛选 自己可以挑选喜欢的模板，点击进去可以看到图示，选好之后点击Copy ID，填到我们的系统中即可 填好ID之后，点击ID后侧的load按钮进行加载，在最下侧的Prometheus处，选择Prometheus，然后点击import 升级控制平面先确定要升级的版本 12apt updateapt list kubeadm -a 这里的升级以具体课程时的版本为准，只需要参考步骤 12345kubeadm/unknown 1.32.4-1.1 amd64 [upgradable from: 1.32.0-1.1]kubeadm/unknown 1.32.3-1.1 amd64kubeadm/unknown 1.32.2-1.1 amd64kubeadm/unknown 1.32.1-1.1 amd64kubeadm/unknown,now 1.32.0-1.1 amd64 [installed,upgradable to: 1.32.4-1.1] 在上一步可以看到一个可用的列表，假设我们要升级的目标为1.32.4-1.1的版本 禁止Master节点接收新调度 12kubectl cordon k8s-masterkubectl get nodes 1234NAME STATUS ROLES AGE VERSIONk8s-master Ready,SchedulingDisabled control-plane 17d v1.32.0k8s-worker1 Ready worker 17d v1.32.0k8s-worker2 Ready worker 17d v1.32.0 驱逐Master节点上的现有任务 1kubectl drain k8s-master --ignore-daemonsets --delete-emptydir-data 123456789node/k8s-master already cordonedWarning: ignoring DaemonSet-managed Pods: kube-system/calico-node-gd44x, kube-system/kube-proxy-zxxgg, monitoring/node-exporter-x7h7levicting pod kube-system/coredns-7c445c467-prx7fevicting pod kube-system/calico-kube-controllers-5b9b456c66-mf6r4evicting pod kube-system/coredns-7c445c467-k6njzpod/calico-kube-controllers-5b9b456c66-mf6r4 evictedpod/coredns-7c445c467-prx7f evictedpod/coredns-7c445c467-k6njz evictednode/k8s-master drained 安装目标的kubeadm、kubelet、kubectl 12apt-get updateapt-get install -y kubelet=1.32.4-1.1 kubeadm=1.32.4-1.1 kubectl=1.32.4-1.1 查看可升级的列表并升级 1kubeadm upgrade plan 输出 12345678910111213141516171819202122232425262728293031323334353637383940414243[preflight] Running pre-flight checks.[upgrade/config] Reading configuration from the &quot;kubeadm-config&quot; ConfigMap in namespace &quot;kube-system&quot;...[upgrade/config] Use 'kubeadm init phase upload-config --config your-config.yaml' to re-upload it.[upgrade] Running cluster health checks[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: 1.32.0[upgrade/versions] kubeadm version: v1.32.4I0426 12:44:49.188198 14629 version.go:261] remote version is much newer: v1.33.0; falling back to: stable-1.32[upgrade/versions] Target version: v1.32.4[upgrade/versions] Latest version in the v1.32 series: v1.32.4Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT NODE CURRENT TARGETkubelet k8s-worker1 v1.32.0 v1.32.4kubelet k8s-worker2 v1.32.0 v1.32.4kubelet k8s-master v1.32.4 v1.32.4Upgrade to the latest version in the v1.32 series:COMPONENT NODE CURRENT TARGETkube-apiserver k8s-master v1.32.0 v1.32.4kube-controller-manager k8s-master v1.32.0 v1.32.4kube-scheduler k8s-master v1.32.0 v1.32.4kube-proxy 1.32.0 v1.32.4CoreDNS v1.11.3 v1.11.3etcd k8s-master 3.5.16-0 3.5.16-0You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.32.4_____________________________________________________________________The table below shows the current state of component configs as understood by this version of kubeadm.Configs that have a &quot;yes&quot; mark in the &quot;MANUAL UPGRADE REQUIRED&quot; column require manual config upgrade orresetting to kubeadm defaults before a successful upgrade can be performed. The version to manuallyupgrade to is denoted in the &quot;PREFERRED VERSION&quot; column.API GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIREDkubeproxy.config.k8s.io v1alpha1 v1alpha1 nokubelet.config.k8s.io v1beta1 v1beta1 no_____________________________________________________________________ 看完之后来升级，一般数据库不升级，或者先备份好再升级 1kubeadm upgrade apply v1.32.4 --etcd-upgrade=false 123456789101112[upgrade] Reading configuration from the &quot;kubeadm-config&quot; ConfigMap in namespace &quot;kube-system&quot;...[upgrade] Use 'kubeadm init phase upload-config --config your-config.yaml' to re-upload it.[upgrade/preflight] Running preflight checks[upgrade] Running cluster health checks[upgrade/preflight] You have chosen to upgrade the cluster version to &quot;v1.32.4&quot;[upgrade/versions] Cluster version: v1.32.0[upgrade/versions] kubeadm version: v1.32.4[upgrade] Are you sure you want to proceed? [y/N]: y...[upgrade] SUCCESS! A control plane node of your cluster was upgraded to &quot;v1.32.4&quot;.[upgrade] Now please proceed with upgrading the rest of the nodes by following the right order. 恢复Master节点的调度能力 123systemctl restart kubeletkubectl uncordon k8s-masterkubectl get nodes 1234NAME STATUS ROLES AGE VERSIONk8s-master Ready control-plane 17d v1.32.4k8s-worker1 Ready worker 17d v1.32.0k8s-worker2 Ready worker 17d v1.32.0 ETCD 备份与恢复备份先安装etcd客户端 1apt install etcd-client -y 备份成文件并查看 123456ETCDCTL_API=3 etcdctl \\--endpoints=https://127.0.0.1:2379 \\--cacert=/etc/kubernetes/pki/etcd/ca.crt \\--cert=/etc/kubernetes/pki/etcd/server.crt \\--key=/etc/kubernetes/pki/etcd/server.key \\snapshot save etcdbackupfile.db 恢复12345678910111213# 先停止服务mv /etc/kubernetes/manifests /etc/kubernetes/manifests.baksleep 1m# 删除现有ETCD，并恢复数据mv /var/lib/etcd /var/lib/etcd.bakETCDCTL_API=3 etcdctl \\--endpoints=https://127.0.0.1:2379 \\--cacert=/etc/kubernetes/pki/etcd/ca.crt \\--cert=/etc/kubernetes/pki/etcd/server.crt \\--key=/etc/kubernetes/pki/etcd/server.key \\--data-dir /var/lib/etcd \\snapshot restore etcdbackupfile.db 1234{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1745474079.1192722,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:306&quot;,&quot;msg&quot;:&quot;restoring snapshot&quot;,&quot;path&quot;:&quot;etcdbackupfile.db&quot;,&quot;wal-dir&quot;:&quot;/var/lib/etcd/member/wal&quot;,&quot;data-dir&quot;:&quot;/var/lib/etcd&quot;,&quot;snap-dir&quot;:&quot;/var/lib/etcd/member/snap&quot;}{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1745474079.1459239,&quot;caller&quot;:&quot;mvcc/kvstore.go:388&quot;,&quot;msg&quot;:&quot;restored last compact revision&quot;,&quot;meta-bucket-name&quot;:&quot;meta&quot;,&quot;meta-bucket-name-key&quot;:&quot;finishedCompactRev&quot;,&quot;restored-compact-revision&quot;:12097}{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1745474079.1544638,&quot;caller&quot;:&quot;membership/cluster.go:392&quot;,&quot;msg&quot;:&quot;added member&quot;,&quot;cluster-id&quot;:&quot;cdf818194e3a8c32&quot;,&quot;local-member-id&quot;:&quot;0&quot;,&quot;added-peer-id&quot;:&quot;8e9e05c52164694d&quot;,&quot;added-peer-peer-urls&quot;:[&quot;http://localhost:2380&quot;]}{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1745474079.1581106,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:326&quot;,&quot;msg&quot;:&quot;restored snapshot&quot;,&quot;path&quot;:&quot;etcdbackupfile.db&quot;,&quot;wal-dir&quot;:&quot;/var/lib/etcd/member/wal&quot;,&quot;data-dir&quot;:&quot;/var/lib/etcd&quot;,&quot;snap-dir&quot;:&quot;/var/lib/etcd/member/snap&quot;} 12345678910# 恢复服务mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifestssystemctl restart kubelet.service# 验证数据已经恢复kubectl get pod 检查etcd是否健康 1234ETCDCTL_API=3 etcdctl --endpoints=https://192.168.8.3:2379 \\--cacert=/etc/kubernetes/pki/etcd/ca.crt \\--cert=/etc/kubernetes/pki/etcd/server.crt \\--key=/etc/kubernetes/pki/etcd/server.key endpoint health 1https://127.0.0.1:2379 is healthy: successfully committed proposal: took = 883.786µs Kustomize 管理这部分不作为课堂学习需要掌握的内容，这部分是课下挑战所用，加油哦 Kustomize 概念Kustomize 是 Kubernetes 的原生配置管理工具，它允许用户通过定义资源和它们之间的依赖关系来描述 Kubernetes 应用程序的配置。 Kustomize 的核心概念大概包括： Kustomization 文件：这是 Kustomize 配置的核心，它是一个 YAML 文件，定义了 Kubernetes 资源和如何定制它们。它可以用来指定要包含的资源、应用补丁、设置标签和注解、生成configmap和secret等 资源（Resources）：在 kustomization.yaml 文件中定义的 Kubernetes 资源列表，可以是文件、目录或者远程仓库中的资源。 生成器（Generators）：如 configMapGenerator 和 secretGenerator，它们可以根据文件或字面值生成 ConfigMap 或 Secret。 补丁（Patches）：用于修改现有资源的字段。Kustomize 支持策略性合并补丁（patchesStrategicMerge）和 JSON 补丁（patchesJson6902）。 基准（Bases）：包含 kustomization.yaml 文件的目录，定义了一组资源及其定制。 覆盖（Overlays）：也是一个目录，它引用基准目录作为基础，并且可以包含额外的定制。覆盖可以用来创建特定环境的配置，如开发、测试和生产环境。 Kustomize 的工作流程通常包括定义基准和覆盖，然后在覆盖中应用补丁和生成器来定制基准资源。这种方式使得用户可以轻松地为不同环境创建和管理 Kubernetes 资源配置。 Kustomize 实验实验概述本实验旨在通过实际操作，让大家掌握 Kustomize 的使用，以便能够根据不同的环境需求（如开发、测试和生产环境）定制和管理 Kubernetes 应用的配置。 实验目标理解 Kustomize 的作用和优势: 理解 Kustomize 如何简化 Kubernetes 应用的配置管理。 创建和管理 Base 目录: 学会创建包含通用资源定义的 Base 目录。 编写 kustomization.yaml 文件来声明资源、生成器和补丁。 定制 Overlay 配置: 学会创建 Overlay 目录以适应特定环境的配置需求。 应用 patchesStrategicMerge 和 patchesJson6902 来定制 Deployment 资源。 生成 ConfigMap 和 Secret: 使用 configMapGenerator 和 secretGenerator 来生成环境特定的配置和敏感信息。 应用环境标签和注解: 在资源上添加环境特定的标签（如 env: dev）和注解。 禁用名称后缀哈希: 配置 generatorOptions 以禁用资源名称的哈希后缀，以保持资源名称的一致性。 验证 Overlay 配置: 使用 kubectl kustomize 命令来验证 Overlay 目录的最终 Kubernetes 资源配置。 部署到 Kubernetes 集群: 使用 kubectl apply -k 命令将定制的 Overlay 配置应用到 Kubernetes 集群。 清理和维护: 学会如何清理实验中创建的资源，包括 Namespace 和各种 Kubernetes 资源。 实验步骤准备Base目录先在base目录中创建一些通用的yaml文件 12mkdir basecd base 在base目录中，创建一个secret，稍后可以在overlay目录中打补丁或者不打，secret文件如下： 这个密码是ABCabc123 12345678cat &gt; Secret.yml &lt;&lt;-EOFapiVersion: v1data: password: QUJDYWJjMTIzkind: Secretmetadata: name: mysqlpassEOF 在base目录中，创建一个Deployment，replicas是3，标签为app: nginx, Deployment文件如下： 12345678910111213141516171819202122232425262728cat &gt; Deployment.yml &lt;&lt;-EOFapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: mysqlname image: class-docker.myk8s.cn/library/mysql imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysqlpass key: passwordEOF 在base目录中，创建一个Service，Service文件如下： 123456789101112131415cat &gt; Service.yml &lt;&lt;-EOFapiVersion: v1kind: Servicemetadata: name: nodeservicespec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 8000 targetPort: 80 nodePort: 31788EOF 最后生成kustomization.yaml，在这个文件中，我们包含上我们刚创建的3个资源 12345678910cat &gt; kustomization.yaml &lt;&lt;-EOFapiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationmetadata: name: lxh-base-kustomizationresources:- Secret.yml- Deployment.yml- Service.ymlEOF 查看现在文件列表 12apt install tree -ytree . 输出 1234567.├── Deployment.yml├── kustomization.yaml├── Secret.yml└── Service.yml0 directories, 4 files 以上在base目录中的文件将用于生成： 一个名为mysqlpass的机密 一个名为nginx-deployment的部署，此部署的pod将具有app: nginx标签，并引用mysqlpass机密作为密码，密码值为ABCabc123 一个名为nodeservice的服务，监听在8000端口，收到请求后，转发给具有app: nginx标签的pod，并启用了31788的nodePort 准备Overlay目录创建开发环境 123cdmkdir -p overlays/developmentcd overlays/development 创建开发环境的kustomization.yaml 文件： Kustomize 功能特性列表参阅： 1https://kubernetes.io/zh-cn/docs/tasks/manage-kubernetes-objects/kustomization/#kustomize-feature-list patchesStrategicMerge补丁将会更新nginx-deployment这个Deployment patchesJson6902补丁也会更新nginx-deployment这个Deployment overlay的对象是刚创建的base目录下的内容 全体对象添加env: dev 禁止添加hash后缀 产生两个新的configmap和secret 12345678910111213141516171819202122232425262728293031323334353637383940cat &gt; kustomization.yaml &lt;&lt;-EOFapiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationnamespace: lxh-devpatches: - path: patchesStrategicMerge-demo.yaml target: kind: Deployment name: nginx-deployment options: allowNameChange: true - path: patchesJson6902-demo.yaml target: kind: Deployment name: nginx-deployment options: allowNameChange: trueresources: - ../../basecommonLabels: env: devgeneratorOptions: disableNameSuffixHash: trueconfigMapGenerator:- name: cmusername files: - configmap-1.yml- name: cmage literals: - cmage=18secretGenerator:- name: username files: - secret-1.yml type: Opaque- name: secrettest literals: - password=LiXiaoHui type: OpaqueEOF 生成器Generator在kustomization.yaml，如果用文件来生成configmap和secret，会将文件名也作为数据的一部分，建议用literals 生成configmap和secret的文件 123cat &gt; configmap-1.yml &lt;&lt;-EOFusername=lixiaohuiEOF 1234cat &gt; secret-1.yml &lt;&lt;-EOFusername=adminpassword=secretEOF 策略性合并与JSON补丁在 Kustomize 中，patchesStrategicMerge 和 patchesJson6902 都用于修改现有的 Kubernetes 资源。 patchesStrategicMerge补丁方式使用 YAML 文件来定义，它允许你直接编辑资源的 YAML 结构，就像编辑原始资源文件一样。这种方式直观且易于理解，特别是对于那些熟悉 Kubernetes 资源配置的人来说。 patchesJson6902 使用的是 JSON 补丁（JSON Patch）的方式，这是一种更为灵活和强大的补丁应用方式。JSON 补丁遵循 JSON Patch 规范（RFC 6902），允许执行更复杂的操作，如添加、删除、替换、测试等。这种方式使用 JSON 格式定义，可能在处理复杂的修改时更加强大。 生成策略性合并补丁 这里的名字一定要和已有的资源的名称一致 更新deployment的replicas为4 12345678cat &gt; patchesStrategicMerge-demo.yaml &lt;&lt;-EOFapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 4EOF 生成JSON补丁 新增deployment下的pod标签为dev: release1 123456789cat &gt; patchesJson6902-demo.yaml &lt;&lt;-EOF[ { &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/template/metadata/labels/dev&quot;, &quot;value&quot;: &quot;release1&quot; }]EOF 目前的文件列表： 123456789root@k8s-master:~/overlays/development# tree ..├── configmap-1.yml├── kustomization.yaml├── patchesJson6902-demo.yaml├── patchesStrategicMerge-demo.yaml└── secret-1.yml0 directories, 5 files 验证overlay最终成果1root@k8s-master:~/overlays/development# kubectl kustomize ./ 输出 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101apiVersion: v1data: cmage: &quot;18&quot;kind: ConfigMapmetadata: labels: env: dev name: cmage namespace: lxh-dev---apiVersion: v1data: configmap-1.yml: | username=lixiaohuikind: ConfigMapmetadata: labels: env: dev name: cmusername namespace: lxh-dev---apiVersion: v1data: password: QUJDYWJjMTIzkind: Secretmetadata: labels: env: dev name: mysqlpass namespace: lxh-dev---apiVersion: v1data: password: TGlYaWFvSHVpkind: Secretmetadata: labels: env: dev name: secrettest namespace: lxh-devtype: Opaque---apiVersion: v1data: secret-1.yml: dXNlcm5hbWU9YWRtaW4KcGFzc3dvcmQ9c2VjcmV0Cg==kind: Secretmetadata: labels: env: dev name: username namespace: lxh-devtype: Opaque---apiVersion: v1kind: Servicemetadata: labels: env: dev name: nodeservice namespace: lxh-devspec: ports: - nodePort: 31788 port: 8000 protocol: TCP targetPort: 80 selector: app: nginx env: dev type: NodePort---apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx env: dev name: nginx-deployment namespace: lxh-devspec: replicas: 3 selector: matchLabels: app: nginx env: dev template: metadata: labels: app: new-label env: dev spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: key: password name: mysqlpass image: class-docker.myk8s.cn/library/mysql imagePullPolicy: IfNotPresent name: mysqlname 发布开发环境123cd /root/overlays/development/kubectl create namespace lxh-devkubectl apply -k . 1234567configmap/cmage createdconfigmap/cmusername createdsecret/mysqlpass createdsecret/secrettest createdsecret/username createdservice/nodeservice createddeployment.apps/nginx-deployment created 查询创建的内容 发现我们新configmap和secret已经生效，两个补丁也都生效了，一个补丁将deployment的pod数量该为4，一个补丁添加了dev=release1的标签 12345678910111213141516171819202122232425root@k8s-master:~/overlays/development# kubectl get configmaps -n lxh-devNAME DATA AGEcmage 1 41scmusername 1 41skube-root-ca.crt 1 11mroot@k8s-master:~/overlays/development# kubectl get secrets -n lxh-devNAME TYPE DATA AGEmysqlpass Opaque 1 47ssecrettest Opaque 1 47susername Opaque 1 47sroot@k8s-master:~/overlays/development# kubectl get service -n lxh-devNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnodeservice NodePort 10.106.128.145 &lt;none&gt; 8000:31788/TCP 51sroot@k8s-master:~/overlays/development# kubectl get deployments.apps -n lxh-devNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 4/4 4 4 55sroot@k8s-master:~/overlays/development# kubectl get pod --show-labels -n lxh-devNAME READY STATUS RESTARTS AGE LABELSnginx-deployment-6f86fd678b-bv688 1/1 Running 0 64s app=nginx,dev=release1,env=dev,pod-template-hash=6f86fd678bnginx-deployment-6f86fd678b-wpk49 1/1 Running 0 64s app=nginx,dev=release1,env=dev,pod-template-hash=6f86fd678bnginx-deployment-6f86fd678b-wr94h 1/1 Running 0 64s app=nginx,dev=release1,env=dev,pod-template-hash=6f86fd678bnginx-deployment-6f86fd678b-xxkbw 1/1 Running 0 64s app=nginx,dev=release1,env=dev,pod-template-hash=6f86fd678b","link":"/2025/05/10/CKA%E8%AE%A4%E8%AF%81/CKA%E7%AC%94%E8%AE%B0/"},{"title":"MySQL数据库","text":"1 Linux安装MySQL 1.1 检查是否安装过MySQLrpm -qa | grep mysql 如果查询出文件，可使用一下命令删除:rpm -e 文件全名 1.2 下载MySQL官网地址:https://downloads.mysql.com/archives/community/ 1.3 解压重命名mkdir /soft tar -zxvf mysql-5.7.35-linux-glibc2.12-x86_64.tar.gz mv mysql-5.7.35-linux-glibc2.12-x86_64 mysql 1.4 增加分组groupadd mysql useradd -r -g mysql -s /bin/false mysql 1.5 创建相关目录mkdir -p /data/mysql chown -R mysql:mysql /data chown -R mysql:mysql /soft chmod 750 /data 1.6 配置环境变量cat &lt;&lt;EOF&gt;&gt; /root/.bash_profile export PATH=\\$PATH:/soft/mysql/bin EOF 1.7 安装依赖yum install -y libaio 1.8 卸载自带mariadb和mysqlrpm -qa | grep mysql rpm -e –nodeps $(rpm -qa | grep mysql) rpm -qa | grep mariadb rpm -e –nodeps $(rpm -qa | grep mariadb) 1.9 MySQL初始化mysqld –initialize –user=mysql –basedir=/soft/mysql –datadir=/data/mysql/ –basedir为mysql解压目录 、-datadir为mysql数据存放目录 红框为MySQL初始化的密码 1.10 配置my.cnf文件vim /etc/my.cnf [mysqld] bind-address=0.0.0.0 port=3306 user=mysql basedir=/soft/mysql datadir=/data/mysql socket=/tmp/mysql.sock log-error=/data/mysql/mysql.err pid-file=/data/mysql/mysql.pid #character config character_set_server=utf8mb4 symbolic-links=0 explicit_defaults_for_timestamp=true 1.11 启动/停止MySQL服务(开机自启)方式1： /soft/mysql/support-files/mysql.server start 方式2:设置MySQL服务开机自启 cp /soft/mysql/support-files/mysql.server /etc/init.d/mysqld 增加配置: vim /usr/lib/systemd/system/mysqld.service [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/soft/mysql/bin/mysqld –defaults-file=/etc/my.cnf LimitNOFILE = 5000 启动MySQL:systemctl start mysqld 停止MySQL:systemctl stop mysqld 查看MySQL状态:systemctl status mysqld 开机自启:systemctl enable mysqld 关闭开机自启:systemctl disable mysqld 第一次登录重置密码:ALTER USER USER() IDENTIFIED BY ‘root’; 1.12 免密登录关闭MySQL服务:service mysqld stop 修改my.cnf，增加skip-grant-tables vim /etc/my.cnf 重新的登录:mysql -u root -p 修改密码: use mysql; update user set authentication_string=password(‘root’) where user=’root’; flush privileges; 删除skip-grant-tables 1.13 设置远程连接use mysql; select host from user where user=’root’; update user set host =’%’ where user =’root’; flush privileges; 2 MySQL的基础查询2.1 基本检索功能检索数据是数据库中最基本的操作，使用的就是SELECT语句 1.查询指定字段 员工表(employee)中存储了关于员工的信息，需要找出所有员工的姓名、性别和电子邮箱地址 SELECT是SQL中的关键字，表示查询数据 FROM也是关键字，表示从那个表中查询 emp_name、sex、email表示要返回的字段，多个字段使用逗号隔开 分号表示SQL语句的结束 SELECT emp_name, sex, email from employee; 2.查询全部字段 查询职位信息表中的全部字段 SQL查询全部字段提供了一个简单的写法，就是使用星号(*)表示全部字段 SELECT * from job; 2.2 实现数据过滤以下语句查询姓名为”刘备”的员工信息: 在SQL语句中，可使用关键字WHERE指定数据的过滤条件。 SELECT employee.emp_name, employee.sex, employee.hire_date, employee.salary from employee WHERE emp_name = ‘刘备’; 1.简单过滤条件 最常见的查询条件是比较运算符，比较运算符可比较两个数据的大小，包括字符、数字以及日期类型数据的比较 以下语句查找2018年1月1日之后入职的员工: SELECT emp_name, hire_date from employee WHERE hire_date &gt;= DATE ‘2018-01-01’; 1.1 BETWEEN运算符 BETWEEN运算符用于查找指定范围之内的数据 查找月薪位于10000元和15000元之间的员工: SELECT emp_name, salary from employee where salary between 10000 and 15000 1.2 IN运算符 IN运算符用于查找指定列表中的数据 查找姓名为”刘备”、”关羽”或”张飞”的员工: SELECT emp_id, emp_name from employee where emp_name IN (‘刘备’, ‘关羽’, ‘张飞’); 2.空值判断条件 在数据库中，空值(NULL)是一个特殊值，表示缺失或未知的数据。在SQL语句中判断一个值是否为空，不能使用等于或不等于运算符。 查找没有上级领导(manager字段为空)的员工: 错误示例:SELECT emp_name,manager from employee where manager = NULL; 为了实现空值的判断，SQL中引入了两个特殊的运算符:IS NULL和IS NOT NULL，分别表示某个字段或表达式的结果未知(空值)或已知(非空) SELECT emp_name, manager from employee where manager IS NULL; 查找有奖金的员工: SELECT emp_name, bonus from employee where bonus IS NOT NULL; 3.文本模糊查找 可使用SQL模糊查找的功能进行文本检索，对应的运算符是LIKE 想要知道姓”关”的员工有哪些，可使用以下查询: SELECT emp_name, sex , salary from employee where emp_name LIKE ‘关%’; LIKE运算符支持以下两个通配符，可以用于指定匹配的模式: 百分号(%):表示匹配零个或多个任意字符 下划线(_):表示匹配一个任意字符 以下是一些常用的模式和匹配的字符串: LIKE’en%’ 匹配以”en”开始的字符串，例如:english、end LIKE’%en%’ 匹配包含”en”的字符串，例如:length、when LIKE’%en’ 匹配以”en”结尾的字符串，例如:ten、when LIKE’Be_’ 匹配以”Be”开头，再加上一个任意字符的字符串，例如:Bed、Bet LIKE’_e%’ 匹配一个任意字符加上”e”开始的字符串，例如:he、year 由于百分号和下划线是LIKE运算符中的通配符，因此如果查找模式中包含了”%”或”_”，就要用到转义字符，转义字符可将通配符当做普通字符使用，创建测试表: CREATE table t_like(c1 varchar(200)); INSERT INTO t_like(c1) values (‘项目进度: 25%已完成’); INSERT INTO t_like(c1) values (‘记录日期: 2021年5月25日’); 表t_like只有一个字段c1，数据类型为字符串，表中包含两条记录。假如需要查找包含”25%”的数据，其中百分号是要查找的内容而不是任意多个字符，可使用转义字符进行查找: SELECT c1 from t_like where c1 LIKE ‘%25#%%’ ESCAPE ‘#‘; ESCAPE关键字为LIKE运算符指定了一个#符号作为转义字符，因此查找模式中的第二哥%代表了百分号，其他的%则是通配符。 提示:对于MySQL和PostgreSQL而言，如果省略ESCAPE子句，默认的转义字符为反斜杠(\\) 使用LIKE运算符进行文本查找时，还需注意英文字母大小写问题 以下语句使用大写字母查找员工的电子邮箱: select email from employee where email LIKE ‘M%’; NOT LIKE运算符可执行与LIKE运算符相反的操作，也就是返回不匹配某个模式的文本 查找t_like表中不包含”25%”的记录: select c1 from t_like where c1 NOT LIKE ‘%25#%%’ ESCAPE ‘#‘; 4.组合过滤条件","link":"/2025/05/10/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E8%AF%AD%E5%8F%A5%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%8D%E5%8A%A1/"},{"title":"Linux的基础服务","text":"1 RAID存储技术与实践 1.1 RAID存储概念独立磁盘冗余阵列(RAID)是一种存储技术 通过将两个或多个硬盘驱动器(HDD)或固态硬盘(SSD)合并成一个协调的存储单元或阵列 从而创建数据丢失的故障安全机制 RAID存储通过将数据重复或重新创建，并将其存储在附加的驱动器上来防止磁盘驱动器数据的完全丢失，这个过程也被称为数据冗余。 提供数据丢失保护的配置被称为“容错”配置，这意味着即使磁盘驱动器发生故障，阵列仍然可以成功运行并提供可恢复的数据。 1.2 RAID的历史与级别1.RAID的历史 RAID的概念最早由加州大学伯克利分校的计算机科学家David Patterson、Garth Gibson和Randy Katz在1987年提出。他们的研究论文“关于RAID的论证”提出了将多个磁盘驱动器组合起来，以提高性能和可靠性的想法。 最初RAID的目标是通过将多个廉价的磁盘驱动器组合起来，以取代昂贵的大型磁盘驱动器，从而提供更高的性能和容错能力。因此，RAID的原始名称是“Redundant Array of Inexpensive Disks”(廉价磁盘冗余阵列)。 余和性能特征的不同，RAID级别分为多个类型，如RAID 0、RAID 1、RAID 5、RAID 6等。 RAID的发展离不开硬件和软件技术的进步。早期的RAID实施通常依赖于软件，即操作系统提供的RAID功能。然而，随着硬件技术的进步，硬件RAID控制器出现了，提供更高的性能和更强大的功能。 RAID的标准化工作也逐渐展开。Storage Networking Industry Association（SNIA）成立了RAID专门兴趣小组，致力于制定和推动RAID的标准化和发展。这些标准化工作为不同RAID级别的实施提供了一致性和互操作性。 2.RAID级别 RAID存储提供了不同的级别，每个级别具有不同的冗余和性能特性。 常见的RAID级别： RAID 0：条带化(数据分块)但没有冗余，提供较高的读写性能。 RAID 1：镜像，数据完全复制到另一个驱动器，提供容错能力。 RAID 5：条带化加分布式奇偶校验，提供数据冗余和读取性能。 RAID 6：类似于RAID 5，但提供更高级别的容错能力。 RAID 10：RAID 1+0，将RAID 1镜像组合成RAID 0条带化，提供较高的容错能力和读写性能。 RAID 50：RAID 5组合成RAID 0，提供较高的性能和容错能力。 RAID 60：RAID 6组合成RAID 0，提供更高级别的性能和容错能力。 1.3 详解RAID各级别1.RAID 0 原理: RAID 0使用数据条带化(striping)的方式将数据分散存储在多个磁盘驱动器上，而不进行冗余备份。数据被分成固定大小的块，并依次存储在每个磁盘上。例如，如果有两个驱动器(驱动器A和驱动器B)，一块数据的第一个部分存储在驱动器A上，第二个部分存储在驱动器B上，以此类推。这种条带化的方式可以同时从多个驱动器读取或写入数据，从而提高系统的性能。 RAID 0的原理与冗余原理图，展示了数据条带化存储的方式 在上述示例中，数据被分成块，并依次存储在两个驱动器上。每个块的一部分存储在驱动器A上，另一部分存储在驱动器B上。 适用场景: RAID 0适用于需要高性能而不关心数据冗余的场景。 以下是几种适合使用RAID 0的场景： 1.视频编辑和处理:在视频编辑中，需要快速读取和写入大量数据。RAID 0可以通过并行读写操作提高数据传输速度，加快视频编辑和处理的速度。 2.大型数据库应用:对于需要频繁访问和查询数据库的应用程序，RAID 0可以提供更快的数据访问速度，加快数据库操作的响应时间。 3.实时流媒体:对于需要实时传输和处理大量数据的流媒体应用，RAID 0可以提供足够的带宽和吞吐量，确保流媒体内容的平滑播放。 优点: 高性能:通过数据条带化和并行读写操作，可提供更快的数据传输速度和更高的系统性能。 成本效益:相对于其他RAID级别(如RAID 1或RAID 5)，RAID 0不需要额外的磁盘用于冗余备份，因此在成本上更具竞争力。 缺点: 缺乏冗余:由于RAID 0不提供数据冗余，如果任何一个驱动器发生故障，所有数据都可能丢失。因此，RAID 0不适合存储关键数据。 可靠性降低:由于没有冗余备份，RAID 0的可靠性相对较低。如果任何一个驱动器发生故障，整个阵列的可用性将受到影响。 2.RAID 1 原理: RAID 1使用数据镜像(mirroring)的方式将数据完全复制到两个或多个磁盘驱动器上。当写入数据时，数据同时写入所有驱动器。这样，每个驱动器都具有相同的数据副本，从而实现数据的冗余备份。如果其中一个驱动器发生故障，系统可以继续从剩余的驱动器中读取数据，确保数据的可用性和完整性。 RAID 1的原理和冗余原理图，展示了数据镜像的方式 在上述示例中，数据被完全复制到两个驱动器上。每个块的数据都同时存储在两个驱动器上，以实现数据的冗余备份。 适用场景: RAID 1适用于对数据冗余和高可用性要求较高的场景。 以下是几种适合使用RAID 1的场景： 1.关键数据存储：对于关键数据的存储，如企业的财务数据、客户信息等，RAID 1可以提供数据冗余备份，以防止数据丢失。 2.数据库服务器：对于需要高可用性和容错性的数据库服务器，RAID 1可以确保数据的持久性和可用性，即使一个驱动器发生故障，也可以从其他驱动器中读取数据。 3.文件服务器：对于共享文件的服务器，RAID 1可以提供冗余备份，确保文件的可靠性和高可用性。 优点: 数据冗余备份：RAID 1通过数据镜像将数据完全复制到多个驱动器上，提供冗余备份，保护数据免受驱动器故障的影响。 高可用性：由于数据的冗余备份，即使一个驱动器发生故障，系统仍然可以从其他驱动器中读取数据，保证数据的可用性和连续性。 读取性能提升：RAID 1可以通过并行读取数据的方式提升读取性能，从而加快数据访问速度。 缺点: 成本增加:由于需要额外的磁盘用于数据冗余备份，RAID 1的成本相对较高。需要考虑额外的硬件成本。 写入性能略低:由于数据需要同时写入多个驱动器，相对于单个驱动器的写入性能，RAID 1的写入性能可能略低。 3.RAID 5 原理: RAID 5使用数据条带化(striping)的方式将数据分散存储在多个磁盘驱动器上，并通过分布式奇偶校验实现数据的冗余备份。数据和奇偶校验信息被组织成数据块，其中奇偶校验信息被分布式存储在不同的驱动器上。当写入数据时，奇偶校验信息也会被更新。如果其中一个驱动器发生故障，系统可以通过重新计算奇偶校验信息来恢复丢失的数据。这种方式可以同时提供性能增强和数据冗余。 RAID 5的原理和冗余原理图，展示了数据条带化和分布式奇偶校验的方式 在上述示例中，数据被分成块，并依次存储在不同的驱动器上。奇偶校验信息被分布式存储在驱动器中。通过奇偶校验信息，可以计算和恢复丢失的数据。 适用场景: RAID 5适用于需要性能增强和数据冗余的场景 以下是几种适合使用RAID 5的场景： 文件服务器:对于文件服务器，RAID 5可以提供高性能的数据访问和数据冗余备份，确保文件的安全性和可用性。 数据库服务器:对于需要高性能和数据冗余的数据库服务器，RAID 5可以提供快速的数据读取和写入，同时保护数据免受驱动器故障的影响。 小型企业环境:对于小型企业，RAID 5提供了经济实惠的解决方案，同时提供了性能和数据冗余的好处。 优点: 性能增强:通过数据条带化和并行读写操作，RAID 5可以提供较高的数据传输速度和系统性能。 数据冗余备份:通过分布式奇偶校验，RAID 5可以提供数据的冗余备份，保护数据免受驱动器故障的影响。 成本效益:相对于其他RAID级别(如RAID 1)，RAID 5只需要额外一个驱动器用于奇偶校验信息，从而在成本上更具竞争力。 缺点: 写入性能受限:由于写入数据时需要重新计算奇偶校验信息，相对于读取操作，RAID 5的写入性能较低。 驱动器故障期间的数据完整性:如果一个驱动器发生故障，系统在恢复数据时需要进行计算，这可能导致数据访问速度较慢，并且在此期间可能会有数据完整性的风险。 4.RAID 6 原理: RAID 6使用数据条带化(striping)的方式将数据分散存储在多个磁盘驱动器上，并通过分布式奇偶校验和双重奇偶校验实现数据的冗余备份。数据和奇偶校验信息被组织成数据块，其中奇偶校验信息被分布式存储在不同的驱动器上，并通过双重奇偶校验提供更高的数据冗余性。当写入数据时，奇偶校验信息也会被更新。如果其中两个驱动器发生故障，系统可以通过重新计算奇偶校验信息来恢复丢失的数据。这种方式可以同时提供性能增强和更高级别的数据冗余。 RAID 6的原理和冗余原理图，展示了数据条带化、分布式奇偶校验和双重奇偶校验的方式 在上述示例中，数据被分成块，并依次存储在不同的驱动器上。奇偶校验信息被分布式存储在驱动器中，并通过双重奇偶校验提供更高级别的数据冗余性。 适用场景: RAID 6适用于需要更高级别的数据冗余和性能增强的场景。 以下是几种适合使用RAID 6的场景： 大容量存储系统:对于需要大容量存储和数据冗余备份的系统，如大型文件服务器或存档系统，RAID 6可以提供更高级别的数据冗余性。 长时间运行的应用程序:对于需要长时间运行的关键应用程序，如数据库服务器，RAID 6可以提供更高级别的数据冗余和故障容忍性。 虚拟化环境:在虚拟化环境中，需要高性能和更高级别的数据冗余来支持多个虚拟机的运行。RAID 6可以满足这些要求。 优点: 更高级别的数据冗余:通过分布式奇偶校验和双重奇偶校验，RAID 6可以提供更高级别的数据冗余性，即使同时发生两个驱动器故障，仍能恢复丢失的数据。 性能增强:通过数据条带化和并行读写操作，RAID 6可以提供较高的数据传输速度和系统性能。 缺点: 写入性能略低:由于数据需要同时写入多个驱动器，并进行双重奇偶校验计算，相对于读取操作，RAID 6的写入性能较低。 较高的成本:由于需要额外的磁盘用于奇偶校验信息和更复杂的计算，RAID 6的成本相对较高。需要考虑额外的硬件成本。 5.RAID 10 原理: RAID 10使用条带化(striping)的方式将数据分散存储在多个磁盘驱动器上，并通过镜像（mirroring）实现数据的冗余备份。数据被分成固定大小的块，并依次存储在不同的驱动器上，类似于RAID 0。然而，每个数据块都会被完全复制到另一个驱动器上，实现数据的冗余备份，类似于RAID 1。这样，RAID 10在提供性能增强的同时，也提供了数据的冗余保护。 RAID 10的原理和冗余原理图，展示了数据条带化和镜像的方式 在上述示例中，数据被分成块，并依次存储在不同的驱动器上。每个块的数据都完全复制到另一个驱动器上，实现数据的冗余备份。 适用场景: RAID 10适用于需要高性能和数据冗余的场景。 以下是几种适合使用RAID 10的场景： 数据库服务器:对于需要高可用性和性能的数据库服务器，RAID 10可以提供快速的数据读取和写入，同时保护数据免受驱动器故障的影响。 虚拟化环境:在虚拟化环境中，需要高性能和数据冗余来支持多个虚拟机的运行。RAID 10可以满足这些要求，提供性能增强和数据保护。 关键业务应用:对于关键业务应用，如金融交易系统或在线电子商务平台，RAID 10可以提供高可用性和快速的数据访问，确保业务的连续性和稳定性。 优点: 高性能:通过数据条带化和并行读写操作，RAID 10可以提供较高的数据传输速度和系统性能。 数据冗余备份:通过数据镜像将数据完全复制到另一个驱动器上，RAID 10提供了数据的冗余备份，保护数据免受驱动器故障的影响。 较高的可靠性:由于RAID 10采用镜像的方式进行数据冗余备份，即使一个驱动器发生故障，仍然可以从其他驱动器中读取数据，确保数据的可用性和连续性。 快速的故障恢复:在RAID 10中，如果一个驱动器发生故障，系统可以直接从镜像驱动器中恢复数据，而无需进行复杂的计算，从而加快故障恢复的速度。 缺点: 较高的成本:相对于其他RAID级别，RAID 10需要更多的驱动器用于数据镜像，增加了硬件成本。 低效的空间利用:由于RAID 10的数据镜像特性，有效的存储容量只等于所有驱动器中一半的容量，因此空间利用率较低。 6.RAID 50 原理: RAID 50使用条带化(striping)的方式将数据分散存储在多个RAID 5组中，并通过RAID 0的条带化方式对这些RAID 5组进行条带化。每个RAID 5组由多个磁盘驱动器组成，并使用分布式奇偶校验来提供数据冗余备份。RAID 0则通过将数据划分为固定大小的块，并将这些块依次存储在多个驱动器上，提供了更高的性能。这样，RAID 50既提供了数据冗余备份，又提供了性能增强。 RAID 50的原理和冗余原理图，展示了数据条带化和分布式奇偶校验的方式 在上述示例中，数据被分成块，并依次存储在不同的RAID 5组中。每个RAID 5组由多个驱动器组成，并使用分布式奇偶校验提供数据的冗余备份。 适用场景: RAID 50适用于需要高性能和更高级别的数据冗余的场景。 以下是几种适合使用RAID 50的场景： 大规模数据存储:对于需要大规模数据存储和数据冗余备份的系统，如视频编辑、数据分析或大型数据库，RAID 50可以提供高性能和较高级别的数据冗余性。 图形渲染和动画制作:在图形渲染和动画制作领域，需要高性能的存储系统来处理大型文件和复杂的渲染任务。RAID 50可以满足这些要求，提供快速的数据读取和写入速度。 虚拟化环境:在虚拟化环境中，需要高性能和更高级别的数据冗余来支持多个虚拟机的运行。RAID 50可以满足这些要求，提供性能增强和数据保护。 优点: 高性能:通过数据条带化和并行读写操作，RAID 50可以提供较高的数据传输速度和系统性能。 更高级别的数据冗余:由于采用了多个RAID 5组的方式，RAID 50提供了更高级别的数据冗余备份，即使同时发生多个驱动器故障，仍能恢复丢失的数据。 缺点: 较高的成本:由于需要更多的驱动器用于数据条带化和数据冗余备份，RAID 50的硬件成本相对较高。 配置和管理复杂性:由于涉及多个RAID 5组和驱动器，RAID 50的配置和管理相对复杂，需要更多的注意和维护。 7.RAID 60 原理: RAID 60采用条带化(striping)的方式将数据分散存储在多个RAID 6组中，并通过RAID 0的条带化方式对这些RAID 6组进行条带化。每个RAID 6组由多个磁盘驱动器组成，并使用分布式奇偶校验来提供数据的冗余备份。RAID 0则通过将数据划分为固定大小的块，并将这些块依次存储在多个驱动器上，提供了更高的性能。这样，RAID 60既提供了更高级别的数据冗余备份，又提供了性能增强。 RAID 60的原理和冗余原理图，展示了数据条带化和分布式奇偶校验的方式 在上述示例中，数据被分成块，并依次存储在不同的RAID 6组中。每个RAID 6组由多个驱动器组成，并使用分布式奇偶校验提供数据的冗余备份。 适用场景: RAID 60适用于需要更高级别的数据冗余和更高性能的场景。 以下是几种适合使用RAID 60的场景： 大型数据库系统:对于大型数据库系统，需要高可用性、高性能和更高级别的数据冗余来确保数据的完整性和可靠性。RAID 60可以提供这些要求。 大规模数据分析:在大规模数据分析领域，需要高性能的存储系统来处理大量数据的读取和写入。RAID 60可以满足这些要求，提供较高的数据传输速度和系统性能。 视频流媒体处理:对于视频流媒体处理应用，需要快速的数据读取和写入，以确保流畅的视频播放和高质量的媒体处理。RAID 60可以满足这些要求。 优点: 更高级别的数据冗余：由于采用了多个RAID 6组的方式，RAID 60提供了更高级别的数据冗余备份，即使同时发生多个驱动器故障，仍能恢复丢失的数据。 高性能:通过数据条带化和并行读写操作，RAID 60可以提供较高的数据传输速度和系统性能。 缺点: 较高的成本:由于需要更多的驱动器用于数据条带化和数据冗余备份，RAID 60的硬件成本相对较高。 配置和管理复杂性:由于涉及多个RAID 6组和驱动器，RAID 60的配置和管理相对复杂，需要更多的注意和维护。 1.4 RAID级别对比 1.5 RAID的实践DELL服务器R730xd配置RAID5： 1.开机等待DELL图标出现，连续按ctrl+r，进入RAID配置界面 2.进入RAID配置界面，如果是下面这个界面，选择Disk group 0这项需要按F2 3.按完F2后会出现下面的信息，选择delete这行，删掉当前的磁盘组 4.还原到没有RAID的时候 5.选中上图中的Controller 0这个选项，按F2，出现下面的界面，挨个选中后，OK确定 6.上一步确定后会返回如下界面 7.然后按F2后找到有create的选项，会出现如下选RAID级别的界面，选定级别后，选择OK 8.上一步确定后会返回如下界面 9.在数字上按F2后找到fast init进行快速格式化 10.重装系统后查看RAID–lsblk 1.6 RAID的演进随着时间的推移，RAID存储技术不断演进和改进。 以下是RAID存储领域的一些重要演进： 硬件RAID控制器: 硬件RAID控制器是RAID技术发展的重要里程碑之一。硬件RAID控制器是一种独立的设备，具有自己的处理器和缓存，用于执行RAID级别的计算和管理。它通过降低主机系统的负担，提供更高的性能和更好的数据保护。 RAID级别的增加: 随着RAID技术的发展，不断引入了新的RAID级别，以满足不同的需求。RAID 5和RAID 6提供了更高级别的容错能力，可以容忍多个磁盘故障。RAID 10结合了RAID 1和RAID 0的优势，提供了更高的性能和容错能力。RAID 50和RAID 60结合了不同级别的RAID，提供更高级别的性能和容错能力。 SSD和RAID: 随着固态硬盘(SSD)技术的发展，SSD在RAID存储中的应用也逐渐增多。SSD具有更高的读写性能和更好的可靠性，可以提供更高的RAID性能。SSD的低延迟和高吞吐量使得RAID存储能够更好地满足高性能计算和数据密集型应用的需求。 1.7 RAID存储的未来RAID存储技术在过去几十年中取得了巨大的进步，为数据存储提供了更高的可靠性和性能。然而，随着大数据、云计算和人工智能等技术的快速发展，对存储系统的需求也在不断增长和变化。 未来的RAID存储将面临更大的挑战和机遇。新的技术和创新将推动RAID存储在容错能力、性能、扩展性和成本效益方面的进一步发展。例如，分布式RAID、混合存储技术和软件定义存储等新兴技术将在未来的RAID存储中发挥重要作用。 2 使用ssh服务管理远程主机2.1 配置网络服务1.配置网卡参数 后续服务的计划IP为: 服务器主机IP地址均为192.168.1.31 客户端主机均为192.168.1.41及192.168.1.51 前面讲解了如何使用Vim文本编辑器来配置网卡参数。在RHEL 8系统中至少 有5种网络的配置方法，下面使用nmtui 命令来配置网络，其具体的配置步骤如下面各图所示。 执行nmtui命令运行网络配置工具: 选中Edit a connection并按下回车键: RHEL 5、RHEL 6系统及其他大多数早期的Linux系统中，网卡的名称是eth0、 eth1eth2、……；RHEL 7中变成了类似于eno16777736这样的名字；而在RHEL 8系 统中网卡的最新名称是类似ens160、ens192这样的，除了网卡的名称发生变化之外，其他一切几乎照旧，因 此这里演示的网络配置实验完全可适用于各种版本的Linux系统。 在服务器主机的网络配置信息中填写IP地址192.168.1.31/24。24表示子网掩码 中的前24位为网络号，后8位是主机号(与写成255.255.255.0的效果一样)。网关、DNS等 信息暂可不必填写，等用到时再补充。 选中要配置的网卡名称，然后按下Edit按钮: 把网卡IPv4的配置方式改成Manual(手动): 按下Show按钮: 填写IP地址和子网掩码: 单击OK按钮保存配置: 至此，在Linux系统中配置网络的步骤就结束了。 如果在安装RHEL 8系统时默认没有激活网卡。只需使用Vim编辑器将网卡配置文件中的ONBOOT 参数修改成yes，在系统重启后网卡就被激活了。 单击Back按钮结束配置工作: 修改完Linux系统中的服务配置文件后，并不会对服务程序立即产生效果。想让服 务程序获取到最新的配置文件，需要手动重启相应的服务，之后就可以看到网络畅通了： 2.创建网络会话 RHEL和CentOS系统默认使用NetworkManager来提供网络服务，这是一种动态管理网络配 置的守护进程，能够让网络设备保持连接状态。可使用nmcli命令来管理NetworkManager服务程 序。nmcli是一款基于命令行的网络配置工具，功能丰富，参数众多。可以轻松地查看网络信息 或网络状态： RHEL 8系统支持网络会话功能，允许用户在多个配置文件中快速切换(类似 firewalld防火墙服务中的区域技术)。在公司网络中使用笔记本电脑需手动指 定网络的IP地址，回到家中则使用DHCP自动分配IP地址，就需要频繁修改 IP地址，使用了网络会话功能后就简单多了—只需在不同的使用环境中激活相应 的网络会话，就可以实现网络配置信息的自动切换了。 使用nmcli命令并按照“connection add con-name type ifname”的格式来创建网络会话。 假设将公司网络中的网络会话称之为company，将家庭网络中的网络会话称之为house，依次创建各自的网络会话。 使用con-name参数指定公司所使用的网络会话名称company，然后依次用ifname参数指 定本机的网卡名称(以实际环境为准)，用autoconnect no参 数将网络会话设置为默认不被自动激活，用ip4及 gw4参数手动指定网络的IP地址： 使用con-name参数指定家庭所使用的网络会话名称house。因为要从外部DHCP服务器 自动获得 IP地址，所以这里不需要进行手动指定。 成功创建网络会话后，使用nmcli命令查看创建的所有网络会话： 使用nmcli命令配置过的网络会话是永久生效的，当上班后，启动company 网络会话，网卡信息就自动配置好了： 把虚拟机系统的网卡(网络适配器)切换成桥接模式，如 下图所示。然后重启虚拟机系统即可。 操作过后就能使用家庭中的路由器设备了。启动 house家庭会话，看一下效果： 如果启用company会话成功，但启用house会话失败且不能获取到动态地址，证明配置是正确的，问题出在了外部网络环境。有3种常见的情况，首先，家中的设备没有 连接路由器，而是通过拨号网络或共享WiFi的方式上网；其次，还在上学或上班的读者在浏 览网页前必须通过学校或公司的验证页面才能访问互联网；最后，检查物理机的防火墙设置， 可暂时关闭后再重试。 不需要网络会话时，用delete命令就能删除： 3.绑定两块网卡 生产环境必须提供7×24小时的网络传输服务。借助网卡绑定技术，不仅能 提高网络传输速度，还可以确保其中一块网卡出现故障时，依然可正常 提供网络服务。假设对两块网卡实施绑定技术，这样在正常工作中会共同传输数 据，使得网络传输的速度变得更快；而且即使有一块网卡突然出现了故障，另外一块网卡便 会立即自动顶替上去，保证数据传输不会中断。 在虚拟机系统中再添加一块网卡设备，确保两块网卡都处在同一种网络连接模式中， 如图1和图2所示。处于相同模式的网卡设备才可以进行网卡绑定，否则这两块网卡无 法互相传送数据。 在虚拟机中再添加一块网卡设备并确保两块网卡处在同一个网络连接中(网卡模式相同): 前面是使用nmtui命令配置网络信息，下面使用nmcli命令配置网卡设备的绑定 参数。网卡绑定的 理论知识类似于前面学习的RAID硬盘组，需要对参与绑定的网卡设备逐个进行“初始 设置”。如下图所示，左侧的ens160及ens192这些原本独立的网卡设备此 时需要被配置成为一块“从属”网卡，服务于右侧的bond0“主”网卡，不应该再有自己的 IP地址等信息。在进行了初始设置之后，它们就可以支持网卡绑定 3.1 创建出一个bond网卡 使用nmcli命令配置网络信息有一定的难度。首先 使用如下命令创建一个bond网卡。其中，命令与参数的意思是创建一个类型为bond(绑定)、 名称为bond0、网卡名为bond0的绑定设备，绑定模式为 balance-rr： 这里使用的是balance-rr网卡绑定模式，其中rr是round-robin的缩写，全称为轮循模式。 round-robin的特点是会根据设备顺序依次传输数据包，提供负载均衡的效果，让带宽的性能 更好一些；而且一旦某个网卡发生故障，会马上切换到另外一台网卡设备上，保证网络传输 不被中断。active-backup是另外一种比较常用的网卡绑定模式，它的特点是平时只有一块网 卡正常工作，另一个网卡随时待命，一旦工作中的网卡发生损坏，待命的网卡会自动顶替上 去。可见，这种网卡绑定模式的冗余能力比较强，因此也称为主备模式。 有一台用于提供NFS或Samba服务的文件服务器，能提供的最大网络传输 速度为100Mbit/s，但访问该服务器的用户数量特别多，因此它的访问压力也很大。在生产 环境中，网络的可靠性是极为重要的，而且网络的传输速度也必须得以保证。针对这样的情 况，比较好的选择就是使用balance-rr网卡绑定模式了。因为balance-rr模式能够让两块网卡 同时一起工作，当其中一块网卡出现故障后能自动备援，且无须交换机设备支援，从而提供 了可靠的网络传输保障。 3.2 向bond0设备添加从属网卡 刚创建成功的bond0设备当前仅仅是个名称，并没有真正能为用户传输数据的网 卡设备，使用命令把ens160和ens192网卡添加进来。其中，con-name参数后 接的是从属网卡的名称(可随时设置)；ifname参数后面接的是两块网卡的名称。 3.3 配置bond0设备的网络信息 用 nmcli命令依次配置网络的IP地址及子网掩码、网关、DNS、搜索域和手动配置等参数。也可以直接编辑网卡配置文件或使用nmtui命令完成下面的操作： 3.4 启动bond0网卡 当用户访问主机IP地址192.168.1.31时，主机实际上是由两块网卡在共同提供服务。可以在本地主机执行ping 192.168.1.31命令检查网络的连通性。为了检验网卡绑定技 术的自动备援功能，可以突然在虚拟机硬件配置中随机移除一块网卡设备，如图下所示。 可以非常清晰地看到网卡切换的过程（一般只丢失一个数据包），另外一块网卡会继续为 用户提供服务。 RHEL 8系统中，网卡绑定切换间隔为1毫秒(1/1000秒)，发生一个 丢包的情况大概率不会出现。 2.2 远程控制服务1.配置sshd服务 SSH是以安全的方式提供远程登录的协议，也是目前远程管理Linux系统的首选方式。在此之前，一般使用FTP或Telnet来进行远程登录。但因为它们 以明文的形式在网络中传输账户密码和数据信息，因此很不安全，很容易受到黑客发起的中 间人攻击，轻则篡改传输的数据信息，重则直接抓取服务器的账户密码。 想要使用SSH协议来远程管理Linux系统，则需要配置部署sshd服务程序。sshd是基于SSH 协议开发的一款远程管理服务程序，不仅使用起来方便快捷，而且能够提供两种安全验证的方法： 基于密码的验证 :用账户和密码来验证登录； 基于密钥的验证 :需要在本地生成密钥对，然后把密钥对中的公钥上传至服务器，并与服务器中的公钥进行比较；该方式相较来说更安全 “Linux系统中的一切都是文件”，因此在Linux系统中修改服务程序的 运行参数，实际上就是在修改程序配置文件的过程。 sshd服务的配置信息保存在 /etc/ssh/sshd_config文件中。运维人员一般会把保存着最主要配置信息的文件称为主配置文件， 配置文件中有许多以井号（#）开头的注释行，要想让这些配置参数生效，需要在修改参数 后再去掉前面的井号。sshd服务配置文件中包含的重要参数如下表所示。 接下来的实验会使用两台虚拟机，分别充当服务器和客户端，其IP地址及 作用如下表所示: 主机地址 操作系统 作用 192.168.1.31 Linux 服务器 192.168.1.41 Linux 客户端 在RHEL 8系统中，已默认安装并启用了sshd服务程序。接下来在客户端使用ssh命令远程连接服务器，其格式为”ssh [参数] 主机IP地址”，要退出登录则执行exit命令。第一 次访问时需要输入 yes来确认对方主机的指纹信息： 如果禁止以root管理员的身份远程登录到服务器，可降低被黑客暴力破解密码 的概率。使用Vim文本编辑器打开服务器上的sshd服务主配置文件(/etc/ssh/sshd_config)， 把第46行#PermitRootLogin yes参数前的井号(#)去掉，并把参数值yes改成no，这样 就不再允许root管理员远程登录了。 一般的服务程序并不会在配置文件修改之后立即获得最新的参数。想让新配置文件生效，需要手动重启相应的服务程序。最好也将这个服务程序加入到开机 启动项中，这样系统在下一次启动时，该服务程序便会自动运行，继续为用户提供服务。 当root管理员再来尝试访问sshd服务程序时，系统会提示”不可访问”的错 误信息。虽然sshd服务程序的参数相对比较简单，但这就是在Linux系统中配置服务程序的 正确方法。 2.安全密钥验证 加密是对信息进行编码和解码的技术，通过一定的算法(密钥)将原本能被直接阅读 的明文信息转换成密文形式。密钥即是密文的钥匙，有私钥和公钥之分。在传输数据时，如 果担心被他人监听或截获，就可以在传输前先使用公钥对数据加密处理，然后再进行传送。 只有掌握私钥的用户才能解密这段数据，除此之外的其他人即便截获了数据，一般也 很难将其破译为明文信息。 在生产环境中使用密码进行验证终归存在着被暴力破解或嗅探截获的风险。如果 正确配置了密钥验证方式，那么sshd服务程序将更加安全。具体的配置步骤如下: 第1步:在客户端主机中生成”密钥对”，记住是客户端 第2步:把客户端主机中生成的公钥文件传送至远程服务器 第3步:对服务器进行设置，使其只允许密钥验证，拒绝传统的密码验证方式 配置文件路径:/etc/ssh/sshd_config 第4步:客户端尝试登录到服务器，此时无须输入密码也可成功登录 如果用户没有密钥信息，即便有密码也会被拒绝，系统甚至不会给用户输入密码 的机会，如下图所示: 3.远程传输命令 既然SSH协议可以让用户远程控制服务器、传输命令 信息，那么是不是也能传输文件呢？ scp是一个基于SSH协议在网络之间进行安全传输的命令，其格式为: “scp [参数] 本地文件 远程账户@远程IP地址:远程目录” cp命令只能在本地硬盘中进行文件复制，而scp不仅能够 通过网络传送数据，而且所有的数据都将进行加密处理。如果想把一些文件通过网络 从一台主机传递到其他主机，这两台主机又恰巧都是 Linux系统，这时使用scp命令就可以轻 松完成文件的传递 scp命令可用的参数以及作用如下表所示: 使用scp命令把文件从本地复制到远程主机时，需要以绝对路径的形式写清本地 文件的存放位置。如果要传送整个文件夹内的所有数据，还需要额外添加参数-r进行递归操 作。然后写上要传送到的远程主机的IP地址，远程服务器便会要求进行身份验证了。当前用 户名称为root，而密码则为远程服务器的密码。如果想使用指定用户的身份进行验证，可使 用用户名@主机地址的参数格式。最后需要在远程主机的IP地址后面添加冒号，并在后面写 上要传送到远程主机的哪个文件夹中。只要参数正确并且成功验证了用户身份，即可开始传 送工作。由于scp命令是基于SSH协议进行文件传送的，而当前又设置好了密钥验证，因 此当前在传输文件时，并不需要账户和密码。 还可以使用scp命令把远程服务器上的文件下载到本地主机，其命令格式为: scp [参数] 远程用户@远程IP地址:远程文件 本地目录。这样就无须先登录远程主机再进行文件 传送了，也就省去了很多周折。例如，可以把远程主机的系统版本信息文件下载过来。 2.3 不间断会话服务当与远程主机的会话被关闭 时，在远程主机上运行的命令也随之被中断。 如果正在使用命令来打包文件，或者正在使用脚本安装某个服务程序，中途是绝对不能关闭 在本地打开的终端窗口或断开网络连接的，甚至连网速的波动都有可能导致任务中断，此时只能 重新进行远程连接并重新开始任务。有时正在执行文件打包操作，同时又想用脚本 来安装某个服务程序，这时会因为打包操作的输出信息占满用户的屏幕界面，而只能再打开一个 执行远程会话的终端窗口。时间久了，难免会忘记这些打开的终端窗口是做什么用的了。 Terminal Multiplexer(终端复用器，简称为Tmux)是一款能够实现多窗口远程控制的开源 服务程序。简单来说就是为了解决网络异常中断或为了同时控制多个远程终端窗口而设计的程 序。用户还可以使用Tmux服务程序同时在多个远程会话中自由切换，能够实现如下功能。 会话恢复:即便网络中断，也可让会话随时恢复，确保用户不会失去对远程会话的控制。 多窗口:每个会话都是独立运行的，拥有各自独立的输入输出终端窗口，终端窗口内 显示过的信息也 将被分开隔离保存，以便下次使用时依然能看到之前的操作记录 会话共享:多个用户同时登录到远程服务器时，可使用会话共享功能让用户之间的输入输出信息共享 在RHEL 8系统中，默认没有安装Tmux服务程序，需要配置软件仓库来安装: 1.管理远程会话 Tmux服务能做的事情非常多，例如创建不间断会话、恢复离线工作、将界面切分为不 同的窗格、共享会话等。直接敲击tmux命令进入会话窗口，如下图所示: 会话窗口底部出现一个绿色的状态栏，分别显示的是会话编号、名称、主机名及系统时间。 退出会话窗口的命令是exit，敲击后即可返回到正常的终端界面，如下图所示: 会话窗口的编号从0开始自动排序(即0、1、2、3、……)，会话窗口数量少的时候 还没关系，数量多的时候区分起来就很麻烦了。创建一个指定名称为backup的会话窗 口。当在命令行中敲下下面这条命令的一瞬间，屏幕会快速闪动一下， 这时就已经进入Tmux会话中了，在里面执行的任何操作都会被后台记录下来。 假设突然要去忙其他事情，但会话窗口中执行的进程还不能被中断，此时便可以用 detach参数将会话隐藏到后台。虽然看起来与刚才没有不同，但实际上可以看到当前的会话 正在工作中： 如果觉得每次输入detach参数都很麻烦，可以直接如下图所示关闭中断窗口(与进 行远程连接时突然断网具有相同的效果)，Tmux服务程序会自动帮我们进行保存。 操作之后，服务和进程都会一直在后台默默运行，不因为窗口被关闭而造成数据 丢失。查看后台有哪些会话： 传统的远程控制中，如果突然关闭会话窗口，一定会导致正在运行的命令也突然终止， 但在 Tmux的不间断会话服务中不会这样。只需查看一下刚刚关闭的离线会话名称， 然后尝试恢复回来，这个会话就可以继续工作了。回归到backup会话中的方法很简单，直接 在tmux命令后面加attach和会话编号或会话名称就可以。关闭会话窗口之前正在进行的一切 工作状态都会被原原本本地呈现出来，丝毫不受影响： 如果不再需要使用这个Tmux会话了，也不用先在tmux命令后面添加attach，再执行exit 命令退出，而是可以直接使用kill命令杀死这个会话。 生产环境中并不是必须先创建会话，然后再开始工作。可以直接使用tmux 命令执行要运行的指令，这样命令中的一切操作都会被记录下来，当命令执行结束后，后台 会话也会自动结束。 2.管理多窗格 实际工作中一个Shell终端窗口总是不够用。Tmux服务有个多窗格功 能，能够把一个终端界面按照上下或左右进行切割，从而使得能同时做多件事情。 创建一个会话。使用”tmux split-window”命令可以创建上下切割的多窗格终端界面， 图1所示。使用”tmux split-window -h”命令可以创建左右切割的多窗格终端界面， 图2所示。 上下切割的多窗格: 左右切割的多窗格: 创建多窗格终端界面后，同时做几件事情都不会乱了。如果觉得两个窗格还不够， 再执行几次上面的命令，退出时执行exit命令即可。 同时按下”Ctrl + B +方向键”调整窗格的尺寸。例如，现在使用的 窗格有些小，想向右扩大一些，则同时如下”Ctrl + B +右箭头键”就行了。 如果需切换到其他窗格工作，但又不能关闭当前的窗格，可使用如下表所示 的命令进行切换。 想调整窗格的位置，把上面与下面的窗格位置互换，可以用如下表所示的命令 进行互换。 如图1所示，原本执行过uptime命令的窗格在下方，只需要在该窗格中执行”tmux swap-pane -U”命令即可与上方窗格互换位置，效果如图2所示。 切换窗格位置前: 切换窗格位置后: 通过输入命令来切换窗格难免有些麻烦，实际上Tmux服务为用户提供 了一系列快捷键来执行窗格的切换。方法是先同时按下Ctrl+B组合键，然后松手后再迅 速按下其他后续按键，而不是一起按下。用于操作会话窗格的常见快捷键如下表所示。 3.会话共享功能 Tmux服务不仅可确保用户在极端情况下也不丢失对系统的远程控制，保证了生产环 境中远程工作的不间断性，而且具有会话共享、分屏切割窗格、会话锁定等实用的功能。 当多个用户同时控制服务器的时候，可把服务 器屏幕内容共享出来。也就是说，每个用户都能够看到相同的内容，还能一起同时操作。会 话共享功能的技术拓扑如下图所示: 接下来实验会使用三台虚拟机，分别充当服务器、客户端A、客户端B，其IP地址及作用如下表: 主机地址 操作系统 作用 192.168.1.31 Linux 服务器 192.168.1.41 Linux 客户端 192.168.1.51 Linux 客户端 要实现会话共享功能，首先使用ssh服务将客户端A远程连接到服务器，随后使用Tmux 服务创建一个新的会话窗口，名称为share： 使用ssh服务将客户端 B 也远程连接到服务器，并执行获取远程会话的命令。接 下来，两台客户端就能看到相同的内容了。 操作完成后，两台客户端的所有终端信息都会被实时同步，它们可以一起共享同一个会 话窗口，为了让大家更好地感受会话共享功能的强大之处，读者可以从两台不同 的客户端同时远程控制到服务器上面，也可以在同一台电脑上创建出两个窗格(下图 )来模拟这一行为，更能清晰地看到数据被同步的过程。 终端界面进行会话同步: 2.4 检索日志信息Linux的日志系统用于保存几乎所有的操作记录和服务运行状态， 并按“报错”“警告”“提示”和“其他”等标注进行分类。可根据所需的信 息进行检索，快速找出想要的信息， RHEL 8系统默认的日志服务程序是rsyslog。可将rsyslog理解成之前syslogd 服务的增强版本，更加注重日志的安全性和性能指标。为便于日后的检索，不同的日志 信息会被写入到不同的文件中。Linux系统中常见的日志文件如下表所示: /var/log/message这个综合性的文件用得最多。在处理Linux系统中出 现的各种故障时，一般是最先发现故障的症状，而找到故障的原因则一定离不开日志信息 的帮忙。 从理论上讲，日志文件分为下面3种类型: 系统日志:记录系统的运行情况和内核信息 用户日志:记录用户的访问信息，包含用户名、终端名称、登入及退出时间、来源IP地址和执行过 的操作等 程序日志:稍微大一些的服务一般都会保存一份与其同名的日志文件，里面记录着服务运行过程中 各种事件的信息；每个服务程序都有自己独立的日志文件，且格式相差较大 稍微大一些的服务都有自己独立的日志文件，为了用户在检索信息时 不至于很麻烦，journalctl 命令应运而生。journalctl命令用于检索和管理系统日志信息，语法格式为“journalctl 参数”。可以根据事件、类型、服务名 称等信息进行信息检索，从而提高了日常排错的效率。 journalctl命令的常见参数如下表所示: 1.查看系统中最后5条日志信息： 2.使用-f参数实时刷新日志的最新内容(与tail -f /var/log/message 命令的效果相同)： 3.rsyslog服务程序中，日志根据重要程度被分为9个等级，如下表所示: 只看系统中较高级别的报错信息，可在journalctl命令中用-p参数进行指定： 4.不仅能够根据日志等级进行检索，还可以用–since参数按照今日(today)、近N小 时(hour)、指定时间范围的格式进行检索，找出最近的日志数据 仅查询今日的日志信息： 仅查询最近1小时的日志信息： 仅查询12点整到14点整的日志信息： 仅查询从2024年12月1日至2024年12月27日的日志信息： 5.查询指定服务的日志信息。默认情况下所有日志信息 都是混在一起的。如果想看具体某项服务的日志信息，可使用_SYSTEMD_UNIT参数 查询，服务名称的后面要有”.service(标准服务名称的写法)” 3 使用Apache服务部署静态网站3.1 网站服务程序网站服务就是 Web网络服务，指允许用户通过浏览器访问互联网中各种资源的服务。如下图所示， Web网络服务是一种被动访问的服务程序，只有接收到互联网中其他主机发出的请求后才 会响应，最终用于提供服务程序的Web服务器会通过HTTP(超文本传输协议)或HTTPS(安 全超文本传输协议)把请求的内容传送给用户。 目前能提供Web网络服务的程序有IIS、Nginx和Apache等。IIS是Windows系统默认的Web服务程序，这是一款图 形化的网站管理工具，不仅可以提供Web网站服务，还可以提供FTP、NMTP、SMTP等服 务。IIS只能在Windows系统中使用 主机与Web服务器之间的通信: Nginx 程序作为一款轻量级的网站服务软件，因其稳定性和丰富的功能而快速占领服务器市场，但 Nginx最被认可的还是其系统资源消耗低且并发能力强的特性，因此得到了国内诸如新浪、网 易、腾讯等门户网站的青睐。 Apache程序是目前拥有很高市场占有率的Web服务程序之一，其跨平台和安全性广泛被 认可且拥有快速、可靠、简单的API扩展。下图所示为Apache 服务基金会的著名Logo。Apache 服务程序可以运行在Linux系统、UNIX系统甚至是Windows系统中，支持基于IP、域名 及端口号的虚拟主机功能，支持多种认证方式，集成有代理服务器模块、安全Socket层(SSL)， 能实时监视服务状态与定制日志消息，并支持各类丰富的模块。 总体来说，Nginx服务程序作为后起之秀，已经通过自身的优势与努力赢得了大批站长 的信赖。 但Apache程序作为老牌的Web服务程序，一方面在Web服务器软件市场具有相当 高的占有率，另一方面 Apache也是RHEL 8系统中默认的Web服务程序，而且还是RHCSA 和RHCE认证考试的必考内容，因此无论从实际应用角度还是从应对红帽认证考试的角度， 都有必要好好学习Apache服务程序的部署，并深入挖掘其可用的丰富功能。 注明:软件仓库的配置过程之前已做笔记 1.安装Apache服务程序。使用dnf命令进行安装时，命令后面 的Apache服务的软件包名称为httpd 2.启用httpd服务程序并将其加入到开机启动项中，使其能够随系统开机而运行， 从而持续为用户提供Web服务: 3.在浏览器(以Firefox为例)的地址栏中输入http://127.0.0.1并按回车键， 可以看到用于提供 Web服务的默认页面了，如下图所示。 httpd服务程序的默认页面: 3.2 配置服务文件参数Linux系统中配置服务就是修改服务的配 置文件。因此需要知道这些配置文件的所在位置以及用途。httpd服务程序的主要配置文 件及存放位置如下表所示: 主配置文件中保存的是最重要的服务参数，一般会被保存到/etc目录中以软件名称命名 的一个文件夹之中，名字为”服务名称.conf”，例如”/etc/httpd/conf/httpd.conf” 配置文件中所有以井号(#)开始的行都是注释行，其目的是对httpd服务 程序的功能或某一行参数进行介绍，在httpd服务程序的主配置文件中，存在3种类型的信息:注释行信息、全局配置、区域 配置，如下图所示: httpd服务主配置文件的参数结构: 全局配置参数是一种全局性的配置参数，可作用于所 有的子站点，既保证了子站点的正常访问，也有效降低了频繁写入重复参数的工作量。区 域配置参数则是单独针对每个独立的子站点进行设置的。httpd服务程序主配置文件中最为常用的参 数如下表所示。 DocumentRoot参数用于定义网站数据的保存路径，参数的默认值 是/var/www/html(网站数据存放到这个目录中)；当前网站普遍的首页面名称是 index.html，因此可向/var/www/html/index.html文件中写入一段内容，替换掉httpd服务程 序的默认首页面。该操作会立即生效。 执行上述操作之后，在Firefox浏览器中刷新httpd服务程序，可看到该程序的首 页面内容已发生改变，如下图所示。 网站数据一般保存在 /var/www/html目录中，把保存网站数据的目录修为/home/wwwroot目录: 第1步:建立网站数据的保存目录，并创建首页文件 第2步:打开httpd服务程序的主配置文件(/etc/httpd/conf/httpd.conf)，将约第122行用于定义 网站数据保存路径的 参数DocumentRoot修改为/home/wwwroot，同时还需要将约第127行与 第134行用于定义目 录权限的参数Directory后面的路径也修改为/home/wwwroot 第3步:重新启动httpd服务程序并验证效果，浏览器刷新页面后的内容如下图所示。 注明:提示权限不足 3.3 SELinux安全子系统SELinux是一个强制访问控制的安全子系统。Linux系统使用SELinux 技术的目的是为了让各个服务进程都受到约束，使其仅获取到本应获取的资源。 SELinux安全子系统对服务程序的功能进行限制、对文件资源的访问进行限制 SELinux服务比较复杂，配 置难度也很大，加之很多运维人员对这项技术理解不深，从而导致很多服务器在部署好Linux 系统后直接将SELinux禁用了。这绝对不是明智的选择。 SELinux服务有3种配置模式，具体如下: enforcing:强制启用安全策略模式，将拦截服务的不合法请求 permissive:遇到服务越权访问时，只发出警告而不强制拦截 disabled:对于越权的行为不警告也不拦截 本书中所有的实验都是在强制启用安全策略模式下进行的，虽然禁用SELinux服务后 确实能够减少报错几率，但这在生产环境中相当不推荐。 SELinux服务主配置文件路径:/etc/selinux/config SELinux服务主配置文件中定义的是SELinux的默认运行状态，可将其理解为系 统重启后的状态，因此它不会在更改后立即生效。使用getenforce命令获得当前SELinux 服务的运行模式： 为了确认上节图中所示的提示权限不足的结果是因为SELinux而导致的，可以用setenforce [0|1]命令修改 SELinux当前的运行模式(0为禁用，1为启用)。 注明:这种修改只是临时的，在系统重启 后就会失效： 再次刷新网页，会看到正常的网页内容，如下图所示。可见，问题是出在了SELinux 服务上。 httpd服务程序的功能是允许用户访问网站内容，因此SELinux肯定会默认放行用户 对网站的请求操作。但是将网站数据的默认保存目录修改为/home/wwwroot，这就 产生问题了。/home目录是用来存放普通用户的家目录数据的，而现在httpd提供的网站服务却要去获取普通用户家目录中的数据，这显然违反了SELinux的监 管原则。 把SELinux服务恢复到强制启用安全策略模式，然后分别查看原始网站数据的保 存目录与当前网站数据的保存目录是否拥有不同的SELinux安全上下文值。 在ls命令中，-Z参数用于查看文件的安全上下文值，-d参数代表对象是个文件夹。 文件上设置的SELinux安全上下文是由用户段、角色段以及类型段等多个信息项共同 组成的。其中，用户段system_u代表系统进程的身份，角色段object_r代表文件目录的角色， 类型段 httpd_sys_content_t代表网站服务的系统文件。 针对当前这种情况，只需要使用semanage命令，将当前网站目录/home/wwwroot的 SELinux安全上下文修改为跟原始网站目录的一样就行了。 semanage命令 semanage命令用于管理SELinux的策略，语法格式为 “semanage [参数] [文件]” SELinux服务极大地提升了Linux系统的安全性， semanage命令不仅能够像传统的chcon命令那样设置文件、目录的策略，还能够管理网络端 口、消息接口。使用semanage命令时，经常用到的几个 参数及其作用如下表所示: 向新的网站数据目录中新添加一条SELinux安全上下文，让这个目录以及里面的 所有文件能够被 httpd服务程序访问到： 执行上述设置之后，还无法立即访问网站，需要使用restorecon命令将设置 好的SELinux安全上下文立即生效。在使用restorecon命令时，可加上-Rv参数对指定的目 录进行递归操作以及显示SELinux安全上下文的修改过程。再次刷新页面，就可以 正常看到网页内容了，如下图所示: 注明:在RHCSA、RHCE或RHCA考试中，都需要先重启您的机器然后再执行判分脚本。 在日常工作中要养 成将所需服务添加到开机启动项中的习惯，比如这 里就需要添加systemctl enable httpd命令 3.4 个人用户主页功能如果想在系统中为每位用户建立一个独立的网站，通常的方法是基于虚拟网站主机功能 来部署多个网站。但这个工作会让管理员苦不堪言(用户数量很庞大时)，而且用户 自行管理网站还会碰到各种权限限制。其实，httpd服务程 序提供的个人用户主页功能完全可以胜任这个工作。该功能可以让系统内所有的用户在自己 的家目录中管理个人的网站，而且访问起来也非常容易。 第1步:在httpd服务程序中，默认没有开启个人用户主页功能。 需要编辑配置文件(/etc/httpd/conf.d/userdir.conf)，在第17行的UserDir disabled 参数前面加上井号(#)，表示让httpd服务 程序开启个人用户主页功能；同时把第24行的UserDir public_html参数前面的井号(#)去掉。UserDir参数表示网站数据在用户家目录中的保存目录名 称，即public_html目录。 第2步:在用户家目录中建立用于保存网站数据的目录及首页面文件。还需要把 家目录的权限修改 为755，保证其他人也有权限读取里面的内容。 第3步:重新启动httpd服务程序，在浏览器的地址栏中输入网址，其格式为”网址/～用户名”(其 中的波浪号是必需的，而且网址、波浪号、用户名之间没有空格)。从理论上讲，现在就 可以看到用户的个人网站了。出乎意料的是，系统显示报错页面，如下图所示。 这一定还是SELinux惹的祸。 第4步:思考这次报错的原因是什么？httpd服务程序提供个人用户主页功能时，用 户的网站数据目录本身就应该是存放到与用户对应的家目录中的，应该不需要修改 家目录的SELinux安全上下文。但前文还讲到了SELinux域的概念。SELinux域确保服 务程序不能执行违规的操作，只能为用户提供服务。httpd服务中突然开启的这项 个人用户主页功能到底有没有被SELinux域默认允许呢？ 使用getsebool命令查询并过滤出所有与HTTP协议相关的安全策略。其中，off 为禁止状 态，on为允许状态 没必要逐个理解SELinux域安全策略规则，只要能通过 名字猜测出相关的策略用途就足够了。比如，想要开启httpd服务的个人用户主页功能， 用到的SELinux域安全策略应该是httpd_enable_homedirs，大致确定后就可以用 setsebool命令修改SELinux策略中各条规则的布尔值了。在setsebool命令 后面加上-P 参数，让修改后的SELinux策略规则永久生效且立即生效。随后刷新网页，其效 果如下图所示: 有时候网站的拥有者并不希望直接将网页内容显示出来，而只想让通过身份验证的用户 看到里面的内容，这时就可以在网站中添加密码功能了。 第1步:先使用htpasswd命令生成密码数据库。-c参数表示第一次生成；后面再分别添 加密码数据库的存放文件，以及验证要用到的用户名称(该用户不必是系统中已有的本地 账户)。 第2步:继续编辑个人用户主页功能的配置文件(/etc/httpd/conf.d/userdir.conf)。把第31～40行的参数信息修改成下列内容，其中以井号(#)开头的内容为添加的注释信息，可将其忽略。随后保存并退出配置文 件，重启httpd服务程序即可生效。 此后，当用户再想访问某个用户的个人网站时，就必须输入账户和密码才能正常访问了。 另外，验证时使用的账户和密码是用htpasswd命令生成的专门用于网站登录的账户和密码， 而不是系统中的账户和密码。登录界面如图1与图2所示: 3.5 虚拟主机功能在虚拟专用服 务器(VPS)与云计算技术诞生以前，IDC服务供应商为了充分 地利用服务器资源，同时也为了降低购买门槛，纷纷启用了虚拟主机功能。 利用虚拟主机功能，可以把一台处于运行状态的物理服务器分割成多个”虚拟的服 务器”。但该技术无法实现目前云主机技术的硬件资源隔离，而只能让这些虚拟的 服务器共同使用物理服务器的硬件资源，供应商只能限制硬盘的使用空间大小。出于各 种考虑的因素(价格低廉)，目前依然有很多企业或个人站长在使用虚拟主机的 形式部署网站。 Apache的虚拟主机功能是服务器基于用户请求的不同IP地址、主机域名或端口号，提 供多个网站同时为外部提供访问服务的技术。如下图所示，用户请求的资源不同，最终 获取到的网页内容也各不相同。 1.基于IP地址 如果一台服务器有多个IP地址，而每个IP地址与服务器上部署的每个网站一一对应， 当用户请求访问不同的IP地址时，会访问到不同网站的页面资源。而且，每个网站都有 一个独立的IP地址，这对搜索引擎优化也大有裨益。 就当前的实验来讲，需要配置的IP地址如图1所示。在配置完毕并重启网络服务之 后，需检查网络的连通性，确保3个IP地址均可正常访问，如图2所示: 使用nmtui命令配置网络参数: 分别检查3个IP地址的连通性: 第1步:分别在/home/wwwroot中创建用于保存不同网站数据的3个目录，并向其中分 别写入网站的首页文件 第2步:从httpd服务的配置文件(/etc/httpd/conf/httpd.conf )中大约第132行处开始，分别追加写入3个基于IP地 址的虚拟主机网站参数，然后保存并退出。需重启httpd服务，这些配置才生效 第3步:此时访问网站，则会看到httpd服务程序的默认首页面中显示“权限不足”。 由于当前的/home/wwwroot目录及里 面的网站数据目录的SELinux安全上下文与网站服务不吻合，因此httpd服务程序无法获取到 这些网站数据目录。需要手动把新的网站数据目录的SELinux安全上下文设置正确，并使用restorecon命令让新设置的SELinux安全上下文立即生效，就可以 立即看到网站的访问效果了，如下图所示： 基于不同的IP地址访问虚拟主机网站: 2.基于主机域名 当服务器无法为每个网站都分配一个独立IP地址的时候，可以尝试让Apache自动识别 用户请求的域名，从而根据不同的域名请求来传输不同的内容。这种情况下的配置更加简 单，只需要保证位于生产环境中的服务器上有一个可用的IP地址(以192.168.1.31为例) 就可以。由于当前还没有介绍如何配置DNS解析服务，因此需要手动定义IP地址与域名 之间的对应关系。/etc/hosts是Linux系统中用于强制把某个主机域名解析到指定IP地址的配 置文件。简单来说，只要这个文件配置正确，即使网络参数中没有DNS信息也依然能够将域 名解析为某个IP地址。 第1步:手动定义IP地址与域名之间对应关系的配置文件(/etc/hosts)，保存并退出后会立即生效。 可通过分别ping这些域名来验证域名是否已经成功解析为IP地址 第2步:分别在/home/wwwroot中创建用于保存不同网站数据的3个目录，向其中分 别写入网站的首页文件。 3.从httpd服务的配置文件(/etc/httpd/conf/httpd.conf)中大约第132行处开始，分别追加写入3个基于主机名 的虚拟主机网站参数，然后保存并退出。需要重启httpd服务，这些配置才生效。 4.因当前的网站数据目录还是在/home/wwwroot目录中，因此还是必须要正确 设置网站数据目录文件的SELinux安全上下文，使其与网站服务功能相吻合。用 restorecon命令让新配置的SELinux安全上下文立即生效，就可以立即访问到虚拟主机网 站，效果如下图所示: 基于主机域名访问虚拟主机网站: 3.基于端口号 基于端口号的虚拟主机功能让用户通过指定端口号访问服务器上的网站资源。 使用Apache配置虚拟网站主机功能时，基于端口号的配置方式是最复杂的。不仅 要考虑httpd服务程序的配置因素，还需考虑到SELinux服务对新开设端口的监控。一般来 说，使用80、443、8080等端口号来提供网站访问服务是比较合理的，如果使用其他端口号 则会受到SELinux服务的限制。 下面的实验不但要考虑目录上应用的SELinux安全上下文的限制，还要考虑SELinux域对httpd服务程序的管控。 第1步:分别在/home/wwwroot中创建用于保存不同网站数据的3个目录，向其中分 别写入网站的首页文件 第2步:在httpd服务配置文件(/etc/httpd/conf/httpd.conf)的第46行～48行分别添加用于监听6111、6222和6333 端口的参数 第3步:从httpd服务的配置文件(/etc/httpd/conf/httpd.conf)中大约第134行处开始，分别追加写入3个基于端口号 的虚拟主机网站参数，然后保存并退出。需要重启httpd服务，这些配置才生效。 第4步:因为把网站数据目录存放在/home/wwwroot目录中，因此还是必须要正确 设置网站数据目录文件的SELinux安全上下文，使其与网站服务功能相吻合。用 restorecon命令让新配置的SELinux 安全上下文立即生效。 配置httpd服务程序和SELinux安全上下文并重启httpd服务后，出 现报错信息。因为SELinux服务检测到6111、6222和6333端口原本不属于Apache服务 应该需要的资源，但现在却以 httpd服务程序的名义监听使用了，所以SELinux会拒绝使用 Apache服务使用这3个端口。可以使用semanage命令查询并过滤出所有与HTTP协议相关且 SELinux服务允许的端口列表。 第5步:SELinux允许的与HTTP协议相关的端口号中默认没有包含6111、6222和6333， 因此需要将这3个端口号手动添加进去。该操作会立即生效，而且在系统重启过后依然有效。 设置好后再重启httpd服务程序，然后就可以看到网页内容了，结果如下图所示。 基于端口号访问虚拟主机网站: 3.6 Apache的访问控制Apache可以基于源主机名、源IP地址或源主机上的浏览器特征等信息对网站上的资源 进行访问控制。通过Allow指令允许某个主机访问服务器上的网站资源，通过Deny指令实 现禁止访问。在允许或禁止访问网站资源时，还会用到Order指令，这个指令用来定义Allow 或Deny指令起作用的顺序，匹配原则是按照顺序进行匹配，若匹配成功则执行后面的默认 指令。比如”Order Allow, Deny”表示先将源主机与允许规则进行匹配，若匹配成功则允许访 问请求，反之则拒绝访问请求。 第1步:先在服务器上的网站数据目录中新建一个子目录，并在这个子目录中创建一个 包含 Successful单词的首页文件 第2步:打开httpd服务的配置文件(/etc/httpd/conf/httpd.conf)，在第161行后面添加下述规则来限制源主机的访问。 这段规则的含义是允许使用Firefox浏览器的主机访问服务器上的首页文件，除此之外的所有 请求都将被拒绝。使用Firefox浏览器的访问效果如图1所示，使用其他浏览器的访问效 果如图2所示。 Firefox浏览器成功访问: 除了匹配源主机的浏览器特征之外，还可以通过匹配源主机的IP地址进行访问控制。 例如只允许 IP地址为192.168.1.41的主机访问网站资源，那么就可以在httpd服 务配置文件的第161行后面添加下述规则。这样在重启httpd 服务程序后再用本机(即服 务器，其IP地址为192.168.1.31)来访问网站的首页面时就会提示访问被拒绝了，如下图 所示: 因IP地址不符合要求而被拒绝访问: 4 使用vsftpd服务传输文件4.1 文件传输协议将计算机联网的首要目的就是获取资料，文件传输是重要的 获取资料的方式。互联网是由几千万台个人计算机、工作站、服务器、小型机、大型 机、巨型机等具有不同型号、不同架构的物理设备共同组成的，即便是个人计算机，也 可能会装有Windows、Linux、UNIX、macOS等不同的操作系统。为了能够在如此复杂多样 的设备之间解决文件传输的问题，文件传输协议(FTP)应运而生。 FTP是一种在互联网中进行文件传输的协议，基于客户端/服务器模式，默认使用20、21号端 口，端口20用于进行数据传输，端口21用于接受客户端发出的相关FTP命令与参数。FTP服 务器普遍部署于内网中，具有容易搭建、方便管理的特点。有些FTP客户端工具还可以支持文 件的多点下载以及断点续传技术，因此得到了广大用户的青睐。FTP的传输拓扑如下图所示: FTP的传输拓扑: FTP服务器是按照FTP协议在互联网上提供文件存储和访问服务的主机，FTP客户端则 是向服务器发送连接请求，以建立数据传输链路的主机。FTP协议有下面两种工作模式: 主动模式:FTP服务器主动向客户端发起连接请求 被动模式:FTP服务器等待客户端发起连接请求(默认工作模式) 防火墙一般用于过滤从外网进入内网的流 量，因此有些时候需要将FTP的工作模式设置为主动模式，才可以传输数据。 由于FTP、HTTP、Telnet等协议的数据都是用明文进行传输的，从设计上就是不 可靠的。为了满足以密文方式传输文件的需求，发明了vsftpd服务程序。vsftpd(very secure ftp daemon，非常安全的FTP守护进程)是一款运行在Linux操作系统上的FTP服务程序， 不仅完全开源而且免费。具有很高的安全性、传输速度以及支持虚拟用户验证 等其他FTP服务程序不具备的特点。在不影响使用的前提下，管理者可自行决定客户端是 采用匿名开放、本地用户还是虚拟用户的验证方式来登录vsftpd服务器。这样即便黑客拿到 了虚拟用户的账号密码，也不见得能成功登录vsftpd服务器。 配置妥当软件仓库之后，就可以安装vsftpd服务程序了。无论是使用yum还是dnf命 令都可以安装，优先选择使用dnf命令。 iptables防火墙管理工具默认禁止了FTP协议的端口号，因此在正式配置vsftpd服务程序 之前，为了避免默认的防火墙策略”捣乱”，需要清空iptables防火墙的默认策略，并 把当前已经被清理的防火墙策略状态保存下来： 把FTP协议添加到firewalld服务的允许列表中： vsftpd服务程序的主配置文件(/etc/vsftpd/vsftpd.conf)内容总长度有127行之多，其 中大多数参数在开头都添加了井号(#)，在grep命令后面添加-v参数，过滤并反选出没有包含井号(#)的 参数行(过滤掉所有的注释信息)，然后将过滤后的参数行通过输出重定向符写回原始的主 配置文件中。这样操作之后，就只剩下12行有效参数了: 下表罗列了vsftpd服务程序主配置文件中常用的参数以及作用: 4.2 vsftpd服务程序vsftpd作为更加安全的文件传输协议服务程序，允许用户以3种认证模式登录FTP服务器。 匿名开放模式:最不安全的一种认证模式，任何人都可以无须密码验证而直接登录 到FTP服务器 本地用户模式:通过Linux系统本地的账户密码信息进行认证的模式，相较于匿名 开放模式更安全，配置起来也很简单。但如果黑客破解了账户的信息，就可以 畅通无阻地登录FTP服务器，从而完全控制整台服务器 虚拟用户模式:更安全的一种认证模式，需要为FTP服务单独建立用户数据库文件， 虚拟出用来进行密码验证的账户信息，这些账户信息在服务器系统中实际是不存 在的，仅供FTP服务程序进行认证使用。这样，即使黑客破解了账户信息也无法登录 服务器，从而有效降低了破坏范围和影响 ftp是Linux系统中以命令行界面的方式管理FTP传输服务的客户端工具。手 动安装ftp客户端工具: 1.匿名访问模式 vsftpd服务程序中，匿名开放模式是最不安全的一种认证模式。任何人都可以 无须密码验证而直接登录FTP服务器。这种模式一般用来访问不重要的公开文件。如果用防火墙管理工具(如TCP Wrapper服 务程序)将vsftpd服务程序允许访问的主机范围设置为企业内网，也可提供基本的安全性 vsftpd服务程序默认关闭了匿名开放模式，需要做的就是开放匿名用户上传、下载文 件的权限，以及创建、删除、更名文件的权限。需要注意的是，针对匿名用户放开这 些权限会带来潜在危险，不建议在生产环境中如此行事。下表罗列了可以向匿名用户开放的权限参数以及作用: 修改vsftpd服务程序配置文件(/etc/vsftpd/vsftpd.conf): 在vsftpd服务程序的主配置文件中正确填写参数，然后保存并退出。还需要重启vsftpd 服务程序，让新的配置参数生效。在生产环境中或者在RHCSA、 RHCE、RHCA认证考试中一定要把配置过的服务程序加入到开机启动项中，以保证服务器在 重启后依然能够正常提供传输服务： 现在就可以在客户端执行ftp命令连接到远程的FTP服务器了。在vsftpd服务程序的匿 名开放认证模式下，账户统一为anonymous，密码为空。连接FTP服务器后，默认 访问的是/var/ftp目录。可切换到该目录下的pub目录中，尝试创建一个新的目录文件， 以检验是否拥有写入权限： 系统显示拒绝创建目录！在前面已清空iptables防火墙策略，也在vsftpd 服务程序的主配置文件中添加了允许匿名用户创建目录和写入文件的权限。在vsftpd服务程序的匿名开放认证模式下，默认访问的是/var/ftp目录。查看 该目录的权限得知，只有root管理员才有写入权限。将目录 的所有者身份改成系统账户ftp即可 尽管使用ftp命令登入FTP服务器后，创建目录时系统依然提示操 作失败，但是报错信息却发生了变化。应该是SELinux服务的问题，使用getsebool命令查看与FTP相关的SELinux域策略都有哪些： 根据经验和策略的名称判断出是ftpd_full_access–&gt; off策略规则导致操作失败。修改策略规则，并在设置时使用-P参数让修改过的 策略永久生效，确保服务器重启后依然能够顺利写入文件。 SELinux域策略修改完毕后，就能够顺利执行文件的创建、修改及删除等操作了： 上面的操作中，由于权限不足，将/var/ftp/pub目录的所有者设置成ftp用户本 身。除了这种方法，也可以通过设置权限的方法让其他用户获取到写入权限（例如777这样 的权限）。但由于vsftpd服务自身带有安全保护机制，因此不要直接修改/var/ftp的权限， 这有可能导致服务被“安全锁定”而不能登录。一定要记得是对里面的pub目录修改权限哦： 2.本地用户模式 相较于匿名开放模式，本地用户模式要更安全，配置也更简单。针对本地用户模式的权限参数以及作用如下表所示: 默认情况下本地用户所需的参数都已经存在，不需要修改。unmask一般被称为”权限掩码”或”权限补码”，能直接影 响到新建文件的权限值。在Linux系统中，新建的普通文件的权限是644，新建的目录的 权限是755。 其实，普通文件的默认权限是666，目录的默认权限是777，这都是写在系统配置文件中 的。但默认值不等于最终权限值。umask参数的默认值是022，根据公式”默认权限−umask ＝实际权限”，所以普通文件的默认权限到手后就剩下644，而目录文件就剩下755了。 配置(/etc/vsftpd/vsftpd.conf)本地用户的参数： 在vsftpd服务程序的主配置文件中正确填写参数，然后保存并退出。还需要重启vsftpd 服务程序，让新的配置参数生效。还需要将配置好 的服务添加到开机启动项中，以便在系统重启后依然可以正常使用vsftpd服务。 现在已经完全可以用本地用户的身份登录FTP服务器了。但使用root管 理员身份登录后，系统提示如下的错误信息： 输入root管理员的密码之前，就已经被系统拒绝访问了。这是因为vsftpd 服务程序所在的目录中默认存放着两个名为”用户名单”的文件(ftpusers和user_list)。 只要里面写有某位用户的名字，就不再允许这位用户登录到FTP服务器上。 vsftpd服务程序为了保证服务器的安全性而默认禁止了root管理员和大多数系统用 户的登录行为，有效地避免黑客通过FTP服务对root管理员密码进行暴力破解。如果确 认在生产环境中使用root管理员不会对系统安全产生影响，只需按照上面的提示删除掉root用户名 即可。也可以选择ftpusers和user_list文件中不存在的一个普通用户尝试登录FTP服务器： 为什么同样是禁止用 户登录的功能，却要制作两个一模一样的文件呢？ 在user_list文件上面。如果把上面主配置文件中userlist_deny的参数 值改成NO，那么user_list列表就变成了强制白名单。它的功能与之前完全相反，只允许列表 内的用户访问，拒绝其他人的访问。 采用本地用户模式登录FTP服务器后，默认访问的是用户的家目录，而且该 目录的默认所有者、所属组都是该用户自己，因此不存在写入权限不足的情况。但是当前的 操作仍然被拒绝，这是因为刚才将虚拟机系统还原到最初的状态了。为此，需要再次开 启SELinux域中对FTP服务的允许策略： 设置SELinux域策略时，一定记得添加-P 参数，否则服务器在重启后会按照原有的策略进行控制，从而导致配置过的服务无法使用。 在配置妥当后再使用本地用户尝试登录FTP服务器，分别执行文件的创建、重命名及删 除等命令。操作均成功！ 3.虚拟用户模式 虚拟用户模式是这3种模式中最安全的一种认证模式，是专门创建出一个账 号来登录FTP传输服务的，而且这个账号不能用于以SSH方式登录服务器。因为安全性较之于前面两种模式有了提升，所以配置流程也会稍微复杂一些。 第1步:安装vsftpd服务。创建用于进行FTP认证的用户数据库文件，其中奇数行 为账户名，偶数行为密码。分别创建zhangsan和lisi两个用户，密码均为redhat 由于明文信息既不安全，也不符合让vsftpd服务程序直接加载的格式，因此需要使用 db_load命令用哈希(hash)算法将原始的明文信息文件转换成数据库文件，并且降低数据库 文件的权限，然后再把原始的明文信息文件删除。 第2步:创建vsftpd服务程序用于存储文件的根目录以及用于虚拟用户映射的系统本地 用户。vsftpd服务用于存储文件的根目录指的是当虚拟用户登录后所访问的默认位置。 Linux系统中的每一个文件都有所有者、所属组属性，例如使用虚拟账户”张三” 新建了一个文件，但系统中找不到账户”张三”，就会导致这个文件的权限出现错误。为此，需要再创建一个可以映射到虚拟用户的系统本地用户。简单来说，就是让虚拟用户默认登录 到与之有映射关系的这个系统本地用户的家目录中。虚拟用户创建的文件的属性也都归属于 这个系统本地用户，从而避免Linux系统无法处理虚拟用户所创建文件的属性权限。 为了方便管理FTP服务器上的数据，可以把这个系统本地用户的家目录设置为/var目录(用来存放经常发生改变的数据)。为了安全起见，将这个系统本地用户设置为不允许登 录FTP服务器，这不会影响虚拟用户登录，而且还能够避免黑客通过这个系统本地用户进行登录。 第3步:建立用于支持虚拟用户的PAM文件 PAM(可插拔认证模块)是一种认证机制，通过一些动态链接库和统一的API把系统提 供的服务与认证方式分开，使得系统管理员可根据需求灵活调整服务程序的不同认证方式。 PAM是一组安全机制的模块，系统管理员可以用来轻易地调整服务程序的认 证方式，而不必对应用程序进行任何修改。PAM采取了分层设计(应用程序层、应用接口层、 鉴别模块层)的思想，其结构如下图所示: 新建一个用于虚拟用户认证的PAM文件vsftpd.vu，其中PAM文件内的”db=”参数为使用 db_load 命令生成的账户密码数据库文件的路径，但不用写数据库文件的后缀： 第4步:在vsftpd服务程序的主配置文件中通过pam_service_name参数将PAM认证文 件的名称修改为vsftpd.vu。PAM作为应用程序层与鉴别模块层的连接纽带，可以让应用程序 根据需求灵活地在自身插入所需的鉴别功能模块。当应用程序需要PAM认证时，则需要在应 用程序中定义负责认证的PAM配置文件，实现所需的认证功能。 例如，在vsftpd服务程序的主配置文件中默认就带有参数pam_service_name=vsftpd，表示登录 FTP服务器时是根据/etc/pam.d/vsftpd文件进行安全认证的。把 vsftpd主配置文件中原有的PAM认证文件vsftpd修改为新建的vsftpd.vu文件即可。该操作中 用到的参数以及作用如下表所示: 第5步:为虚拟用户设置不同的权限。虽然账户zhangsan和lisi都是用于vsftpd服务程 序认证的虚拟账户，但依然想对这两人进行区别对待。比如，允许张三上传、创建、 修改、查看、删除文件，只允许李四查看文件。这可以通过vsftpd服务程序来实现。只需新 建一个目录，在里面分别创建两个以zhangsan和lisi命名的文件，其中在名为zhangsan的文 件中写入允许的相关权限(使用匿名用户的参数)： 再次修改vsftpd主配置文件(/etc/vsftpd/vsftpd.conf)，通过添加user_config_dir参数来定义这两个虚拟用户 不同权限的配置文件所存放的路径。为了让修改后的参数立即生效，需要重启vsftpd服务程 序并将该服务添加到开机启动项中： 第6步:设置SELinux域允许策略，然后使用虚拟用户模式登录FTP服务器 此时，不但可以使用虚拟用户模式成功登录到 FTP 服务器，还可以分别使用账户zhangsan 和lisi来检验他们的权限。李四只能登录，没有其他权限： 张三不仅可以登录，还可以创建、改名和删除文件，因此张三的权限是满的。生产环境中要根据真实需求来灵活配置参数: 使用不同的方式登录文件传输服务器后，默认所在的位置，如下表所 示: 4.3 TFTP(简单文件传输协议)简单文件传输协议(TFTP)是基于UDP协议在客户端 和服务器之间进行简单文件传输的协议。提供不复杂、开销不大的文件传输服 务，可将其当作FTP协议的简化版本。 TFTP的命令功能不如FTP服务强大，甚至不能遍历目录，在安全性方面也弱于FTP服 务。由于TFTP 在传输文件时采用的是UDP协议，占用的端口号为69，因此文件的传 输过程也不像FTP协议那样可靠。但因为TFTP不需要客户端的权限认证，也就减少了 无谓的系统和网络带宽消耗，因此在传输琐碎的文件时，效率更高。 在系统上安装相关的软件包，进行体验。其中，tftp-server是服务程序，tftp是用 于连接测试的客户端工具，xinetd是管理服务： Linux系统中TFTP服务使用xinetd服务程序来管理。xinetd服务可用来管理 多种轻量级的网络服务，而且具有强大的日志功能。专门用于控制比较小的应用程序 的开启与关闭，想开启那个服务，就编辑对 应的xinetd配置文件的开关参数。 安装TFTP软件包后，还需要在xinetd服务程序中将其开启。RHEL 8系统中tftp所对应的配置文件默认不存在，需要用户根据示例文件(/usr/share/doc/xinetd/sample.conf自行创建: 创建的配置文件路径:/etc/xinetd.d/tftp 重启xinetd服务并将它添加到系统的开机启动项中，以确保TFTP服务在系统重 启后依然处于运行状态。考虑到有些系统的防火墙默认没有允许UDP协议的69端口，需要手动将该端口号加入到防火墙的允许策略中： TFTP的根目录为/var/lib/tftpboot。可使用安装好的tftp命令尝试访问其中的文件。使用tftp 命令访问文件时，会用到下表中的参数: 5 Samba或NFS实现文件共享5.1 Samba文件共享服务FTP文件传输服务确实可以让主机之间的文件传输变得简单方便，但FTP协议的本质是传输文件，而非共享文件。 Samba服务程序现在已经成为在Linux系统与 Windows系统之间共享文件的最佳选择。 Samba服务程序的配置方法与之前很多服务的配置方法类似，需要先通过软 件仓库来安装Samba 服务程序。顺带再安装 一个samba-client软件包，这是用于测试共享目录的客户端程序： 打开Samba服务程序的主配置文件(/etc/samba/smb.conf)。 第17～22行代表共享每位登录用户的家目录内容，这个默认操作着实有些危险，建议不要共享，将其删除掉。 第24～29行是用SMB协议共享本地的打印机设备，方便局域网内的用户远程使用打印机设备。当前我们没有打印机设备，因此建议也将其删除掉，不共享。 第31～37行依然为共享打印机设备的参数，同样建议予以删除。 对Samba服务的主配置文件删减操作之后，最后的有效配置参数只剩下了8 行。所剩不多的参数中，还能继续删除不需要的参数。例如，第5～8行参数中所提到的cups 的全称为Common UNIX Printing System(通用UNIX打印系统)，依然是用于打印机或打印 服务器的，继续予以删除。 为了避免在工作中使用到了打印机服务而不知如何配置，下面对上述代码进行详细的注 释说明(见下表): 上面的代码中，security参数代表用户登录Samba服务时采用的验证方式。共有4 种可用参数: share:代表主机无须验证密码。相当于vsftpd服务的匿名公开访问模式 user:代表登录Samba服务时需要使用账号密码进行验证，通过后才能获取到文件。 默认的验证 方式，最为常用 domain:代表通过域控制器进行身份验证，用来限制用户的来源域 server:代表使用独立主机验证来访用户提供的密码。相当于集中管理账号，并不 常用 在最早期的RHEL/CentOS系统中，Samba服务使用的是PAM(可插拔认证模块)来调 用本地账号和密码信息，后来在5、6版本中替换成了用smbpasswd命令来设置独立的Samba 服务账号和密码。RHEL 7/8 版本将传统的验证方式换成使用 tdbsam数据库进行验证。这是一个专门用于保存Samba服务账号密码的数据库，用户需要用 pdbedit命令进行独立的添加操作。 1.配置共享资源 Samba服务程序的主配置文件包括全局配置参数和 区域配置参数。全局配置参数用于设置整体的资源共享环境，对里面的每一个独立的共享资 源都有效。区域配置参数则用于设置单独的共享资源，且仅对该资源有效。创建共享资源的 方法很简单，只要将下表中的参数写入到Samba服务程序的主配置文件中，然后重启服 务即可。 用于设置Samba服务程序的参数以及作用: 第1步:创建用于访问共享资源的账户信息。在RHEL 8系统中，Samba服务程序默认 使用的是用户密码认证模式(user)。这种认证模式可以确保仅让有密码且受信任的用户访问 共享资源。不过，只有建立账户信息数据库之后，才能使用用户 密码认证模式。另外，Samba服务程序的数据库要求账户必须在当前系统中已经存在，否则 日后创建文件时将导致文件的权限属性混乱不堪，由此引发错误。 pdbedit命令用于管理Samba服务程序的账户信息数据库，格式为”pdbedit [选项] 账户”。第一次把账户信息写入到数据库时需要使用-a参数，以后在执行修改密码、 删除账户等操作时就不再需要该参数了。pdbedit命令中使用的参数以及作用如下表 所示: 第2步:创建用于共享资源的文件目录。创建时不仅要考虑到文件读写权限的问题，由于/home目录是系统中普通用户的家目录，还需考虑应用于该目录的SELinux安全上下文 带来的限制。在Samba 的帮助手册中显示，正确的文件上下文值应该是samba_share_t，所以只 需要修改完毕后执行restorecon命令，就能让应用于目录的新SELinux安全上下文立即生效。 第3步:设置SELinux服务与策略，使其允许通过Samba服务程序访问普通用户家目 录。执行getsebool命令，筛选出所有与Samba服务程序相关的SELinux域策略，根据策略的 名称选择出正确的策略条目进行开启即可： 第4步:在Samba服务程序的主配置文件(/etc/samba/smb.conf) 中，根据上面表中所提到的格式写入共享信息。 第5步:Samba服务程序的配置工作基本完毕。Samba服务程序在Linux系统中的名字为 smb，所以重启smb服务并加入到启动项中，保证在重启服务器后依然能够为用户持续提供服务。 为了避免防火墙限制用户访问，这里将iptables防火墙清空，再把Samba服务添加到 firewalld防火墙中，确保万无一失。 第6步:使用”systemctl status smb”命令查看服务器是否启动了Samba服务。查看Samba服务都共享了哪些目录可以使用smbclient命令来查看共享详情；-U 参数指定了用户名称(用哪位用户挂载了 Samba服务，就用哪位用户的身份进行查看)；-L 参数列出了共享清单。 2.Windows挂载共享 无论Samba共享服务是部署Windows系统上还是部署在Linux系统上，通过Windows 系统进行访问时，其步骤和方法都是一样的。假设Samba共享服务部署在Linux系统上， 并通过Windows系统来访问 Samba服务。Samba共享服务器和Windows客户端的IP地址根据下表来设置。 主机名称 操作系统 IP地址 Samba共享服务器 RHEL 8 192.168.1.31 Linux客户端 RHEL 8 192.168.1.41 Windows客户端 Windows 11 192.168.1.51 在Windows系统中访问共享资源，只需要单击Windows系统的”开始”按钮后输入两 个反斜杠，然后再添加服务器的IP地址(\\\\192.168.1.31)即可，如下图所示。 现在应该就能看到 Samba 共享服务的登录界面了。先使用linuxprobe账 户的系统本地密码尝试登录，结果出现了如下图所示的报错信息。由此可以验证，在RHEL 8系统中，Samba服务程序使用的是独立的账户信息数据库。所以，即便在Linux系统中有一 个linuxprobe账户，Samba 服务程序使用的账户信息数据库中也有一个同名的linuxprobe账户。 在正确输入Samba服务数据库中的linuxprobe账户名以及使用pdbedit命令设置的密码 后，就可以登录到Samba服务程序的共享界面中了，如下图所示。此时，可以尝试执行查 看、写入、更名、删除文件等操作。 由于Windows系统的缓存原因，可能在第二次登录时提供了正确的账户和密码，依 然会报错，只需要重新启动Windows客户端就没问题了 3.Linux挂载共享 Samba服务程序还可以实现Linux系统之间 的文件共享，在客户端安装支持文件共享服务的软件包(cifs-utils) 在Linux客户端创建一个用于挂载Samba服务共享资源的目录(smb_satabase)。mount命令中的-t参数用于指定协议 类型，-o参数用于指定用户名和密码，最后追加上服务器IP地址、共享名称和本地挂载目录 即可。服务器IP地址后面的共享名称指的是配置文件中[database]的值，而不是服务器本地挂 载的目录名称。 如果在每次重启电脑后都需要使用mount命令手动挂载远程共享目录会很麻 烦。按照Samba服务的用户名、密码、共享域的顺序将相关信息写入一个认证文 件中，然后让/etc/fstab文件和系统自动加载它。为了保证不被其他人随意看到，最后把这个 认证文件的权限修改为仅root管理员可读写： 将挂载信息写入/etc/fstab文件中，以确保共享挂载信息在服务器重启后依然生效： 注明:需执行mount -a让配置生效 Linux客户端成功挂载了Samba服务的共享资源。进入挂载目录/smb_database后可以 看到Windows 系统访问Samba服务程序时留下来的文件 5.2 NFS(网络文件系统)共享文件的主机都是Linux 系统时，可以在客户端部署NFS(网络文件系统)服务共享文件。NFS服务 可以将远程Linux系统上的文件共享资源挂载到本地主机的目录上，从而使得本地主机 (Linux客户端)基于TCP/IP协议，像使用本地主机上的资源那样读写远程Linux系统上的 共享文件。 由于RHEL 8系统中默认安装了NFS服务，接下来，准备配置NFS服务。 使用软件仓库检查RHEL 8系统中是否已经安装了NFS软件包： 第1步:为了检验NFS服务配置的效果，使用两台Linux主机(分别充当NFS 服务器和NFS客户端)，并按下表设置所使用的IP地址。 主机名称 操作系统 IP地址 NFS服务器 RHEL 8 192.168.1.31 NFS客户端 RHEL 8 192.168.1.41 配置好防火墙，以免默认的防火墙策略禁止正常的NFS共享服务: 第2步:在NFS服务器上建立用于NFS文件共享的目录，设置足够的权限确保其他 人也有写入权限。 第3步:NFS服务程序的配置文件为/etc/exports，默认情况下里面没有任何内容。按照”共享目录的路径 允许访问的NFS客户端(共享权限参数)”的格式，定义要共享 的目录与相应的权限。 例如，如果想要把/nfsfile目录共享给192.168.1.0/24网段内的所有主机，让这些主机都 拥有读写权限，在将数据写入到NFS服务器的硬盘中后才会结束操作，最大限度保证数据不 丢失，以及把来访客户端root管理员映射为本地的匿名用户等，则可以按照下面命令中的格 式，将下表中的参数写到NFS服务程序的配置文件中。 注明:NFS客户端地址与权限之间没有空格 在NFS服务的配置文件中巧用通配符能够实现很多便捷功能。比如，匹配IP地址就有3 种方法： 第一种是直接写*号，代表任何主机都可以访问； 第二种则是采用的 192.168.1.*通配格式，代表来自192.168.1.0/24网段的主机 第三种则是直接写对方的IP 地址，如192.168.1.41，代表仅允许某个主机进行访问 第4步:启动和启用NFS服务程序。由于在使用NFS服务进行文件共享之前，需要使 用RPC(Remote Procedure Call，远程过程调用)服务将NFS服务器的IP地址和端口号等信 息发送给客户端。因此，在启动NFS服务之前，还需重启并启用rpcbind服务程序， 并将这两个服务一并加入开机启动项中。 NFS客户端的配置步骤也十分简单。先使用showmount命令查询NFS服务器的远程共享信 息，该命令的必要参数如下表所示，其输出格式为”共享的目录名称 允许使用客户端地址” 在NFS客户端创建一个挂载目录。使用mount命令并结合-t参数，指定要挂载的文 件系统的类型，并在命令后面写上服务器的IP地址、服务器上的共享目录以及要挂载到本地 系统(客户端)的目录。 挂载成功后就应该能顺利看到在执行前面的操作时写入的文件内容了。如果希望 NFS文件共享服务能一直有效，则需要将其写入到fstab配置文件(/etc/fstab)中： 5.3 autofs自动挂载服务无论是Samba服务还是NFS服务，都要把挂载信息写入到/etc/fstab中，这样远程 共享资源就会自动随服务器开机而进行挂载。虽然这很方便，但如果挂载的远程资源 太多，则会给网络带宽和服务器的硬件资源带来很大负载。如果在资源挂载后长期不使 用，也会造成服务器硬件资源的浪费。 autofs自动挂载服务与mount命令不同，autofs服务程序是一 种Linux系统守护进程，当检测到用户试图访问一个尚未挂载的文件系统时，将自动挂载该 文件系统。之前是将挂载信填入/etc/fstab文件后，系统在每次开机时都自动将其挂载， 而autofs服务程序则是在用户需要使用该文件系统时才去动态挂载，从而节约了网络资源和 服务器的硬件资源。 首先需安装autofs服务程序: 处于生产环境中的Linux服务器，一般会同时管理许多设备的挂载操作。如果所有设 备的挂载信息都写入autofs服务的主配置文件中，会让主配置文件臃肿不堪，不利于服务 执行效率，也不利于日后修改里面的配置内容。因此，在autofs服务程序的主配置文件中需 要按照”挂载目录 子配置文件”的格式进行填写。挂载目录是设备挂载位置的上一级目录。例 如，光盘设备一般挂载到/media/cdrom 目录中，那么挂载目录写成/media即可。对应的子配置 文件则是对这个挂载目录内的挂载设备信息作进一步的说明。子配置文件需要用户自行定义， 文件名字没有严格要求，但后缀建议以.misc结束。具体的配置参数如下图红框中所示。 主配置文件路径:/etc/auto.master 在子配置文件中，应按照”挂载目录 挂载文件类型及权限 :设备名称”的格式进行填写。 例如，要把光盘设备挂载到/media/iso目录中，可将挂载目录写为iso，而-fstype为文件系统格 式参数，iso9660为光盘设备格式，ro、nosuid及nodev为光盘设备具体的权限参数，/dev/cdrom 则是定义要挂载的设备名称。配置完成后再顺手将autofs服务程序启动并加入到系统启动项中： 查看当前的光盘设备挂载情况，确认光盘设备没 有被挂载上，而且/media目录中根本就没有iso子目录： 但却可以使用cd命令切换到这个iso子目录中，而且光盘设备会被立即自动挂 载上，然后也就能顺利查看光盘内的内容了。 光盘设备的名称变成了/dev/sr0，实际上它和/dev/cdrom是快捷方式的 关系，只是名称不同。 再对NFS服务动测试该功: 首先把NFS共享目录卸载掉。 在autofs服务程序的主配置文件中会有一个”/misc /etc/auto.misc”参数，这个auto.misc相当于自动挂载的参考文件，它默认就已经存在，所以 这里不需要进行任何操作： 找到对应的auto.misc文件(etc/auto.misc)，填写本地挂载的路径和NFS服务器的挂载信息： 填写后重启autofs服务程序，当用户进入到/misc/nfsfile目录时，便会自动挂载共 享信息： 6 使用BIND提供域名解析服务待完善6.1 DNS域名解析服务​ 相较于数字构成的IP地址，域名更容易被理解和记忆，通常更习惯通过域名的方式来访问网络中的资源 ​ 网络中的计算机之间只能基于IP地址来相互识别对方的 身份，而且要想在互联网中传输数据，也必须基于外网的IP地址来完成 ​ 为了降低用户访问网络资源的门槛，域名系统(DNS)技术应运而而生，这是用于管理和解析域名与IP地址对应关系的技术 ​ 接受用户输入的域名或IP地址，然后自动查找与之匹配(具有映射关系)的IP地址或域名， 即将域名解析为IP地址(正向解析) ​ 将IP地址解析为域名(反向解析)。这样一来，只 需要在浏览器中输入域名就能打开想要访问的网站了 ​ DNS域名解析技术的正向解析是最常使用的一种工作模式 ​ 鉴于互联网中的域名和IP地址对应关系数据库太过庞大，DNS域名解析服务采用了类似目录树的层次结构来记录域名与IP地址之间的对应关系，从而形成了一个分布式的数据库系统 ​ DNS域名解析服务采用的目录树层次结构： ​ 域名后缀一般分为国际域名和国内域名。原则上来讲，域名后缀都有严格的定义，但在 实际使用时可以不必严格遵守 ​ 目前最常见的域名后缀有.com(商业组织)、.org(非营利组 织)、.gov(政府部门)、.net(网络服务商)、.edu(教育机构)、.pub(公共大众)、.cn(中国国家顶级域名)等 ​ DNS技术作为互联网基础设施中重要的一环，为了为网民提供不间断、 稳定且快速的域名查询服务，保证互联网的正常运转，提供了下面3种类型的服务器： ​ 主服务器:在特定区域内具有唯一性，负责维护该区域内的域名与IP地址之间的对 应关系 ​ 从服务器:从主服务器中获得域名与IP地址的对应关系并进行维护，以防主服务器宕机等情况 ​ 缓存服务器:通过向其他域名解析服务器查询获得域名与IP地址的对应关系，并将经常查询的域名信息保存到服务器本地，以此来提高重复查询时的效率 ​ 主服务器是用于管理域名和IP地址对应关系的真正服务器，从服务器帮助主 服务器“打下手”，分散部署在各个国家、省市或地区，以便让用户就近查询域名，从而减轻主服务器的负载压力。缓存服务器不太常用，一般部署在企业内网的网关位置，用于加速用 户的域名查询请求 ​ DNS域名解析服务采用分布式的数据结构来存放海量的“区域数据”信息，在执行用户发起的域名查询请求时，具有递归查询和迭代查询两种方式： ​ 递归查询:DNS服务器在收到用户发起的请求时，必须向用户返回一个准确的查询结果 ​ 如果DNS服务器本地没有存储与之对应的信息，则该服务器需要询问其他服务器，并将返回的查询结果提交给用户 ​ 迭代查询:DNS服务器在收到用户发起的请求时，并不直接回复查询结果，而是告诉另一台 DNS服务器的地址，用户再向这台DNS 服务器提交请求，这样依次反复，直到返回查询结果 ​ 当用户向就近的一台DNS服务器发起对某个域名的查询请求之后(以 www.linuxprobe.com为例)，其查询流程大致如下图所示： ​ 当用户向网络指定的DNS服务器发起一个域名请求时，通常情况下会有本地DNS服务器向上级的DNS服务器发送迭代查询请求；如果该DNS服务器没有要查询的信息，则会进一步向上级DNS服务器发送迭代查询请求，直到获得准确的查询结果为止。其中最高级、最权威的根DNS服务器总共有13台，分布在世界各地，其管理单位、具体的地理位置，以及IP地址如下表所示： ​ 实际上用于根域名的服务器总共有504台，从A到M进行了排序，并共用13个IP地址，以此进行负载均衡， 以抵抗分布式拒绝服务(DDoS)攻击 ​ 随着互联网接入设备数量的增长，原有的IPv4体系已经不能满足需求，IPv6协议在全球 开始普及。基于IPv6的新型地址结构为新增根服务器提供了契机。我国的“下一代互联网国家工程中心”于 2013年联合日本、美国相关运营机构和专业人士发起“雪人计划”，提出以 IPv6为基础、面向新兴应用、自主可控的一整套根服务器解决方案和技术体系，并于2017 年11月在全球完成25台IPv6根服务器的架设(我国部署了其中的4台，打破了我国过去没 有根服务器的困境) 6.2 安装bind服务程序​ BIND服务是全球范围内使用最广泛、最安全可靠且高效的域名解析服务程序 ​ DNS域名解析服务作为互联网基础设施服务， 责任之重可想而知，生产环境中安装部署bind服务程序时加上chroot(牢笼机制)扩展包，有效地限制bind服务程序仅能对自身配置文件进行操作，确保整个服务器的安全 1[root@linuxprobe ~]# yum -y install bind-chroot ​ 如果要为用户提供健全的DNS查询服务，需要本地保存相关的域名数据库，把所有域名和IP地址的对应关系都写入到某个配置文件中，估 计要有上千万条的参数，这样既不利于程序的执行效率，也不方便日后的修改和维护。因此在bind服务程序中有以下3个比较关键的文件： ​ 主配置文件(/etc/named.conf):只有59行，去除注释信息和空行之后，有效的参数仅有30行左右，用来定义bind服务程序的运行 ​ 区域配置文件(/etc/named.rfc1912.zones): ​ 保存域名和IP地址对应关系的所在位置，对应着每个域和相应IP地址所在的具体位置，需要查看或修改时，可根据这个位置找到相关文件 ​ 数据配置文件目录(/var/named):用来保存域名和IP地址真实对应关系的数据配置文件 ​ Linux系统中，bind服务程序的名称为named ​ 首先找到主配置文件(/etc/named.conf)，把第11行和第19行的地址均修改为any，分别表示服务器上的所有IP地址均可提供DNS域名解析服务、允许所有人对本服务器发送DNS查询请求 123456789101112[root@linuxprobe ~]# vim /etc/named.confoptions { listen-on port 53 { any; }; # 修改地址为any listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; allow-query { any; }; # 修改地址为any...... ​ bind服务程序的区域配置文件(/etc/named.rfc1912.zones)用来保存域名和IP地址对应关系的所在位置 ​ 定义了域名与IP地址解析规则保存的文件位置以及服务类型等内容，而没有包含具体的域名、IP地址对应关系等信息 ​ 服务类型有3种，分 别为hint(根区域)、master(主区域)、slave(辅助区域)，其中常用的master和slave指的就是主服务器和从服务器 ​ 将域名解析为IP地址的正向解析参数和将IP地址解析为域名的反向解析参数分别如下所示： ​ 1.正向解析参数: ​ 2.反向解析参数: ​ 分别修改bind服务程序的主配置文件、区域配置文件与数据配置文件 ​ 遇到bind服务程序启动失败的情况，若认为这是由于参数写错而导致的， 则可以执行named-checkconf命令和named-checkzone命令，分别检查主配置文件与数据配置 文件中语法或参数的错误 ​ 1.正向解析 ​ DNS域名解析服务中，正向解析是指根据域名(主机名)查找到对应的IP地址 ​ 当用户输入了一个域名后，bind服务程序会自动进行查找，并将匹配到的IP地址返 给用户，如下所示。这是最常用的 DNS工作模式 ​ 第1步：编辑区域配置文件(/etc/named.rfc1912.zones) ​ 该文件中默认已经有了一些无关紧要的解析参数，旨在让 用户有一个参考 ​ 可将下面的参数添加到区域配置文件的最下面，也可以将该文件 中的原有信息全部清空，而只保留自己的域名解析信息 123456[root@linuxprobe ~]# vim /etc/named.rfc1912.zoneszone &quot;linuxprobe.com&quot; IN { type master; file &quot;linuxprobe.com.zone&quot;; allow-update { none; };}; ​ 第2步:编辑数据配置文件 ​ 从/var/named目录中复制一份正向解析的模板文件 (named.localhost) ​ 把域名和IP地址的对应数据填写数据配置文件中并保存。复制时加上-a参数，这可以保留原始文件的所有者、所属组、权限属性等信息，以便让bind服 务程序顺利读取文件内容 1234567891011121314151617181920[root@linuxprobe ~]# cd /var/named/[root@linuxprobe named]# ls -al named.localhost-rw-r-----. 1 root named 152 6月 21 2007 named.localhost[root@linuxprobe named]# cp -a named.localhost linuxprobe.com.zone[root@linuxprobe named]# vim linuxprobe.com.zone$TTL 1D@ IN SOA linuxprobe.com. root.linuxprobe.com. (0 ; serial1D ; refresh1H ; retry1W ; expire3H ) ; minimum@ IN NS ns.linuxprobe.com.ns IN A 192.168.1.31www IN A 192.168.1.31# 检查文件有效性[root@linuxprobe named]# named-checkzone linuxprobe.com /var/named/linuxprobe.com.zonezone linuxprobe.com/IN: loaded serial 0OK ​ 第3步：保存并退出后文件(/var/named/linuxprobe.com.zone)后重启named服务程序，让新的解析数据生效 1234[root@linuxprobe named]# systemctl restart named[root@linuxprobe named]# systemctl enable namedCreated symlink /etc/systemd/system/multi-user.target.wants/named.service → /usr/lib/systemd/system/named.service.[root@linuxprobe named]# systemctl status named 1234567891011121314151617zone文件详解：# $TTL 缓存的生存周期# @ = zonename = test.hhy 当前域# IN 互联网# SOA 开始授权# NS dns服务端 nameserver# A ipv4 正向# AAAA IPV6# CNAME 别名# MX 邮件交互记录 5 数字代表优先级 数字越小优先级越高# 0 ; serial 更新序列号 # 1D ; refresh 更新间隔（从服务器下载数据）# 1H ; retry 失败重试# 1W ; expire 区域文件的过期时间# 3H ) ; minimum 缓存的最小生存周期# D Day（天）、H Hour（时）、W Week（周） ​ 在解析文件中，A记录类型表示将域名指向一个IPv4地址，而AAAA表示将域名指向 一个IPv6地址。还有8种记录类型，如下表所示: 记录类型 作用 A 将域名指向一个IPv4地址 CNAME 将域名指向另外一个域名 AAAA 将域名指向一个IPv6地址 NS 将子域名指定由其他DNS服务器解析 MX 将域名指向邮件服务器地址 SRV 记录提供特定的服务的服务器 TXT 文本内容一般为512字节，常作为反垃圾邮件的SPF(发送方策略框架)记录 CAA CA证书颁发机构授权校验 显性URL 将域名重定向到另外一个地址 隐形URL 与显性URL类似，但会隐藏真实目标地址 ​ 第4步:检验解析结果 ​ 为了检验解析结果，先把Linux系统网卡中的DNS地址参数修改成本机IP地址(下图)，就可以使用由本机提供的DNS查询服务了 123456789101112[root@linuxprobe ~]# nmcli connection down ens160成功停用连接 &quot;ens160&quot;（D-Bus 活动路径：/org/freedesktop/NetworkManager/ActiveConnection/6）[root@linuxprobe ~]# nmcli connection up ens160连接已成功激活（D-Bus 活动路径：/org/freedesktop/NetworkManager/ActiveConnection/4）[root@linuxprobe ~]# nslookup&gt; ns.linuxprobe.comServer: 192.168.1.31Address: 192.168.1.31#53Name: ns.linuxprobe.comAddress: 192.168.1.31# nslookup命令用于检测能否从DNS服务器中查询到域名与IP地址的解析记录，进而更准确地检验DNS服务器是否已经能够为用户提供服务 ​ 若解析出的结果不是192.168.1.31，很有可能是虚拟机选择了联网模式，并由互联网 DNS服务器进行了解析 ​ 此时应确认服务器信息是否为”Address: 192.168.1.31#53”，即由本地服务器192.168.1.31的53端口号进行解析；若不是，则重启网络后再试一下 ​ 2.反向解析 ​ 反向解析是将用户提交的IP地址解析为对应的域名信息，一般用于对某个IP地址上绑定的所有域名进行整体屏蔽 ​ 屏蔽由某些域名发送的垃圾邮件 ​ 可以针对某个IP地址进行反向解析，大致判断出有多少个网站运行在上面 ​ 第1步：编辑区域配置文件(/etc/named.rfc1912.zones) ​ 编辑该文件时，除了不要写错格式之外，还需记住此处定义的数据配置文件名称，因为还需在/var/named目录中建立与其对应的同名文 件。 ​ 反向解析是把IP地址解析成域名格式，因此在定义zone(区域)时应该要把IP地址反写，比如原来是192.168.1.0，反写后应该就1.168.192，只需写出IP地址的网络位即可，把下列参数添加至正向解析参数的后面 1234567891011[root@linuxprobe ~]# vim /etc/named.rfc1912.zoneszone &quot;linuxprobe.com&quot; IN { type master; file &quot;linuxprobe.com.zone&quot;; allow-update { none; };};zone &quot;1.168.192.in-addr.arpa&quot; IN { type master; file &quot;192.168.1.arpa&quot;; allow-update {none;};}; ​ 第2步:编辑数据配置文件: ​ 从/var/named目录中复制一份反向解析的模板文件 (named.loopback)，然后把下面的参数填写到文件中。IP地址仅需要写主机位，如下图 所示： 123456789101112131415161718[root@linuxprobe ~]# cd /var/named[root@linuxprobe named]# cp -a named.loopback 192.168.1.arpa[root@linuxprobe named]# vim 192.168.1.arpa$TTL 1D@ IN SOA @ rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS @ A 127.0.0.1 AAAA ::1 PTR localhost.31 PTR ns.linuxprobe.com41 PTR www.linux.com# 增加反向解析：把192.168.1.41通过PTR解析成www.linux.com[root@linuxprobe named]# systemctl restart named ​ 第3步:检验解析结果： ​ 正向解析实验中，已经把系统网卡中的DNS地址参数 修改成了本机IP地址，因此可以直接使用nslookup命令来检验解析结果 ​ 仅需输入IP地址即可查询到对应的域名信息 1234567[root@linuxprobe ~]# nslookup&gt; 127.0.0.11.0.0.127.in-addr.arpa name = localhost.&gt; 192.168.1.3131.1.168.192.in-addr.arpa name = ns.linuxprobe.com.1.168.192.in-addr.arpa.&gt; 192.168.1.4141.1.168.192.in-addr.arpa name = www.linux.com.1.168.192.in-addr.arpa. 6.3 部署从服务器​ 作为重要的互联网基础设施服务，保证DNS域名解析服务的正常运转至关重要 ​ 在DNS域名解析服务中，从服务器可以从主服务器上获取指定的区域数据文件，从而起到备份解析记录与负载均衡的作用 ​ 通过部署从服务器不仅可以减轻主服务器的负载压力，还可以提升用户的查询效率 ​ 在本实验中，主服务器与从服务器分别使用的操作系统和IP地址如下表所示： 主机名称 操作系统 IP地址 主服务器 RHEL 8 192.168.1.31 从服务器 RHEL 8 192.168.1.41 ​ 首先找到主配置文件(/etc/named.conf)，把第11行和第19行的地址均修改为any，分别表示服务器上的所有IP地址均可提供DNS域名解析服务、允许所有人对本服务器发送DNS查询请求 123456789101112[root@linuxprobe ~]# vim /etc/named.confoptions { listen-on port 53 { any; }; # 修改地址为any listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; allow-query { any; }; # 修改地址为any...... ​ 第1步:在主服务器的区域配置文件(/etc/named.rfc1912.zones)中允许该从服务器的更新请求，即修改allow-update {允许更新区域信息的主机地址;};参数，然后重启主服务器的DNS服务程序 12345678910111213[root@linuxprobe ~]# vim /etc/named.rfc1912.zoneszone &quot;linuxprobe.com&quot; IN { type master; file &quot;linuxprobe.com.zone&quot;; allow-update { 192.168.1.41; };};zone &quot;1.168.192.in-addr.arpa&quot; IN { type master; file &quot;192.168.1.arpa&quot;; allow-update { 192.168.1.41; };};[root@linuxprobe ~]# systemctl restart named在主服务器上配置防火墙放行规则，让 DNS 协议流量可以被顺利传递。 ​ 第2步：在主服务器上配置防火墙放行规则，让DNS协议流量可以被顺利传递 123[root@linuxprobe ~]# iptables -F[root@linuxprobe ~]# firewall-cmd --permanent --zone=public --add-service=dnssuccess ​ 第3步：在从服务器上安装 bind-chroot 软件包 ​ 修改配置文件，让从服务器也能够对外提供DNS服务，并且测试其与主服务器的网络连通性 1234567891011121314[root@Client ~]# dnf -y install bind-chroot[root@Client ~]# vim /etc/named.confoptions { listen-on port 53 { any; }; listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; allow-query { any; };......[root@Client ~]# ping -c 4 192.168.1.31 ​ 第4步：在从服务器中填写主服务器的IP地址与要抓取的区域信息，然后重启服务 ​ 注意：此时的服务类型应该是 slave（从），而不再是 master（主） ​ masters参数后面应该为主服务器的IP地址，而且file参数后面定义的是同步数据配置文件后要保存到的位置，稍后可以在该目录内看到同步的文件 12345678910111213[root@Client ~]# vim /etc/named.rfc1912.zoneszone &quot;linuxprobe.com&quot; IN { type slave; masters { 192.168.1.31; }; file &quot;slaves/linuxprobe.com.zone&quot;;};zone &quot;1.168.192.in-addr.arpa&quot; IN { type slave; masters { 192.168.1.31; }; file &quot;slaves/192.168.1.arpa&quot;;};[root@Client ~]# systemctl restart named# masters参数比正常的主服务类型master多了个字母s，表示可以有多个主服务器 ​ 第5步：检验解析结果 ​ 当从服务器的DNS服务程序在重启后，一般就已经自动从主服务器上同步了数据配置文件，而且该文件默认会放置在区域配置文件中所定义的目录位置中。​ 随后修改从服务器的网络参数，把DNS地址参数修改成 192.168.1.41，这样即可使用从服务器自身提供的DNS域名解析服务 ​ 最后就可以使用 nslookup 命令顺利看到解析结果了 123[root@linuxprobe slaves]# cp /var/named/192.168.1.arpa /var/named/slaves/[root@linuxprobe slaves]# ls192.168.1.arpa 7 使用DHCP动态管理主机地址7.1 动态主机配置协议动态主机配置协议(DHCP)是一种基于UDP协议且仅限于在局域网内部使用的网络协 议，主要用于大型的局域网环境或者存在较多移动办公设备的局域网环境中，用途是为局域 网内部的设备或网络供应商自动分配IP地址等参数，提供网络配置的”全家桶”服务。 DHCP就是让局域网中的主机自动获得网络参数的服务。在下图所示的拓扑 图中存在多台主机，如果手动配置每台主机的网络参数会相当麻烦，日后维护起来也让人头大。借助于DHCP，不仅可以为主机自动分配网络参数，还可以 确保主机使用的IP地址是唯一的，还能为特定主机分配固定的IP地址。 DHCP的拓扑示意图: DHCP的常见术语: 作用域:一个完整的IP地址段，DHCP根据作用域来管理网络的分布、IP地址的分 配及其他配置参数 超级作用域:用于管理处于同一个物理网络中的多个逻辑子网段，包含可以统一 管理的作用域列表 排除范围:把作用域中的某些IP地址排除，确保这些IP地址不会分配给DHCP客 户端 地址池:在定义了DHCP的作用域并应用了排除范围后，剩余的用来动态分配给客户 端的IP地址范围 租约:DHCP客户端能够使用动态分配的IP地址的时间 预约:保证网络中的特定设备总是获取到相同的IP地址 7.2 部署dhcpd服务程序dhcpd是Linux系统中用于提供DHCP的服务程序。 在确认软件仓库配置妥当之后，安装dhcpd服务程序，其软件包名称为dhcp-server： 查看dhcpd服务程序的配置文件内容： dhcp的服务程序的配置文件中只有3行注释语句，意味着需 自行编写这个文件。如果不知道怎么编写，可以看一下配置文件中第2行的参考示例 文件，其组成架构如下图所示: 配置文件参考示例路径:/usr/share/doc/dhcp-server/dhcpd.conf.example dhcpd服务程序配置文件的架构: 一个标准的配置文件应包括全局配置参数、子网网段声明、地址配置选项以及地址配 置参数: 全局配置参数用于定义dhcpd服务程序的整体运行参数 子网网段声明用于 配置整个子网段的地址属性 考虑到dhcpd服务程序配置文件的可用参数比较多，以下为最常用的参数(下表): 7.3 自动管理IP地址DHCP的设计初衷是为了更高效地集中管理局域网内的IP地址资源。DHCP服务器会自 动把IP地址、子网掩码、网关、DNS地址等网络信息分配给有需要的客户端，当客户端 的租约时间到期后还可以自动回收所分配的IP地址，以便交给新加入的客户端。 为了让实验更有挑战性，模拟一个真实生产环境的需求： 机房运营部门:明天会有100名学员自带笔记本电脑来我司培训学习，请保证他们能够 使用机房的本地DHCP服务器自动获取IP地址并正常上网。 机房所用的网络地址及参数信息如下表所示: 参数名称 值 默认租约时间 21600秒 最大租约时间 43200秒 IP地址范围 192.168.1.50~192.168.1.150 子网掩码 255.255.255.0 网关地址 192.168.1.1 DNS服务器地址 192.168.1.1 搜索域 linuxprobe.com 按照下表来配置DHCP服务器以 及客户端: 主机类型 操作系统 IP地址 DHCP服务器 RHEL 8 192.168.1.1 DHCP客户端 Windows 11 使用DHCP自动获取 作用域一般是个完整的IP地址段，而地址池中的IP地址才是真正供客户端 使用的，因此地址池应该小于或等于作用域的IP地址范围。由于VMware Workstation 虚拟机软件自带DHCP服务，为了避免与自己配置的dhcpd服务程序产生冲突，应先按照 图1和图2将虚拟机软件自带的DHCP功能关闭。 单击虚拟机软件的“虚拟网络编辑器”菜单: 关闭虚拟机自带的DHCP功能: DHCP客户端与服务器需要 处于同一种网络模式—仅主机模式(Hostonly)，否则就会产生物理隔离，从而无法获取IP 地址。 在确认DHCP服务器的IP地址等网络信息配置妥当后，就可以配置dhcpd服务程序了。 在配置dhcpd服务程序时，配置文件中的每行参数后面都需要以分号(;)结尾，另外，dhcpd服务程序配置文件内的参数都十分重要，在下表中列出了每 一行参数，并对其用途进行了简单介绍。 配置文件路径:/etc/dhcp/dhcpd.conf dhcpd服务程序配置文件中使用的参数以及作用: 在红帽认证考试以及生产环境中，都需要把配置过的dhcpd服务加入到开机启动项中， 以确保当服务器下次开机后dhcpd服务依然能自动启动，并顺利地为客户端分配IP地址等信 息。 dhcpd服务程序配置好后就可以开启客户端来检验IP分配效果了。在日常工 作中，Windows11是主流的桌面操作系统，只要确保两个主机都处于同一个网络模式内， 如图1那样设置Windows系统的网络为DHCP模式，再稍等片刻即可自动获取到 网卡信息了，如图2所示。 设置网络模式: 自动获取到IP地址: 如果在生产环境中配置dhcpd服务，则有可能会因为DHCP没有被防火墙放行而导致 失败，此时执行下面的命令即可： 正常情况下，DHCP的运作会经历4个过程：请求、提供、选择和确认。当客户端顺 利获得一个IP 地址及相关的网络信息后，就会发送一个ARP( 地址解析协议)请求给服务器。在dhcpd 服务程序收到这条信息后，也会再把这个IP地址分 配给其他主机，从根源上避免了IP地址冲突的情况。 7.4 分配固定IP地址在DHCP协议中有个术语是“预约”，用来确保局域网中特定的设备总是获取到固定 的IP地址。换句话说，就是dhcpd服务程序会把某个IP地址私藏下来，只将其用于相匹配的 特定设备。 要想把某个IP地址与某台主机进行绑定，就需要用到这台主机的MAC地址。MAC 地址即网卡上一串独立的标识符，具备唯一性，因此不会存在冲突的情况。在Linux系统中 查看MAC地址的示例如图1所示，在Windows系统中查MAC地址的示例如图2所示。 Linux系统中查看网卡MAC地址: Windows系统中查看网卡MAC地址: 在Linux系统或Windows系统中，都可以通过查看网络的状态来获知主机的MAC地址。 在dhcpd服务程序的配置文件中，按照如下格式将IP地址与MAC地址进行绑定: 如果不方便查看主机的MAC地址，该怎么办呢？首先启动dhcpd服务程序，为老板的主机分配一个 IP地址，这样就会在DHCP服务器本地 的日志文件中保存这次的IP地址分配记录。然后查看日志文件，就可以获悉主机的MAC地 址了(即下面加粗的内容)： 在Windows系统中看到的MAC地址，其格式类似于 00-50-56-c0-00-01，间隔符为减号(-)。但是在 Linux系统中，MAC地址的间隔符则变成 了冒号(:) dhcpd服务程序配置文件路径:/etc/dhcp/dhcpd.conf 确认参数填写正确后就可以保存并退出配置文件，然后就可以重启dhcpd服务程序了。 如果刚刚为这台主机分配了IP地址，由于它的IP地址租约时间还没 有到期，因此不会立即换成新绑定的IP地址。要想立即查看绑定效果，需要重启一下客户 端的网络服务，如下图示。 。* 8 使用Postfix与Dovecot部署邮件系统待完善8.1 电子邮件系统​ 使用”姓名@计算机主机名称”的格式来规范电子信箱的名 称 ​ 电子邮件系统基于邮件协议来完成电子邮件的传输，常见的邮件协议有下面这些: ​ 1.简单邮件传输协议(SMTP):用于发送和中转发出的电子邮件，占用服务器的TCP/25端口 ​ 2.邮局协议版本3:用于将电子邮件存储到本地主机，占用服 务器的TCP/110端口 ​ 3.Internet消息访问协议版本4:用于在本地主机上访问邮件，占用服务器的TCP/143端口 ​ 在电子邮件系统中，为用户收发邮件的服务器名为邮件用户代理(MUA) ​ 电子邮件系统能够让用户在离线的情况下依然可以完成数据的接收，肯定得有一个用于保存用户邮件的“信箱”服务器，这个服务器的名字为邮件投递代理(MDA)，工作职责是把来自于邮件传输代理(MTA) 的邮件保存到本地的收件箱中 ​ MTA的工作职责是转发处理不同电子邮件服务供 应商之间的邮件，把来自于MUA的邮件转发到合适的MTA服务器 ​ 例如，从新浪信箱 向谷歌信箱发送一封电子邮件，这封电子邮件的传输过程如下图所示： ​ 在电子邮件系统中，用户发送邮件后不必等待投递工作完成即可下线 ​ 如果对方邮件服务器(MTA)宕机或对方临时离线，则发件服务器(MTA)就会把要发送的内容自动地暂时保存到本地，等检测到对方邮件服务器恢复后会立即再次投递，期间一般无须运维人员维护处 理，随后收信人(MUA)就能在自己的信箱中找到这封邮件了 ​ 生产环境中部署企业级的电子邮件系统时，有4个注意事项需留意： ​ 1.添加反垃圾与反病毒模块:有效阻止垃圾邮件或病毒邮件对企业信箱的干扰 ​ 2.对邮件加密:保护邮件内容不被黑客盗取和篡改 ​ 3.添加邮件监控审核模块: ​ 监控企业全体员工的邮件中是否有敏感词以及透露企业资料等违规行为 ​ 4.保障稳定性: ​ 电子邮件系统的稳定性至关重要，运维应做到保证电子邮件系统的稳定运行，及时做好防范分布式拒绝服务(DDoS)攻击的准备 8.2 部署基础的电子邮件系统​ 最基础的电子邮件系统要能提供发件服务和收件服务，为此需要使用基于SMTP的Postfix服务程序提供发件服务功能 ​ 使用基于POP3协议的Dovecot服务程序提供收件服务功能 ​ 用户可以使用Outlook Express或Foxmail等客户端服务程序正常收发邮件 ​ 电子邮件系统的工作流程如下图： ​ 早期的Linux系统中，默认使用的发件服务由Sendmail服务程序提供，在RHEL 8系统中已替换为Postfix服务程序 ​ 相较于Sendmail服务程序，Postfix服务程序减少了很多不必要的配置步骤，在稳定性、并发性方面也有很大改进 ​ 信箱地址一般类似于root@linuxprobe.com这样，也就是按照”用户名@主机 地址(域名)”格式来规范的 ​ 要想更好地检验电子邮件系统的 配置效果，需要先部署bind服务程序，为电子邮件服务器和客户端提供DNS域名解析服务 ​ 1.配置服务器主机名称(/etc/hostname)，保证服务器主机名称与发信域名保持一致 123456789[root@linuxprobe ~]# vim /etc/hostnamemail.linuxprobe.com[root@linuxprobe ~]# hostnamelinuxprobe.com[root@linuxprobe ~]# hostnamectl set-hostname mail.linuxprobe.com[root@linuxprobe ~]# hostnamemail.linuxprobe.com# 修改主机名称文件后如果没有立即生效，可以重启服务器；或者再执行一条：hostnamectl set-hostname mail.linuxprobe.com命令，立即设置主机名称 ​ 2.清空iptables防火墙默认策略，并保存策略状态，避免因防火墙中默认存在的策略阻止了客户端DNS解析域名及收发邮件 12[root@linuxprobe ~]# iptables -F[root@linuxprobe ~]# iptables-save ​ 3.把DNS协议加入到firewalld防火墙的允许列表中： 1234[root@linuxprobe ~]# firewall-cmd --permanent --zone=public --add-service=dnssuccess[root@linuxprobe ~]# firewall-cmd --reloadsuccess ​ 4.为电子邮件系统提供域名解析 1.安装bind服务程序 2.在/etc目录中找到该服务程序 的主配置文件(/etc/named.conf)，把第11行和第19行的地址均修改为any，分别表示服务器上的所有IP 地址均可提供DNS域名解析服务，以及允许所有人对本服务器发送DNS查询请求 3.编辑区域配置文件(/etc/named.rfc1912.zones): 该文件中默认已经有了一些无关紧要的解析参数，旨在让 用户有一个参考。可以将下面的参数添加到区域配置文件的最下面。当然，也可以将该文件 中的原有信息全部清空，而只保留自己的域名解析信息。 4.编辑数据配置文件: 从/var/named目录中复制一份正向解析的模板文件件 (named.localhost)，然后把域名和IP地址的对应数据填写数据配置文件中并保存。在复制正向解析模板文件时，在cp命令后面追加-a参数，以便让新文件继承原文件 的属性和权限信息： 编辑/var/named/linuxprobe.com.zone文件如下: 5.在保存并退出后文件后记得重启named服务程序: 第4步:修改好配置文件后重启bind服务程序，这样电子邮件系统所对应的服务器主机名为mail.linuxprobe.com，而邮件域为@linuxprobe.com。把服务器的DNS地址修改成本地IP 地址，如下图所示: 第5步:让新配置的网卡参数立即生效： 对主机名执行ping命令，若能ping通，则证明上述操作全部正确: 注明:在执行 ping操作时，也会获得主机名对应的IP地址，证明上述操作全部正确 1.配置Postfix服务程序 Postfix是一款由IBM资助研发的免费开源电子邮件服务程序，能够很好地兼容Sendmail 服务程序，可以方便Sendmail用户迁移到Postfix服务上。Postfix服务程序的邮件收发能力强 于Sendmail 服务，而且能自动增加、减少进程的数量来保证电子邮件系统的高性能与稳定性。 另外，Postfix服务程序由许多小模块组成，每个小模块都可以完成特定的功能，因此可在生 产工作环境中根据需求灵活搭配。 第1步:安装Postfix服务程序 第2步:配置Postfix服务程序。 Postfix服务程序主配置文件(/etc/ postfix/main.cf)中7个最应该掌握的参数，如下表: Postfix服务程序主配置文件中的重要参数: 在Postfix服务程序的主配置文件中，总计需要修改5处: 1.在第95 行定义一个名 为myhostname的变量，用来保存服务器的主机名称: 2.在第102行定义一个名为mydomain的变量，用来保存邮件域的名称: 3.在第118行调用前面的mydomain变量，用来定义发出邮件的域。调用变量的好处是避 免重复写入信息，以及便于日后统一修改： 4.在第135行定义网卡监听地址。可以指定要使用服务器的哪些IP地址对外 提供电子邮件服务；也可以写成all，表示所有IP地址都能提供电子邮件服务： 5.在第183行定义可接收邮件的主机名或域名列表。可以直接调用前 面定义好的myhostname和mydomain变量(如果不想调用变量，也可以直接调用变量中的值)： 第3步:创建电子邮件系统的登录账户: Postfix与vsftpd服务程序一样，都可以调用本 地系统的账户和密码，因此在本地系统创建常规账户即可。最后重启配置妥当的postfix服务 程序，并将其添加到开机启动项中。 2.配置Dovecot服务程序 Dovecot是一款能够为Linux系统提供IMAP和POP3电子邮件服务的开源服务程序，安 全性极高，配置简单，执行速度快，而且占用的服务器硬件资源也较少，因此是一款值得推 荐的收件服务程序。 第1步:安装Dovecot服务程序软件包 第2步:配置部署Dovecot服务程序。Dovecot服务程序的主配置文件(/etc/dovecot/dovecot.conf)需进行如下修 改: 1.首先是第24行，把Dovecot服务程序支持的电子邮件协议修改为imap、pop3和lmtp。 然后在这一行下面添加一行参数，允许用户使用明文进行密码验证。因 为Dovecot服务程序为了保证电子邮件系统的安全而默认强制用户使用加密方式进行登录， 而由于当前还没有加密系统，因此需要添加该参数来允许用户的明文登录。 2.在主配置文件的第49行，设置允许登录的网段地址，可以在这里限制只有 来自于某个网段的用户才能使用电子邮件系统。如果想允许所有人都能使用，则不用修改本 参数： 第3步:配置邮件格式与存储路径: 在Dovecot服务程序单独的子配置文件(/etc/dovecot/conf.d/10-mail.conf)中，定义一 个路径，用于指定将收到的邮件存放到服务器本地的哪个位置。路径默认已经定义好 了，只需将该配置文件中第25行前面的井号(#)删除即可。 第4步:切换到配置Postfix服务程序时创建的boss账户，在家目录中建立用于保存邮件的 目录。重启Dovecot服务并将其添加到开机启动项中。至此，对Dovecot服务程序的 配置部署步骤全部结束。 把上面提到的邮件协议在防火墙中的策略 予以放行，这样客户端就能正常访问了： 3.客户使用电子邮件系统 使用Windows操作系统中自带 的Outlook软件来进行测试(也可以使用其他电子邮件客户端来测试，比如Foxmail)。按 下表设置电子邮件系统及DNS服务器和客户端主机的IP地址，以便能正常解析邮件 域名。设置后的结果如下图所示。 服务器与客户端的操作系统与IP地址: 主机名称 操作系统 IP地址 电子邮件系统及DNS服务器 RHEL 8 192.168.1.31 客户端主机 Windows 11 192.168.1.41 第1步:在Windows 11系统中运行Outlook软件程序。在初次运行该软件时会出现一个”Microsoft Outlook 2010 启动” 页面，引导完成软件的配置过程 第2步:配置电子邮件账户: 在下图所示的“账户配置”页面中单击“是”单选按钮， 然后单击“下一步”按钮。 第3步:填写电子邮件账户信息，在下图所示的页面中，在“您的姓名”文本框中输 入您的名字（可以为自定义的任意名字），在“电子邮件地址”文本框中输入服务器系统内的 账户名和发件域，在“密码”文本框中输入该账户在服务器内的登录密码。填写完毕之后， 单击“下一步”按钮。 第4步:进行电子邮件服务登录验证。由于当前没有可用的SSL加密服务，因此在Dovecot 服务程序的主配置文件中写入了一条参数，让用户可以使用明文登录到电子邮件服务。 Outlook软件默认会通过 SSL加密协议尝试登录电子邮件服务，所以在进行图1所示的“搜 索luoyu@linuxprobe.com服务器设置”大约30～60秒后，系统会出现登录失败的报错信息。 此时只需再次单击“下一步”按钮，即可让Outlook软件通过非加密的方式验证登录，如图2所示。最后验证成功的界面如图3所示，点击“完成”按钮: 第5步:向其他信箱发送邮件。在成功登录Outlook软件后即可尝试编写并发送新邮件 了。只需在软件界面的空白处单击鼠标右键，在弹出的菜单中单击“新建电子邮件”选项，然后在邮件界面中填写收件人的信箱地址以及完整的邮件内容后单击“发送”按 钮: 10 使用iSCSI服务部署网络存储10.1 iSCSI技术介绍硬盘是计算机硬件设备中重要的组成部分之一，硬盘存储设备读写速度的快慢也会对服 务器的整体性能造成影响。硬盘存储结构、RAID磁盘阵列技术以及 LVM技术等都是用于存储设备的技术，尽管这些技术有软件层面和硬件层面之分，但都旨在解决硬盘存储设备的读写速度问题，或者竭力保障存储数据的安全。 为了进一步提升硬盘存储设备的读写速度和性能，一直在努力改进物理硬盘设备的 接口协议。当前的硬盘接口类型主要有IDE、SCSI和SATA这3种: IDE:成熟稳定、价格便宜的并行传输接口 SATA:传输速度更快、数据校验更完整的串行传输接口 SCSI:用于计算机和硬盘、光驱等设备之间系统级接口的通用标准，具有系统资源占用率低、转速高、传输速度快等优点 无论使用什么类型的硬盘接口，硬盘上的数据总是要通过计算机主板上的总线与CPU、 内存设备进行数据交换，物理环境上的限制给硬盘资源的共享带来了各种不便。后来， IBM公司开始动手研发基于TCP/IP协议和SCSI接口协议的新型存储技术，就是目 前能看到的互联网小型计算机系统接口(iSCSI)。这是一种将SCSI接口与以太网技术相结合的新型存储技术，用来在网络中传输SCSI接 口的命令和数据。不仅克服了传统SCSI接口设备的物理局限性，实现了跨区域的存储 资源共享，而且可以在不停机的状态下扩展存储容量。 首先，iSCSI存储技术非常便捷，在访问存储 资源的形式上发生了很大变化，摆脱了物理环境的限制，同时还可以把存储资源分给多个服 务器共同使用，因此是一种非常推荐使用的存储技术。但是，iSCSI存储技术受到了网速的制 约。以往硬盘设备直接通过主板上的总线进行数据传输，现在则需要让互联网作为数据传输 的载体和通道，因此传输速率和稳定性是iSCSI技术的瓶颈。随着网络技术的持续发展，相 信iSCSI技术也会随之得以改善。 既然要通过以太网来传输硬盘设备上的数据，那么数据是通过网卡传入到计算机中的 么？与一般的网卡不同(连接网络总 线和内存，供计算机上网使用)，iSCSI-HBA卡连接的是SCSI接口或FC(光纤通道)总线 和内存，专门用于在主机之间交换存储数据，其使用的协议也与一般网卡有本质的不同。运 行Linux系统的服务器会基于iSCSI协议把硬盘设备命令与数据打包成标准的TCP/IP数据包， 然后通过以太网传输到目标存储设备，而当目标存储设备接收到这些数据包后，需要基于 iSCSI协议把TCP/IP数据包解压成硬盘设备命令与数据。 iSCSI-HBA卡实拍图: iSCSI技术具有硬件成本低、操作简单、维护方便以及扩展性强等优势，为 我们提供了数据集中化存储的服务，而且其以区块为单位的数据存储空间，在简化了存储空 间管理步骤的前提下，还增添了存储空间的弹性。对于用户而言，仿佛计算机上多了一块新 的”本地硬盘”，可以使用本地的计算机操作系统进行管理，就像是使用本地硬盘那样来使用 远程存储空间。这种高扩展性和低组建成本、低维护成本的整合存储方式，正是大部分预算 受限的中小企业和办公室所需要的。 10.2 创建RAID磁盘阵列使用iSCSI存储技术为远程用户提供共享存储资源的前提是保障用于存放资源的服务器的稳定性与可用性，否则一旦在使用过程中出现故障，则维护的难度相较于本地硬盘 设备要更加复杂、困难。 首先在虚拟机中添加4块新硬盘，用于创建RAID 5磁盘阵列和备份盘，如下图所示: 添加4块用于创建RAID 5级别磁盘阵列的新硬盘: 启动虚拟机系统，使用mdadm命令创建RAID磁盘阵列。-Cv参数为创建阵列并 显示过程，/dev/md0 为生成的阵列组名称，-n 3参数为创建RAID 5磁盘阵列所需的硬盘个数， -l 5参数为RAID磁盘阵列的级别，-x 1参数为磁盘阵列的备份盘个数。在命令后面要逐一写 上使用的硬盘名称: 上述命令成功执行之后，得到一块名称为/dev/md0的新设备，这是一块RAID 5 级别的磁盘阵列，并且还有一块备份盘为硬盘数据保驾护航，可使用mdadm -D命 令查看设备的详细信息。另外，由于在使用远程设备时极有可能出现设备识别顺序发 生变化的情况，因此，如果直接在fstab挂载配置文件中写入/dev/sdb、/dev/sdc等设备 名称的话，就有可能在下一次挂载了错误的存储设备。而UUID值是设备的唯一标识符， 用于精确地区分本地或远程设备。可以把这个值记录下来，一会儿准备填写到 挂载配置文件中。 10.3 配置iSCSI服务端iSCSI技术在工作形式上分为服务端(target)与客户端(initiator): iSCSI服务端即用于 存放硬盘存储资源的服务器，作为前面创建的RAID磁盘阵列的存储端，为用户提供 可用的存储资源。 iSCSI客户端则是用户使用的软件，用于访问远程服务端的存储资源。 按下表配置iSCSI服务端和客户端所用的IP地址: 主机名称 操作系统 IP地址 iSCSI服务端 RHEL 8 192.168.1.61 iSCSI客户端 RHEL 8 192.168.1.31 第1步:RHEL 8/CentOS 8系统中默认安装了iSCSI服务端程序，配置好软件仓库后安装iSCSI服务端的交换式配置工具。相较于直接修改配置文件，通 过交互式的配置过程来完成对参数的设定既又方便又安全。在 dnf命令的后面添加-y参数后， 在安装过程中就不需要再进行手动确认了： iSCSI是跨平台的协议，因此用户也可以在Windows系统下搭建iSCSI服务端，再共享 给Linux系统主机。不过类似于DataCore软件公司推出的 SANmelody或FalconStor软件公司推出的iSCSI Server for Windows等软件，在Windows 系统上使用都是要收费的。 第2步:配置iSCSI服务端共享资源: targetcli是用于管理iSCSI服务端存储资源的专用配置命令，提供类似fdisk命令的交互式配置功能，将iSCSI共享资源的配置内容抽 象成”目录”的形式，只需将各类配置信息填入到相应的”目录”中即可。难点 主要在于认识每个”参数目录”的作用。当把配置参数正确地填写到”目录”中后，iSCSI 服务端也就可以提供共享资源服务了。 执行targetcli命令后就能看到交互式的配置界面了。在该界面中允许使用很多Linux 命令，比如利用ls查看目录参数的结构，使用cd切换到不同的目录中。 /backstores/block是iSCSI服务端配置共享设备的位置。需要把刚刚创建的RAID 5 磁盘阵列 md0文件加入到配置共享设备的”资源池”中，并将该文件重新命名为disk0，这样 用户就不会知道是由服务器中的哪块硬盘来提供共享存储资源，而只会看到一个名为disk0 的存储设备。 第3步:创建iSCSI target名称及配置共享资源。iSCSI target名称是由系统自动生成的， 这是一串用于描述共享资源的唯一字符串。在扫描iSCSI服务端时即可看到这个字 符串。 注明:在iSCSI自动生成的名称中，最后一个.为句号，不是名称中的一部分。 系统在生成这个target名称后，还会在/iscsi参数目录中创建一个与其字符串同名的新 “目录”用来存放共享资源。需要把前面加入到iSCSI共享资源池中的硬盘设备添加到 这个新目录中，这样用户在登录iSCSI服务端后，即可默认使用这硬盘设备提供的共享存储 资源了。 第4步:设置访问控制列表(ACL): iSCSI协议是通过客户端名称进行验证的。也就是 说，用户在访问存储共享资源时不需要输入密码，只要iSCSI客户端的名称与服务端中设置 的访问控制列表中某一名称条目一致即可，因此需要在 iSCSI服务端的配置文件中写入一串 能够验证用户信息的名称。acls参数目录用于存放能够访问 iSCSI服务端共享存储资源的客户 端名称。在刚刚系统生成的iSCSI target后面追加上类似于:client 的参数，这样既能保证 客户端的名称具有唯一性，又非常便于管理和阅读： 第5步:设置iSCSI服务端的监听IP地址和端口号。生产环境中的服务器上可能有 多块网卡，那么到底是由哪个网卡或IP地址对外提供共享存储资源呢？配置文件中默认是 允许所有网卡提供iSCSI服务，如果认为这有些许不安全，可以手动删除： 使系统使用服务器IP地址 192.168.1.61的3260端口向外提供iSCSI共 享存储资源服务： 第6步:在参数文件配置妥当后，浏览刚刚配置的信息，确保上述提到的“目录”都已 经填写了正确的内容。在确认信息无误后输入exit命令退出配置。 注意，千万不要习惯性地 按Ctrl + C组合键结束进程，这样不会保存配置文件，工作也就白费了。 清空iptables防火墙中的默认策略，设置firewalld防火墙，使其放行iSCSI服务或3260/TCP 端口号： 10.4 配置Linux客户端无论是什么服务，客 户端的配置步骤都要比服务端的配置步骤简单一些。在RHEL 8系统中，已经默认安装了iSCSI 客户端服务程序initiator。如果系统没有安装，可以使用软件仓库手动安装。 iSCSI协议是通过客户端的名称来进行验证的，而该名称也是iSCSI客户端的 唯一标识，必须与服务端配置文件中访问控制列表中的信息一致，否则客户端在尝试访 问存储共享设备时，系统会弹出验证失败的保存信息。 编辑iSCSI客户端中的initiator名称文件(/etc/iscsi/initiatorname.iscsi)，把服务端的访问控制列表名称填写进来， 然后重启客户端iscsid服务程序并将其加入到开机启动项中： iSCSI客户端访问并使用共享存储资源的基本操作为”先 发现，再登录，最后挂载并使用”。 iscsiadm是用于管理、查询、插入、更新或删除iSCSI数据库 配置文件的命令行工具，用户需要先使用这个工具扫描发现远程iSCSI服务端，然后查看找到的 服务端上有哪些可用的共享存储资源。其中，-m discovery参数的目的是扫描并发现可用的存储 资源，-t st参数为执行扫描操作的类型，-p 192.168.1.61参数为iSCSI服务端的IP地址： 在使用iscsiadm命令发现了远程服务器上可用的存储资源后，接下来准备登录iSCSI服 务端。其中，-m node参数为将客户端所在主机作为一台节点服务器，-T参数为要使用的存储 资源，-p 192.168.1.61参数 依然为对方iSCSI服务端的IP地址。最后使用–login或-l参数进行登录验证。 [root@linuxprobe ~]# iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.linuxprobe.x8664:sn.5eeae0bacdc0 -p 192.168.1.61 –login 在iSCSI客户端成功登录之后，会在客户端主机上多出一块名为/dev/sdc的设备文件。udev服务在命名硬盘名称时，与硬盘插槽是没有关系的。接下来便能够像使 用本地主机上的硬盘那样来操作这个设备文件了。 下面进入标准的磁盘操作流程。这个 设备文件本身只有40GB的容量，因此不必进行分区，而是直接格式化并挂载使用。 使用df命令查看挂载情况: 从此以后，这个设备文件就如同是客户端本机上的硬盘那样工作。由于 udev服务是按照系统识别硬盘设备的顺序来命名硬盘设备的，当客户端主机同时使用多个远程存 储资源时，如果下一次识别远程设备的顺序发生了变化，则客户端挂载目录中的文件也将随之混 乱。为了防止发生这样的问题，应该在/etc/fstab配置文件中使用设备的UUID进行挂载。这样， 不论远程设备资源的识别顺序再怎么变化，系统也能正确找到设备所对应的目录。 blkid命令用于查看设备的名称、文件系统及UUID。可使用管道符进 行过滤，只显示与/dev/sdc设备相关的信息： 由于/dev/sdc是一块网络存储设备，而iSCSI协议是基于 TCP/IP网络传输数据的，因此必须在/etc/fstab配置文件中添加上_netdev参数，表示当系统联 网后再进行挂载操作，以免系统开机时间过长或开机失败： 如果不再需要使用iSCSI共享设备资源了，可以用iscsiadm命令的-u参数将其设备卸载： [root@linuxprobe ~]# iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.linuxprobe.x8664:sn.5eeae0bacdc0 -u 这种获取iSCSI远程存储的方法依赖的是RHEL 8系统自带的iSCSI initiator软件程序。该 软件程序将以太网卡虚拟成iSCSI卡，进而接收数据，然后基 TCP/IP协议在主机与iSCSI存 储设备之间实现数据传输功能。这种方式仅需主机与网络即可实现，因此成本是最低的。但是， 在采用这种方式传输数据时，与iSCSI和TCP/IP相关的命令数据会消耗客户端自身的CPU计 算性能，因此存在一定的额外开销。一般建议在低I/O或者低带宽要求的环境中使用这种方式。 如果在后续的生产环境中需要进行大量的远程数据存储，建议自行配备iSCSI HBA(主机总线适配器)硬件卡设备，并将其安装到iSCSI服务器上，从而实现iSCSI 服务器与交换机之间、iSCSI服务器与客户端之间的高效数据传输。与initiator的软件方式相 比，iSCSI HBA硬件卡设备不需要消耗CPU 计算性能，而且它是专用的远程数据存储设备， 因此对iSCSI的支持也会更好。但是，iSCSI HBA硬件卡设备的价格会稍微贵一些，需 要在性能和成本之间进行权衡。 10.5 配置Windows客户端使用Windows系统的客户端也可以正常访问iSCSI服务器上的共享存储资源，操作 原理及步骤与 Linux系统的客户端基本相同。在进行下面的实验之前，请先关闭Linux系统 客户端，以免这两台客户端主机同时使用iSCSI共享存储资源而产生潜在问题。按下表配置iSCSI服务器和Windows客户端所用的IP地址。 主机名称 操作系统 IP地址 iSCSI服务端 RHEL 8 192.168.1.61 Windows系统客户端 Windows 11 192.168.1.71 第1步:运行iSCSI发起程序。在 Windows 11操作系统中已经默认安装了iSCSI客户端 程序，在控制面板中找到”Windows 工具”标签，点击后然后单击”iSCSI发起程序”(见图1)，在第一次运行iSCSI 发起程序时，系统会提示“Microsoft iSCSI 服务端未运行”，单击“是”按钮即可自动启动并 运行iSCSI发起程序 第2步:扫描发现iSCSI服务端上可用的存储资源。不论是Windows系统还是Linux系 统，要想使用iSCSI共享存储资源，都必须先进行扫描发现操作。运行iSCSI发起程序后在 “目标”选项卡的“目标”文本框中写入iSCSI服务端的IP地址，然后单击“快速连接”按 钮，如下图1所示。 在弹出的“快速连接”对话框中可看到共享的硬盘存储资源，此时显示“无法登录到目 标”属于正常情况，单击“完成”按钮即可，如图2所示。 回到“目标”选项卡页面，可以看到共享存储资源的名称已经出现，如下图所示。 第3步:准备连接iSCSI服务端的共享存储资源。由于在iSCSI服务端程序上设置了ACL， 使得只有客户端名称与ACL策略中的名称保持一致时才能使用远程存储资源，因此首先需要在 “配置”选项卡中单击“更改”按钮，随后在修改界面写入iSCSI服务器配置过的 ACL策略名称，最后重新返回到 iSCSI发起程序的“目标”界面。 在确认iSCSI发起程序名称与iSCSI服务器ACL策略一致后，重新单击“连接”按钮，如下图所示，并单击“确认”按钮。大约 1～3 秒后，状态会更新为“已连接” 第4步:访问iSCSI远程共享存储资源。右键单击桌面上的“计算机”图标，打开计算 机管理程序，如下图所示。 开始对磁盘进行初始化操作，如下图所示。Windows 系统的初始化过程步骤如下面各图所示。 接下来即可进入正常的使用过程。由于整个传输过程是完全透明的，而且像一块本地硬盘 那样稳定，因此不知情的用户可能都察觉不到这是一块远程存储设备。不过，这只是理论状态， 实际上的 iSCSI数据传输速率并不能完全达到本地硬盘的性能，会或多或少地受到网络带宽的 影响，只不过差别不明显罢了。考虑到iSCSI存储技术还有一个优势，就是安全性高，这对于 数据集中存储来讲显得十分重要。因此，在进行数据存储与传输时，iSCSI值得一试！ 11 使用MariaDB数据库管理系统11.1 数据库管理系统数据库是指按照某些特定结构来存储数据资料的数据仓库。在当今这个大数据技术迅速 崛起的年代，数据库技术发展到了现如今存储海量数据的大型分布式模式。 数据库管理系统能够对数据库中存放的数据进行建立、修改、删除、查找、维护等操 作的软件程序。通过把计算机中具体的物理数据转换成适合用户理解的抽象逻辑数据，Linux运维工作的工程师也可以对数据库进行基本 的管理操作。 MariaDB和MySQL的Logo如下图所示: MariaDB当前由开源社区进行维护，是MySQL的分支产品，与MySQL具有高度的 兼容性，与 MySQL API和命令均保持一致。MariaDB自带了一个新的存储引擎Aria， 用于替代MyISAM。因此，MariaDB与MySQL 一样好用。 由于各大公司之间存在着竞争关系或利益关系，外加MySQL在被收购之后逐渐 由开源向闭源软件转变，很多公司抛弃了MySQL。谷歌、维基百科等决定将MySQL数据 库上的业务转移到MariaDB数据库，Linux开源系统的领袖红帽公司也决定在RHEL 8、CentOS 8 以及最新的Fedora系统中，将MariaDB作为默认的数据库管理系统，而且红帽公司更是将数据 库知识加入到了RHCE认证的考试内容中。随后，还有数十个常见的Linux系统(如openSUSE、 Slackware等)也做出了同样的表态。 虽然IT行业巨头都决定采用MariaDB数据库管系统，但并不意味着 MariaDB较之于MySQL有明显的优势。MariaDB和MySQL 在性能上基本保持一致，两者的操作命令也十分相似。从务实的角度来讲，在掌握了MariaDB数据 库的命令和基本操作之后，在今后的工作中即使遇到 MySQL数据库，也可以快速上手。 11.2 初始化mariadb服务MariaDB数据库管理系统有了很多新鲜的扩展特性，例如对微秒级别的支持、线程池、子查询优化、进程报告等。在配置妥当软件仓库后，即可安装部署MariaDB 数据库主程序及服务端程序了。 安装完毕后，启动服务程序，并将其加入到开机启动项中： 为了确保数据 库的安全性和正常运转，需要先对数据库程序进行下面5步的初始化操作: 1.设置root管理员在数据库中的密码值(该密码并非root管理员在系统中的密码， 这里的密码值默认应该为空，可直接按回车键) 2.设置root管理员在数据库中的专有密码 3.删除匿名用户，并使用root管理员从远程登录数据库，以确保数据库上运行的业务的 安全性 4.删除默认的测试数据库，取消测试数据库的一系列访问权限 5.刷新授权列表，让初始化的设定立即生效 很多生产环境中都需要使用站库分离的技术(网站和数据库不在同一个服务器上)， 如果需要让 root管理员远程访问数据库，可在上面的初始化操作中设置策略，以允许root管 理员从远程访问。然后还需要设置防火墙，使其放行对数据库服务程序的访问请求。数据库 服务程序默认占用3306端口，在防火墙策略中服务名称统一叫作mysql： 首次登录MariaDB数据库。管理数据库的命令为mysql，其中， -u参数用来指定以root管理员的身份登录，而-p参数用来验证该用户在数据库中的密码值。 输入help命令查看mariadb服务能做的操作，语 句的用法与MySQL一模一样： 登录MariaDB数据库后执行数据库命令时，都需要在命令后面用分号(;)结尾，这也 是与Linux命令最显著的区别。执行如下命 令查看数据库管理系统中当前都有哪些数据库： 使用数据库命令将root管理员在数据库管理系统中的密码值修改 为linuxprobe 输入新密码(linuxprobe)后，便可顺利进入数据库管理工具中： 11.3 管理用户以及授权为了保障数据库系统的安全性，以及让 其他用户协同管理数据库，可以在MariaDB数据库管理系统中创建多个专用的数据库 管理用户，然后再分配合理的权限，以满足工作需求。为此，可使用root 管理员登录 数据库管理系统，然后按照”CREATE USER 用户名@主机名 IDENTIFIED BY ‘密码’;”的格 式创建数据库管理用户。 创建的用户信息可以使用SELECT命令语句来查询。下面命令查询的是用户luke的主机 名称、用户名称以及经过加密的密码值信息： 用户luke仅仅是一位普通用户，没有数据库的任何操作权限。切 换到luke用户来查询数据库管理系统中当前都有哪些数据库。可以发现，该用户甚至没法查 看完整的数据库列表: 数据库管理系统所使用的命令一般都比较复杂。以GRANT命令为例进行说明: GRANT命令用于为用户进行授权，常见格式如下表所示。在使用GRANT命令时需要 写上要赋予的权限、数据库及表单名称，以及对应的用户及主机信息。 GRANT命令的常见格式以及解释: 用户的授权工作肯定是需要数据库管理员来执行的。以root管理员的身份登 录到数据库管理系统中，针对mysql数据库中的user表单向用户luke授予查询、更新、删除 以及插入等权限。 在执行完上述授权操作之后，再查看一下用户luke的权限： 上面输出信息显示用户luke拥有了针对mysql数据库中user表单的一系列权限了。 再切换到用户luke，就能够看到mysql数据库了，而且还能看到表单user(其 余表单会因无权限而被继续隐藏)： 切换回root管 理员用户，移除刚才的授权。 除了移除授权的命令(REVOKE)与授权命令(GRANTS)不同之外，其余部分 都是一致的。执行移除授权命令后，再来查看用户luke的信息： 不再需要某个用户时，可以直接用DROP命令将其删除： 11.4 创建数据库与表单MariaDB数据库管理系统中，一个数据库可以存放多个数据表，数据表单是数据库中最核心的内容。可根据需求定义数据库表结构，然后在其中合理地存放数据， 以便后期轻松地维护和修改。 用于创建数据库的命令以及作用: 建立数据库是管理数据的起点。 创建一个名为linuxprobe的数据库，然后再查 看数据库列表，此时就能看到它了： MariaDB与MySQL同属关系型数据库。关系型数据库有些类似于表格的概念，一个关系型数据库由一个或多个表格/表单组成，如下图所示: 在下图中，表头表示每一列的名称；列表示具有相同数据类型的数据集合；行表示用 来描述事物的具体信息；值表示行的具体信息，每个值均与该列的其他数据类型相同；键表 示用来识别某个特定事物的方法，在当前列中具有唯一性。 数据库存储概念: 在新建的linuxprobe数据库中创建表单mybook，然后进行表单的初始化，即定义 存储数据内容的结构。分别定义3个字段项，其中，字符型字段name(长度为15字符) 用来存放图书名称，整型字段 price和pages分别存储图书的价格和页数。当执行完下述命令 之后，就可以看到表单的结构信息： 11.5 管理表单及数据向mybook数据表单中插入一条图书信息。需要使用INSERT命令，并在命 令中写清表单名称以及对应的字段项。使用该 命令插入一条图书信息，书名为linuxprobe，价格和页数分别是60元和518页。命令 执行后也就意味着图书信息已经成功写入到数据表单中，然后就可以查询表单中的内容了。 在使用SELECT命令查询表单内容时，需要加上想要查询的字段；如果想查看表单中的所有 内容，则可以使用星号(*)通配符来显示： 对数据库运维人员来讲，需要会的操作是增、删、改、查。创建数据表 单并在其中插入内容是第一步，还需掌握数据表单内容的修改方法。例如，可以使用 UPDATE命令将刚才插入的linuxprobe图书信息的价格修改为55元，然后再使用SELECT命 令查看该图书的名称和定价信息。注意，因为这里只查看图书的名称和定价，而不涉及页码， 所以无须再用星号通配符来显示所有内容。 修改指定的某一条记录，用WHERE命令进行限定即可。先插入两条 图书信息： 使用WHERE命令仅将名称为linuxcool的图书价格修改为60元，不影响其他图 书信息： 还可以使用DELETE命令删除某个数据表单中的内容。下面使用DELETE命令删除 数据表单mybook中的所有内容，然后再查看该表单中的内容，可以发现该表单内容为 空了： 数据表单中一般会存放成千上万条数据信息。比如刚刚创建的用于保存图书 信息的mybook表单，随着时间的推移，里面的图书信息也会越来越多。在这样的情况下， 如果只想查看其价格大于某个数值的图书，又该如何定义查询语句呢？ 先使用INSERT插入命令依次插入4条图书信息： 要想让查询结果更加精准，就需要结合使用SELECT与WHERE命令了。WHERE 命令是在数据库中进行匹配查询的条件命令。通过设置查询条件，就可以仅查找出符合该条 件的数据。下表列出了WHERE命令中常用的查询参数以及作用: 分别在mybook表单中查找出价格大于75元或价格不等于80元的 图书，其对应的命令如下所示: 匹配的条件越多，获得的信息就越精准。在WHERE命令的后面追加AND操作符，可 以进行多次匹配。例如，执行下述命令，找到价格为30元、页数为158的图书的名称： 11.6 数据库的备份与恢复mysqldump命令用于备份数据库数据，格式为”mysqldump [参数] [数据库名称]”。参数与mysql命令大致相同，-u参数用于定义登录数据库的用户名称，-p参数表示密码提示 符。 将linuxprobe数据库中的内容导出为一个文件，并保存到root管理员的家目录中： 进入MariaDB数据库管理系统，彻底删除linuxprobe数据库，这样mybook数据表 单也将被彻底删除。然后重新建立linuxprobe数据库： 使用输入重定向符把刚刚备份的数据库文件导入。登录MariaDB数据库，就又能看到linuxprobe数据 库以及mybook数据表单了。 12 使用PXE+Kickstart无人值守安装服务12.1 无人值守系统如果生产环境中有数百台服务器都需要安装系统，就需要使用PXE+TFTP+FTP+DHCP+Kickstart服务搭建出一个无人 值守安装系统。这种无人值守安装系统可以自动地为数十台甚至上百台的服务器安装系统。 无人值守安装系统的工作流程如下图所示: PXE是Intel公司开发的技术， 让计算机通过网络来启动操作系统(前提是计算机上安装的网卡支持PXE技术)，主要用于 在无人值守安装系统中引导客户端主机安装Linux操作系统。Kickstart是一种无人值守的安 装方式，其工作原理是预先把原本需要运维人员手工填写的参数保存成一个ks.cfg文件，当 安装过程中需要填写参数时则自动匹配Kickstart生成的文件。所以只要Kickstart文件包含了安装过程中需要人工填写的所有参数，那么从理论上来讲完全不需要运维人员的干预，就可 以自动完成安装工作。 由于当前的客户端主机并没有完整的操作系统，因此也就不能完成 FTP 协议的验证了， 所以需要使用TFTP协议帮助客户端获取引导及驱动文件。vsftpd服务程序用于将完整的系统 安装镜像通过网络传输给客户端。当然，只要能将系统安装镜像成功传输给客户端即可，因 此也可以使用httpd来替代vsftpd服务程序。 12.2 部署相关服务程序在本章会部署多款服务，这些服务以及作用如下表所示: 接下来实验中即将用到的服务及作用: 1.配置DHCP服务程序 DHCP服务程序用于为客户端主机分配可用的IP地址，这是服务器与客户端主机进 行文件传输的基础，因此要先行配置DHCP服务程序。 首先按下表为无人值守系统设置 IP地址，然后按照图1和图2在虚拟机的虚拟网络编辑器中关闭自身的DHCP服务， 避免与配置的服务冲突。 主机名称 操作系统 IP地址 无人值守系统 RHEL 8 192.168.1.41 客户端 未安装操作系统 除了上面提及的服务之外，PXE+KickStart无人值守安装系统还会用到诸如sips、slp、 mountd 等多项相关的服务协议，因此本实验会临时关闭firewalld防火墙和selinux，以便数据能够正常 地传送： selinux的配置文件路径:/etc/selinux/config，修改后重启生效 挂载好光盘镜像并把仓库文件配置妥当后，就可以安装DHCP服务程序软件包了： 这里使用的配置文件(/etc/dhcp/dhcpd.conf)与第14章中的配置文件有两个主要区别: 1.允许了BOOTP引导程序协议，旨在让局域网内暂时没有操作系统的主机也能获取 静态IP地址 2.在配置文件的最下面加载了引导驱动文件pxelinux.0，目的是让客户端主机获取到IP地址后主动获取引导驱动文件，自行进入下一 步的安装过程。 确认DHCP服务程序的参数都填写正确后，重新启动该服务程序，并将其添加到开机启动 项中。这样在设备下一次重启之后，可以在无须人工干预的情况下，自动为客户端主机安装系统。 RHEL 8系统中存在一些”讨厌”的服务，参数错误会 导致服务启动失败，但有时却不会在屏幕上向用户显示任何提示信息。建议在启动dhcpd后 查看一下服务状态，以免后续实验中客户端分配不到网卡信息。若输出状态为”active (running)”则表示服务已正常运行: 2.配置TFTP服务程序 vsftpd是一款功能丰富的文件传 输服务程序，允许用户以匿名开放模式、本地用户模式、虚拟用户模式来进行访问认证。但 当前的客户端主机还没有安装操作系统，该如何进行登录认证呢？TFTP作为一种基于 UDP协议的简单文件传输协议，不需要进行用户认证即可获取到所需的文件资源。因此接下 来配置TFTP服务程序，为客户端主机提供引导及驱动文件。当客户端主机有了基本的驱动 程序之后，再通过vsftpd服务程序将完整的光盘镜像文件传输过去。 TFTP是一种非常精简的文件传输服务程序，运行和关闭是由xinetd网络守护进程服 务来管理的。xinetd服务程序会同时监听系统的多个端口，然后根据用户请求的端口号调取相 应的服务程序来响应用户的请求。需要开启TFTP服务程序时，只需在xinetd服务程序的配 置文件中把disable参数改成no 就可以了。如果配置文件不存在，则复制下面的内容进来， 手动创建一下： xinetd服务程序的配 置文件路径:/etc/xinetd.d/tftp 保存配置文件并退出，然后重启xinetd服务程序，并将其加入到开机启动项中: 3.配置SYSLinux服务程序 SYSLinux是一个用于提供引导加载的服务程序，也是一个包含了很多引导文件的文件夹。在安装好SYSLinux服务程序后，/usr/share/syslinux 目录中会出现很多引导文件。 首先需要把SYSLinux提供的引导文件(前文提到的文件pxelinux.0)复制 到TFTP服务程序的默认目录中，这样客户端主机就能够顺利地获取到引导文件了。另外在 RHEL 8系统光盘镜像中也有一些需要调取的引导文件。确认光盘镜像已经被挂载到 /media/cdrom目录后，使用复制命令将光盘镜像中自带的一些引导文件也复制到TFTP服务 程序的默认目录中。 cp命令后面接的句点(.)表示当前工作目录。上述cp命令表示将文件复制 到当前工作目录(/var/lib/tftpboot)中。复制过程中，若多个目录保存着相同的文件，手动敲击y键进行覆盖即可。 在TFTP服务程序的目录中新建pxelinux.cfg目录。虽然该目录的名字带有后缀，但 依然也是目录，而非文件！将系统光盘中的开机选项菜单复制到该目录中，并命名为default。 这个default文件就是开机时的选项菜单，如下图所示。 Linux系统的引导菜单界面: 默认的开机菜单中有3个选项：安装系统、对安装介质进行检验、排错模式。 编辑default文件(pxelinux.cfg/default)，把第1行的default参数修改为linux，系统在开机 时就会默认执行那个名称为linux的选项了。对应的linux选项大约在第64行，将默认的光盘 镜像安装方式修改成FTP文件传输方式，并指定好光盘镜像的获取网址以及Kickstart应答文 件的获取路径： 在安装源的后面加入quiet参数，意为使用静默安装方式，不再需要用户进行确认。 文件修改完毕后保存即可。开机选项菜单是被调用的文件，因此不需要单独重启任何服务。 4.配置vsftpd服务程序 在这套无人值守安装系统的服务中，光盘镜像是通过FTP协议传输的，因此要用到 vsftpd服务程序，也可以使用httpd服务程序来提供Web网站访问的方式，只要能确保 将光盘镜像顺利传输给客户端主机即可。如果打算使用Web网站服务来提供光盘镜像，需将上面配置文件中的光盘镜像获取网址和Kickstart应答文件获取网址修改一下。 RHEL 8系统版本的vsftpd服务默认不允许匿名公开访问模式，因此需要手动进行开启： vsftpd主配置文件路径:/etc/vsftpd/vsftpd.conf 在配置文件修改正确之后，将相应的服务程序添加到开机启 动项中，这样无论是在生产环境中还是在红帽认证考试中，都可以在设备重启之后依然能提供相应的服务。 确认系统光盘镜像已经正常挂载到/media/cdrom目录后，把目录中的光盘镜像文件全 部复制到vsftpd服务程序的工作目录中： 将SELinux安全子系统中放行 FTP传输协议的允许策略，设置成on(开启)。 5.创建Kickstart应答文件 使用PXE + Kickstart 部署的是一套”无人值守安装系统服务”，而不是”无人 值守传输系统光盘镜像服务”，因此还需要让客户端主机能够一边获取光盘镜像，一边自动帮 用户填写好安装过程中出现的选项。如果生产环境中有100台服务器，它们需要 安装相同的系统环境，那么在安装过程中单击的按钮和填写的信息也应该都是相同的。创建一个类似于备忘录的需求清单呢，在无人值守安装系统时，会从这个需 求清单中找到相应的选项值。 Kickstart其实并不是一个服务程序，而是 一个应答文件了。Kickstart应答文件中包含了系统安装过程中需要使用的选项和参数 信息，系统可以自动调取这个应答文件的内容，从而彻底实现无人值守安装系统。在root管理员的家目录中有一个名为 anaconda-ks.cfg的文件，它就是应答文件。将这个文件复制到vsftpd服务程序的工作目 录中(在开机选项菜单的配置文件中已经定义了该文件的获取路径，也就是vsftpd服务程序 数据目录中的pub子目录)。使用chmod命令设置该文件的权限，确保所有人都有可读的权限， 以保证客户端主机顺利获取到应答文件及里面的内容： Kickstart的应答文件(/var/ftp/pub/ks.cfg)共有44行左右的参数和注释内容: 第1～10行表示安装硬盘的名称为sda及使用LVM技术。这便要求在后续新 建客户端虚拟机时，硬盘一定要选择SCSI或SATA类型的，否则会变成/dev/hd 或/dev/nvme开头的名称，进而会因找不到硬盘设备而终止安装进程。 第8行的软件仓库，应改为由FTP服务器提供的网络路径。 第10行的安装源，也需要 由CDROM改为网络安装源： 第11～20行，keyboard参数为硬盘类型，一般都不需要修改。但第17行 的网卡信息，要让网卡默认处于DHCP模式，否则几十、上百台主机同时被创建出来 后，会因IP地址相互冲突而导致后续无法管理。 第21行～30行，timezone参数定义了系统默认时区为”上海”。如果服务器时 间不准确，则按照如下修改即可。第29行，创建了一个普通用户，密码值可复制/etc/shadow 文件中的加密密文，它由系统自动创建。 第31～44行表示要安装的软件来源。graphical-server-environment即带有图形化界 面的服务器环境，它对应的是安装界面中的Server With GUI选项。 实际算下来的修改并不多，默认参数就已经非常合适了。最后预览一下ks.cfg 文件的全貌。生产环境中需要用到这个文件，则可以直接复制并使用下面的内容： Kickstart应答文件将使用FTP服务进行传输，然后由安装向导进行调用，因此也不需要 重启任何服务。 12.3 自动部署客户机按上文的方法成功部署各个相关的服务程序后，就可以使用PXE + Kickstart无 人值守安装系统了。在采用下面的步骤建立虚拟主机时，一定要把客户端的网络模式设定成 与服务端一致的“仅主机模式”，否则两台设备无法进行通信，也就更别提自动安装系统了。 其余硬件配置选项并没有强制性要求，参考这里的配置选项来设定。 第1步:打开”新建虚拟机向导”程序，选择”自定义(高级)”配置类型，然后单击”下 一步”按钮，如下图所示。在随后的虚拟机硬件兼容性选项中，选择默认的”Workstation 17.5 or later” 第2步:将虚拟机操作系统的安装来源设置为”稍后安装操作系统”。这样做是让 虚拟机真正从网络中获取系统安装镜像，同时避免VMware Workstation虚拟机软件按照 内设的方法自行安装系统。单击”下一步”按钮，如下图所示。 第3步:将”客户机操作系统”设置为Linux，版本为”Red Hat Enterprise Linux 8 64位”， 然后单击”下一步”按钮，如下图所示。 第4步:对虚拟机进行命名并设置安装位置。然后单击”下一步”按钮，如下图所示。在随后设置 虚拟机处理器的个数及核心数、内存容量值时，根据实际情况自行选择。 第5步:设置虚拟机主机的网络连接类型为”使用仅主机模式网络”，如下图所示。 一定要确保服务器与客户端同处于相同的网络模式，否则客户端无法获得从服务器传送过来 的系统镜像及应答文件。随后的SCSI控制器类型选择默认的LSI Logic 第6步:设置硬盘类型并指定容量。设置”虚拟磁盘类型”为SCSI或SATA，如下图1所示。随后在硬盘创建确认界面，选择”创建新虚拟磁盘”选项。 将”最大磁盘大小”设置为20GB。这个20GB指的是虚拟机系统能 够使用的最大上限，而不是会被立即占满，因此设置得稍微大一些也没有关系。然后单击”下一步”按钮，如下图2所示。随后的确认硬盘文件名称界面选择默认值即可 第7步:结束”新建虚拟机向导程序”后，先不要着急打开虚拟机系统。单 击图1中的”自定义硬件”按钮，在弹出的如图2所示的界面中，把”网络适配器” 设备同样也设置为”仅主机模式”，移除其他不需要的硬件，然后单击 “确定”按钮。 现在同时准备好了PXE + Kickstart无人值守安装系统与虚拟主机。生产环境 中，只需要将配置妥当的服务器上架，接通服务器和客户端主机之间的网线，然后启动 客户端主机即可。直到安装完毕时才需要运维人员进行简单的初始化 工作。","link":"/2025/04/19/Linux/Linux%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9C%8D%E5%8A%A1/"},{"title":"Linux的基本使用","text":"1 访问命令行 1.1 登录Linux系统12345678图形化：系统菜单-注销-或切换用户字符界面：Ctrl+alt+F2-F6 在本机上切换用户：su - root退出登录：ctrl+d、exit、logout)网络登录：ssh ip、ssh 主机名[kiosk@foundation0 ~]$ ssh serverassh 用户名@ip/主机名[kiosk@foundation0 ~]$ ssh student@servera 1.2 终端切换12CLI Ctrl+alt+Fx xin （2，6）GUI Ctrl+alt+F1 1.3 Shell简介&emsp;&emsp;是一个解释器，可以帮助用户将指令信息传递内核 &emsp;&emsp;红帽企业Linux中为用户提供的默认shell是bash，bash是与UNIX类似的系统上使用的其中一个最成功的shell改进版本 1.4 基本组成123456789[kiosk@foundation0 ~]$ #$普通用户[kiosk@foundation0 ~]$ su - root #切换用户：su - 用户名Password: #输入用户登录密码Last login: Sat Feb 22 15:11:13 CST 2020 on tty3[root@foundation0 ~]# #超级用户ctrl+d or exit 退出登录 注明： &emsp;&emsp;1.bash shell在概念上与微软的cmd相似，但bash具有更加复杂的脚本语言 &emsp;&emsp;2.与win系统powershell类似、mac的管理终端使用工具也是使用的bash shell 1.5 GNOME Shell123456789ALT+F2 输入 gnome-terminal # 启动终端win+l # 锁定# 关闭 重启 init 0 init 6 poweroff reboot systemctl poweroff systemctl reboot shutdown -h 20：00 shutdown -r 0ctrl+alt 上\\|下 箭头 # 工作区切换 1.6 Shell的特性1.6.1 linux命令语法&emsp;&emsp;完成具体功能的命令、扩展该命令功能的选项、命令要操作的对象 &emsp;&emsp;cmd 【-option】 【arg1】 【arg2】 1234567891011121314简单的命令示例：whoamidatetouch file1；mkdir dir1完成某些工作的指令扩展命令功能的选项参数lsls -a ls -a ~/.bashrcls -a -l ~/.bashrcls -al ~/.bashrc 1.6.2 命令的基础分类12345678910111213141516171819202122232425262728293031323334353637383940# 回显式命令date +%Y%m%ddate +%Y-%m-%d# 交互式命令passwd# tab补全按一下是补全按两下列出可用命令tab键 输入单词或命令前面几个首字母后，保证唯一可补全，不唯一可列出能选择的命令# 历史命令-historyenv--能容纳1000条[root@servera ~]# env | grep SIZEHISTSIZE=1000[root@servera ~]# history -w [root@servera ~]# vim ~/.bash_history 记录历史命令文件，vim是一个文本工具，可以打开后面的文件，进入后:q退出[root@servera ~]# history -c 清除[root@servera ~]# history history的其他方法：！！！23 历史命令的编号！h 命令首字母当前历史命令支持的最大条数[root@foundation0 /]# grep ^HISTSIZE /etc/profileHISTSIZE=1000历史命令存放文件路径[root@foundation0 /]# set | grep HISTFILEHISTFILE=/root/.bash_historyvim /etc/profileexport HISTFILE=/root/.newfilesource /etc/profilehistory -wcat /root/.newfilectrl+R 搜索历史命令 1.7 命令行快捷键 快捷键 说明 ctrl + shift + t 当前画面添加一个标签 ctrl + shift + n 打开一个新的标签 alt + 1，alt + 2 切换标签 ctrl + shift + =，ctrl + - 扩大与缩小终端字体 ctrl + shift + w 关闭标签 1.8 Shell常用快捷键 快捷键 说明 ctrl + a 光标跳至行首 ctrl + e 光标跳至行尾 ctrl + u 从光标所在位置清空至行首 ctrl + k 从光标所在位置清空至行末 ctrl + 左箭头 光标向左跳一个单词 ctrl + 右箭头 光标向右跳一个单词 ctrl + w 回删一个单词 alt + d 删除光标后一个单词 esc + . 或 alt + . 调用之前使用过的路径，alt+.一直点可以向上翻阅路径 2 从命令行管理文件2.1 系统目录结构&emsp;&emsp;根(/)目录下每个目录的作用: 目录名 作用 bin 用户可执行目录(命令root和普通) sbin 系统可执行目录(命令root) lib 库文件目录(32位) lib64 库文件目录(64位) dev 设备文件目录dev usr 应用程序目录 var 服务器数据目录(数据日志) src 服务器数据目录 etc 配置文件目录 tmp 临时文件目录 boot 服务器启动目录(内核和启动文件) media 媒介目录(u盘、cdrom) mnt 其他挂载点 opt 第三方应用程序目录 proc 伪文件系统(内核参数、进程信息、硬件信息) sys 伪文件系统(配置文件目录、内核参数、进程信息、硬件信息) run 进程锁目录 root root管理员家目录 home 普通用户家目录 2.2 文件类型 文件类型 说明 全称 - 普通文件 file d 目录文件 directory c 字符设备文件 character b 块设备文件 block s 套接字文件 socket p 管道文件 pipe l 符号链接文件(软链接) symbolic 2.3 文件名定位文件12[root@foundation0 home]# cd /[root@foundation0 /]# cd /etc/ 2.4 路径12345678910111213141516171819202122232425262728293031# 路径的表示:1.绝对路径(通常以/开头) 例如：根开头 cd /etc/sysconfig2.相对路径: 非根开头 cd .. # 导航路径# pwd[root@foundation0 yum.repos.d]# pwd/etc/yum.repos.d# cdcd - 返回之前的目录cd or cd ~ 家目录cd . 当前目录cd .. 上一级目录# lslsls -a ls -a /home ls -a -l ls -al[root@foundation0 ~]# ls -a .viminfo.viminfo[root@foundation0 ~]# ls -a -l .viminfo-rw-------. 1 root root 2545 Mar 13 13:12 .viminfo[root@foundation0 ~]# ls -al .viminfo-rw-------. 1 root root 2545 Mar 13 13:12 .viminfo[root@foundation0 /]# ls -l -d /homedrwxr-xr-x. 4 root root 30 Mar 13 11:38 /home 2.5 查看文件内容12345678cat cat /etc/passwd # 将文件内容打印到屏幕上tail tail /var/log/message # 默认查看文件后10行。-F （追踪）指定文件不存在时再创建相同名称文件 tail -n 5 或 tail -5 /var/log/message head head /var/log/message # 默认查看文件头10行 head -5 /var/log/message less less /var/log/message more more /var/log/message vim vim /etc/passwd # 文本编辑器 2.6 命令行管理文件/目录2.6.1 管理文件/目录的命令123456 创建 touch mkdir -p 改名 mv mv 移动 mv mv 拷贝 cp cp -r 删除 rm rm -rtouch、mkdir、rm、cp、mv 2.6.2 touch命令(管理文件)123456[root@servera opt]# man touch[root@servera opt]# touch /file4 /tmp/file5[root@servera opt]# ls /file4;ls /tmp/file5/file4/tmp/file5[root@servera opt]# touch file{10..20} 2.6.3 mkdir命令(管理目录)1234567891011121314151617181920212223242526mkdir选项： -p：递归创建 -v：显示过程[root@servera opt]# lsdir1[root@servera opt]# mkdir dir2 /dir3[root@servera opt]# lsdir1 dir2[root@servera opt]# mkdir dir3/dir4mkdir: cannot create directory ‘dir3/dir4’: No such file or directory[root@servera opt]# mkdir -pv dir3/dir4mkdir: created directory 'dir3'mkdir: created directory 'dir3/dir4'[root@servera opt]# ls -R dir3 # -R递归查看，可以查看多级目录内容dir3:dir4dir3/dir4:[root@servera opt]# ll -R dir3/dir4dir3/dir4:total 0[root@servera opt]# ll dir3total 0drwxr-xr-x. 2 root root 6 Mar 13 22:23 dir4[root@servera opt]# ll dir3/dir4/total 0[root@servera opt]# ll dir3/dir4/ -ddrwxr-xr-x. 2 root root 6 Mar 13 22:23 dir3/dir4/ 2.6.4 rm命令(删除)1234567891011121314151617181920[root@servera opt]# man rm[root@servera opt]# lsdir1 dir2 dir3 file1 file2 file3[root@servera opt]# rm file1rm: remove regular empty file 'file1'? y # 询问是否删除y删除n不删除[root@servera opt]# rm file2rm: remove regular empty file 'file2'? n[root@servera opt]# rm -f file2 强制删除不询问[root@servera opt]# lsdir1 dir2 dir3 file3[root@servera opt]# rm -f file* *代表一个或多个字符[root@servera opt]# lsdir1 dir2 dir3[root@servera opt]# rm dir1 rm: cannot remove 'dir1': Is a directory[root@servera opt]# rm -r dir1 删除目录需要-r表示递归rm: remove directory 'dir1'? y[root@servera opt]# rm -rf dir2[root@servera opt]# lsdir3 2.6.5 copy命令(复制)1234567891011[root@servera opt]# lsdir1 dir2 file1 file2 file3[root@servera opt]# cp file1 /tmp/[root@servera opt]# ls /tmp/file1/tmp/file1[root@servera opt]# cp file1 /tmp/file10[root@servera opt]# ls /tmp/file10/tmp/file10[root@servera opt]# cp /etc/man_db.conf .[root@servera opt]# lsdir1 dir2 file1 file2 file3 man_db.conf 2.6.6 mv命令(移动文件/目录)123456789101112131415[root@servera opt]# mv file1 /[root@servera opt]# lsdir1 dir2 file2 file3 man_db.conf[root@servera opt]# mv file2 /file20[root@servera opt]# lsdir1 dir2 file3 man_db.conf[root@servera opt]# mv file3 file30[root@servera opt]# lsdir1 dir2 file30 man_db.conf[root@servera opt]# mv dir1 /[root@servera opt]# lsdir2 file30 man_db.conf[root@servera opt]# mv dir2 dir20[root@servera opt]# lsdir20 file30 man_db.conf 2.7 通配符规则 通配符 规则 * 匹配0个或多个任意字符 ？ 匹配1个任意字符 [ ] 匹配中括号内一个字符 [ - ] 匹配中括号内连续范围的一个字符 [ ^ ] 取反，匹配非中括号内的字符，表示一定有一个字符，但不是中括号内出现的。【^ab】 {a,b}或{a..c} 匹配括号中的字符或连续的字符 3 在线获取帮助3.1 MAN手册说明 命令 说明 man 1 用户命令 man 2 系统调用 man 3 库调用 man 4 特殊文件 man 5 配置文件 man 6 游戏 man 7 杂项 man 8 系统命令 3.2 获取帮助的方法12345678910111213141516171819# man命令mandbman passwdman -k passwd man 5 passwdman setfacl | grep -B 1 lisa# --helpsetfacl --help | grep \\\\-asetfacl --help | grep -w \\\\-a # pinfopinfo 回车 upinfo ls# rpm包中提供帮助rpm -qa | grep httpdrpm -ql 软件包名称rpm -qc 4 创建、查看、编辑文本4.1 VIM的模式分类 模式 功能 命令模式 光标移动、复制、删除 输入模式 输入文本内容 末行模式 保存退出、设置环境 4.2 VIM的模式说明4.2.1 命令模式 命令 解释 h j k l 左下上右 方向键 上下左右 1G、nG n代表一个数字，去第1行或n行 gg 将光标定位到文章的顶端 G 将光标定位到文章的底端（\\$定位光标到行尾，0和^定位光标到行首） x，X 向后删除一个字符、向前删除一个字符 dd，ndd 删除1行，n是一个数字，n行 例如：dgg、dG、d\\$、d0 D yy，nyy 复制1行，复制n行 p，P 粘贴到下一行，粘贴到上一行 u 撤销 ZZ 保存退出 4.2.2 插入模式12345678a 字符后进入插入模式i 当前字符位置进入插入模式o 在下一行新创建一行进入插入模式A 在行尾进入插入模式I 在行首进入插入模式O 在上一行新创建一行进入插入模式s 删除光标位置字符并进入插入模式S 删除光标所在行并进入插入模式 4.2.3 末行模式 命令 说明 w 保存 q 退出 wq 退出并保存 q! 强制退出 x 保存退出 set nu 设置行号 set nonu 取消行号 ：w /newfile 另存为其他文件 例子：”:w /man.txt” ：r /newfile 读取/newfile到本文件中 例子： “:r /etc/passwd” ：！ command vim编辑过程中，查询linux “:! ls /“ : e！ 重新读取文件 4.2.4 其他模式12345678v、V或Ctrl+V # 可视模式R # 替换模式/word，？word # /向下查找，？向上查找n，N # 定位到下一个匹配字符，定位到上一个匹配字符# 视图模式# 视图模式修改方法：ctrl+v ， jjj，I， 写入#号，esc 4.3 VIM的缩进与保存1234567891011：set all # 查看末行模式的帮助：set autoindent # 保存上下缩进：set tabstop=2 # 调整tab键缩进：set nu # 设置行号[root@foundation0 ~]# vim ~/.vimrc 仅对当前用户生效set nuset tabstop=2[root@foundation0 ~]# vim /etc/vimrc 全局设置，每个用户使用vim工具都有行号set nu 4.4 VIM的替换12345678910:s/// @@@ AAA ; ;;:s/old/new/:s/old/new/g:#,#s/old/new/g #井号代表一个数字比如：1,5s/old/new/g:%s/old/new/g:#,$s/old/new/g #井号代表数字，比如1，$s ，$代表末行，该命令为1行至末行# 修改某一段ip地址: %s/192.168.1/172.25.250/g# 取消文本中某个字段：:%s/10.10.10.10//g 4.5 关于重定向1234567891=stand，2=error，&amp;=1+2echo $SHELLecho 123456 &gt; file1grep root /etc/passwd &gt; /opt/a.txtgrep apache /etc/passwd &gt; /opt/a.txtgrep -n ^$ /etc/resolv.confgrep na /etc/resolv.conf &gt; /root/lines.txttc/resolv.conf &gt; /root/lines.txt 5 管理本地用户和组5.1 USER-用户&emsp;&emsp;基本概念：用户用于访问计算机资源 12340 超级用户 1000以下 系统用户1000以上 普通用户 组与用户ID对应(自然创建) 5.1.1 useradd-添加用户1234567891011121314语法:useradd 选项 选项参数 用户名option：-u：指定用户uid-g：指定主要群组-G：指定附加群组-s：指定shell环境 /bin/bash /sbin/nolgoin /bin/false-c：指定描述-d：指定用户家目录(通常不更改，如果设置，需要是未存在的目录)例：useradd user1 创建user1passwd user1 为user1设置密码id user1 查询用户信息 5.1.2 用户配置文件123456789101112131415161718192021222324252627282930313233# 用户配置文件路径:/etc/passwd[kiosk@foundation0 ~]$ vim /etc/passwdroot:x:0:0:root:/root:/bin/bash#用户名：密码占位符：UID：GID：描述：家目录：shell环境# 练习1： [root@servera /]# useradd -u 2000 user1[root@servera /]# id user1[root@servera /]# groupadd group1[root@servera /]# tail -1 /etc/group[root@servera /]# useradd -g group1 user2[root@servera /]# tail -1 /etc/passwd[root@servera /]# useradd -G wheel user3[root@servera /]# useradd -c student -d /user4dir -s /sbin/nologin user4[root@servera /]# tail -1 /etc/passwd[root@servera /]# su - user1[user1@servera /]# ctrl+d[root@servera /]# su - user1[root@servera /]# su - user2 需要密码[root@servera /]# ctrl+d 退出用户[root@servera /]# passwd user2 超级用户设置密码123456123456[root@servera /]# su - user1[root@servera /]# su - user2 输入密码[root@servera /]# useradd -G root,tom user5 #将user5同时加入到组root和tom组中# 练习2：tom10，uid 3000 ，gid devops，shell环境为/bin/false,描述 student，家目录/tom10dir,附加组 root。useradd -u 3000 -g devops -s /bin/false -c student -d /tom10dir -G root tom10 5.1.3 usermod-修改用户1234567891011121314151617181920212223语法：usermod 选项 选项参数 用户名option：-u：指定用户uid-g：指定主要群组-G：指定附加群组-s：指定shell环境 /bin/bash /sbin/nolgoin /bin/false-c：指定描述-d：指定用户家目录（通常不更改，且如设置需要是未存在的目录）-a：额外指定附加组# 练习1:[root@servera /]# usermod -u 3000 user1[root@servera /]# usermod -g group1 user1[root@servera /]# usermod -G root user1[root@servera /]# usermod -s /bin/false user1 #shell环境为/bin/false的用户和系统无任何交互[root@servera /]# su - user1[root@servera /]# usermod -c heihei user1# 练习2：tom11的附加组，root。想额外添加一个附加组为devops[root@servera opt]# usermod -a -G root tom11[root@servera opt]# usermod -G root,devops tom11 5.1.4 userdel-删除用户12345语法：userdel 选项 选项参数 用户名option：-r：删除用户同时删除邮箱和家目录[root@servera /]# userdel -r user5 5.2 PASSWORD-密码5.2.1 设置密码1234567语法:passwd 用户名# 方法1：[root@foundation0 /]# echo 123456 | passwd --stdin zhangsan 非交互式Changing password for user zhangsan.passwd: all authentication tokens updated successfully.# 方法2：[root@foundation0 /]# passwd zhangsan 交互式 5.2.2 密码配置文件12345678910111213路径:/etc/shadowuser2:$6$9R47OYVVaxga34EJ$Y3pGf5EnHpn6vfiBk5ZU1U89d7UiySOsnAs/fkFMuPRyhCZAvv0a6UXRVLGXqRUKwP34Sg0W/CYb1VQp7H08L0:20015:0:99999:7:::说明:第一列: 用户名第二列: 密码(有密码状态,无密码状态,!!帐号锁定,* 该帐号永久不能登陆系统)第三列: 密码的最后一次修改时间（从1970年1月1日至今的天数）18834=今天第四列: 密码的最小时间(和第三列比较，密码修改相隔时间，或理解为密码自最后一次修改后多少天内不能再重复修改)第五列: 密码的最大时间(密码有效期) 99999表示永久不过期(和第3列比，相当于自最后一次修改多久后必须变更密码，否则过期)第六列: 密码过期前警告时间（和第5列比，在过n天你的密码就过期了，需要重设密码。）第七列: 密码过期后帐号（宽限时间，第五列密码的最大时间到期后，还可以使用系统的宽限时间，该期间中可以继续使用系统，但是再次登入系统时强制修改密码，否则无法进入）第八列: 帐号有效期（账号失效后，无论密码是否过期都不能使用。）第九列: 保留列 5.3 GROUP-用户组5.3.1 groupadd12345语法：groupadd 选项 选项参数 组名-g：指定组ID[root@servera /]# groupadd group10[root@servera /]# groupadd -g 3000 group10 5.3.2 groupmod1234语法：groupmod 选项 选项参数 组名-n：更改组名 groupmod -n 新组名 旧组名[root@servera /]# groupmod -n group100 group10 5.3.3 groupdel123# 删除组信息groupdel groupname[root@foundation0 ~]# groupdel haha1 5.3.4 gpasswd&emsp;&emsp;加入群组与清除群组成员 12345678910gpasswd-a：添加用户到群组-d：从组中清除用户[root@foundation0 ~]# useradd -G upup user5 添加用户时指定附加组（次要群组）[root@foundation0 ~]# usermod -G upup user1 修改用户时指定附加组（次要群组） [root@foundation0 ~]# gpasswd -a user2 rootAdding user user2 to group root[root@foundation0 ~]# gpasswd -d user2 rootRemoving user user2 from group root 5.3.5 用户组配置文件1234567路径:/etc/group[root@localhost ~]# vim /etc/groupupup:x:2006:第一段: 组名第二段: 组密码占位符号第三段: gid第四段: 用户列表 6 控制对文件的访问6.1 系统安全的技术点对比12345#Linux操作系统涉及的安全部分: 防火墙 semanage port ，selinux semanage ...软件app semanage boolean文件系统权限 特殊权限 facl 隐藏权限 semanage fcontext 6.2 文件权限说明 权限表示 权限解释 r read(读) w write(写) x execute(执行) 6.3 权限表示12345# ls -l test-rw-r--r--. 1 stu1 class1 35 May 21 14:09 testrw-r--r-- #中间9位是权限，逻辑分三组，所有者 所属组 其他人权限stu1 所有者class1 所属组 6.4 系统权限的作用 权限 对文件的影响 对目录的影响 r cat ls w vim touch，rm，mkdir x ./script cd 6.5 符号修改文件权限 对象 设置方式 权限 u(user) \\+ (添加) r g(group) \\-(减去) w o(other) = (设置） x a(all) s(SUID、SGID)、t(Sbit) 6.6 数字方式修改文件权限 rwx 8进制表示 数字表示 r– 100 4 -w- 010 2 –x 001 1 6.7 文件权限设置-chmod12345678#语法：chmod 权限 文件名u g o a + - = r w x s t[root@node1 opt]# ll test -rw-r--r--. 1 root root 0 Nov 24 04:55 test[root@node1 opt]# chmod u+x test [root@node1 opt]# ll test -rwxr--r--. 1 root root 0 Nov 24 04:55 test 6.8 文件属主和属组-chown123456789101112131415161718192021222324252627282930#语法：chown 所有者:所属组 文件名 chown 该命令可以作用于文件、目录，修改时保证所有者的用户及组都是存在的。例:chown user2:user2 newfile# 练习:[root@node1 opt]# ll test-rwxr--r--. 1 root root 0 Nov 24 04:55 test[root@node1 opt]# id studentuid=1000(student) gid=1000(student) groups=1000(student),10(wheel)[root@node1 opt]# useradd harry[root@node1 opt]# chown student test;ll test-rwxr--r--. 1 student root 0 Nov 24 04:55 test[root@node1 opt]# chown :harry test;ll test-rwxr--r--. 1 student harry 0 Nov 24 04:55 test[root@node1 opt]# useradd sally[root@node1 opt]# chown sally:sally test;ll test-rwxr--r--. 1 sally sally 0 Nov 24 04:55 test# -R参数[root@node1 opt]# ll -d dir1drwxrwxr-x. 2 root root 19 Nov 24 05:15 dir1[root@node1 opt]# chown -R sally:sally dir1[root@node1 opt]# ll -d dir1drwxrwxr-x. 2 sally sally 19 Nov 24 05:15 dir1 6.9 文件默认权限-umask123456789101112131415#系统默认定义权限对于文件是666、对于目录是777#查看umask值[root@servera /]# umask0022#修改方法umask[root@servera /]# umask 0002修改完后，可以去文件和目录查看权限，看是否和之前不一样，看完改回来#永久生效[root@servera /]# echo 'umask 0002' &gt;&gt; ~/.bash_profile#将后面文件中的值加载到当前shell中。系统登录会读取~/.bash_profile文件自动加载[root@servera /]# source ~/.bash_profile #source 6.10 umask的计算12345678910111213141516171819202122232425262728293031323334#示例1：文件默认权限666umask后三位022快捷方法：变成权限后相减rw-rw-rw- = 666 文件系统默认权限----w--w- = 022 umask值------------------------------rw-r--r-- = 644 创建文件时的默认权限目录默认权限777rwxrwxrwx = 777 目录系统默认权限----w--w- = 022 umask值------------------------------rwxr-xr-x = 755 创建目录时的默认权限故系统中应设置为:0022#示例2:文件权限是r-- --- ---， 400 文件夹是dr-x --- --- 500umask?目录rwxrwxrwx = 777r-x------ = 500--------------------w-rwxrwx = 277文件rw-rw-rw- = 666-w-rwxrwx = umask ------------------r------- = 400参考umask计算方法：https://www.cnblogs.com/wyllearning/p/16482006.html如果减法时目录和文件权限不一致时，以目录的为准计算umask值 6.10 特殊权限&emsp;&emsp;文件系统权限可以完成一些基本权限功能设置，但有些特殊要求是达不到的，可能需要特殊权限来完成 &emsp;&emsp;Linux系统中特殊权限有三个：SUID 4 、SGID 2 、SBIT 1 6.10.1 SUID 41234567891011121314151617# 通常设置在二进制可执行文件（命令）上，并具有执行权限的情况下# 作用：设置了该权限的命令，被其他用户执行时，会临时获取文件所有者权限[student@clear ~]$ cat /etc/shadow #普通用户无法查看/etc/shadow[student@clear ~]$ su - root #切换root身份[root@clear ~]# chmod u+s /usr/bin/cat #数字修改方式：chmod 4755 /usr/bin/cart[root@clear ~]# ll /usr/bin/cat-rwsr-xr-x. 1 root root 34512 Aug 13 2018 /usr/bin/cat[student@clear ~]$ su - studnet[student@clear ~]$ cat /etc/shadow #能够看见内容，临时获取拥有者权限#数字修改法：chmod 4755 /usr/bin/cat# check：创建两个不同用户登录操作系统，进入dirt目录分别创建文件，尝试互相删除对方文件，结果应不能互相删除文件# 大S和小s区别， 1.执行权限位大S是，没有x 2.执行权限位小s是该位，有x 6.10.2 SGID 2123456789#该权限通常设置在`目录`上，设置了该权限的目录，在该目录中创建`子文件及目录`时会`继承`父目录所属组[root@clear opt]# chmod g+s dir1[root@clear opt]# ll -d dir1drwxr-sr-x. 2 student student 22 Nov 19 04:51 dir1[root@clear opt]# touch dir1/root1.txt[root@clear opt]# ll dir1/root1.txt-rw-r--r--. 1 root student 0 Nov 19 04:52 dir1/root1.txt#数字修改法： chmod 2755 dir1 6.10.3 SBIT 1123456789101112#该权限通常设置在目录上，设置了该权限的目录，其他用户在该目录中只能删除所有者是自己的文件[root@clear opt]# chmod 1777 /opt/share/[root@clear opt]# ll -d /opt/share/drwxrwxrwt. 2 root root 6 Nov 19 04:56 /opt/share/[root@clear opt]# su - studentLast login: Sat Nov 19 04:55:24 EST 2022 on pts/0[student@clear ~]$ touch /opt/share/student.txt.haha[student@clear ~]$ logout[root@clear opt]# su - tomLast login: Sat Nov 19 04:55:38 EST 2022 on pts/0[tom@clear ~]$ rm -f /opt/share/student.txt.haharm: cannot remove '/opt/share/student.txt.haha': Operation not permitted 7 进程监控及管理1234yum install -y psmiscpstree -p# 程序被开启会产生一个或多个进程，他们都有对应父进程与子进程，每个进程都有进程号PID# systemd 1 不能被杀死，除非重启，关机 7.1 查看进程12345678910111213# ps以静态的方式查看系统进程ps -lps aux ps aux | grep http[root@servera ~]# ps -l# 查看httpd进程ps aux |grep httpd# top以同态的形式查看进程top 7.2 终止进程7.2.1 kill命令12345678910111213141516171819202122#语法 kill -s 信号名称 或-n 信号编号#Options:-s sig SIG is a signal name-n sig SIG is a signal numberkill -s SIGKILL httpdkill -n 9 httpd # 或 kill -9 httpd[root@node1 /]# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1[root@node1 /]# vim 1 &amp;[1] 1372[root@node1 /]# ps -l[root@node1 /]# kill -n 9 1372[root@node1 /]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD0 S 0 1313 1312 0 80 0 - 59084 - pts/1 00:00:00 bash0 R 0 1374 1313 0 80 0 - 63799 - pts/1 00:00:00 ps[1]+ Killed vim 1 7.2.2 killall命令12345678910#语法：killall 守护进程名称#yum install -y httpdsystemctl start httpdps aux | grep httpdkillall httpdyum provides killallyum install -y psmisc-23.1-3.el8.x86_64killall httpdps aux | grep httpd 7.3 作业控制jobs123456789101112131415[root@servera ~]# dd if=/dev/zero of=./bigfile bs=1M count=1000ctrl + z [root@servera ~]# jobs[1]+ Stopped dd if=/dev/zero of=./bigfile bs=1M count=1000[root@servera ~]# bg ％1[1]+ dd if=/dev/zero of=./bigfile bs=1M count=1000 &amp;[root@servera ~]# jobs[1]+ Running dd if=/dev/zero of=./bigfile bs=1M count=1000 &amp;[root@servera ~]# fg %1[root@servera ~]# kill -9 %2[2]- Stopped vim file2[root@servera ~]# jobs[2]- Killed vim file2[3]+ Stopped nice -n -10 vim file4 7.4 进程优先级调整12345678910111213141516#nice值#超级用户root 可以修改nice值范围 -20~19#普通用户user 可以修改nice值范围 0-191.进程优先级数字越小，优先级越高2.优先级不能直接改，可以通过nice值来影响优先级3.旧优先级 + nice值 = 新优先级 80 -10 = 70#两种方法：1.产生新进程时，设置nice值 nice -n -5 vim file2 &amp;2.修改现有进程nice值 renice -n 10 PID# ps -l 查看需要更改的进程号 renice -n 10 28183 8 控制服务与守护进程8.1 服务状态关键字段 字段 描述 Loaded 服务单元是否加载到内存 Active 服务单元是否在运行，运行了多久 Main PID 服务的主进程ID，包括命令名称 Status 有关该服务的其他信息 8.2 systemctl管理服务12345678910111213141516171819202122232425262728systemctl -t help# 列入.service扩展名，代表服务，如web服务systemctl list-units --type service 列出当前服务器加载的服务单元systemctl status httpd.service 查看某个服务 [root@servera system]# systemctl status httpd# 查看服务是否启动[root@servera system]# systemctl is-active httpdactive# 查看服务是否开机启动[root@servera system]# systemctl enable httpd[root@servera system]# systemctl is-enabled httpdenabled[root@servera system]# systemctl disable httpdRemoved /etc/systemd/system/multi-user.target.wants/httpd.service.[root@servera system]# systemctl is-enabled httpddisabled# 常见特征：1、安装 yum install -y httpd2、启动 systemctl start httpd.service （单元文件）/usr/lib/systemd/system/3、查进程 ps aux | grep httpd , 每个服务有自己的守护进程/usr/sbin/httpd4、查端口 netstat -ntlp ，找到80端口，对应Listen监听状态 对应httpd服务# vim /etc/service 该文件记录了系统服务的端口和协议的对应关系 8.3 服务状态分类 关键字 描述 loaded 单元配置文件已处理 active（running） 正在通过一个或多个持续进程与运行 active（exited） 已成功完成一次性配置 active（waiting） 运行中，但正在等待事件 inactive 不在运行 enabled 在系统引导时启动 disabled 未设为在系统引导时启动 static 无法启动，但可以由某一启动的单元自动启动 8.4 管理系统服务&emsp;&emsp;语法：systemctl 管理命令 unitname 管理命令 描述 status 查看状态 start 开启 stop 关闭 restart 重启 reload 加载配置文件 enable 开机启动 disable 关闭开机启动 is-active 查看服务状态是否启动 is-enabled 查看服务是否开机自启动 list-dependencies 【unitname】 查看单元依赖 mask 禁止服务，无法启动或开机启动 unmask 解除码 9 OPENSSH服务9.1 ssh常用功能123456789[root@servera ~]# vim /etc/hosts 系统是否做了dns,ip和域名及主机名的映射[root@servera ~]# ssh root@172.25.250.11[root@serverb /]# scp root@172.25.250.10:/opt/newfile .root@172.25.250.10's password: newfile 100% 0 0.0KB/s 00:00 [root@servera opt]# ssh root@172.25.250.11 'yum install -y httpd'ssh root@172.25.250.11 'yum install -y httpd' 9.2 ssh免密登录12345678[root@servera ssh]# ssh-keygen 后面三个回车[root@servera ssh]# ssh-copy-id root@serverb[root@serverb /]# cd /root/.ssh/[root@serverb .ssh]# lsauthorized_keys known_hosts[root@servera ssh]# ssh root@serverb# a免密远程b，如果想b远程a免密，需要相同的配置 9.3 ssh服务控制1234# 拒绝root登录[root@serverb ~]# vim /etc/ssh/sshd_configPermitRootLogin no[root@serverb ~]# systemctl reload sshd（或restart） 9.4 sudo命令12345678910111213141516171819202122232425# 将用户设置为特权用户[student@servera ~]$ yum remove -y httpdError: This command has to be run under the root user.[root@servera /]# vim /etc/sudoers 或者 visudo## Allow root to run any commands anywhere root ALL=(ALL) ALLstudent ALL=(ALL) ALL[student@servera ~]$ sudo yum remove -y httpd[sudo] password for student: student# 将账号添加到特权用户组中，培训环境默认特权用户组是wheel组，在/etc/sudoers文件中用%wheel来表示usermod -G wheel tom# 设置特权组中用户切换时不需要密码%admin ALL=(ALL) NOPASSWD: ALL# 添加一个特权组admin，而且组内有一个成员是harry。最终harry账号应当为特权账号[root@serverb ~]# groupadd admin[root@serverb ~]# visudo[root@serverb ~]# useradd -G admin harry[root@serverb ~]# su - harry[harry@serverb ~]$ sudo -i[sudo] password for harry: 10 日志分析与存储10.1 系统中的日志文件 日志文件 存储的消息类型 /var/log/messages 大多数系统日志消息处存放处 /var/log/secure 与安全性和身份验证时间相关的syslog消息 /var/log/maillog 与邮件服务器相关的syslog消息 /var/log/cron 与计划任务执行相关的syslog消息 /var/log/boot.log 与系统启动相关的消息 10.1.1 rsyslog服务管理日志123456[root@haha log]# yum provides /etc/rsyslog.conf #查看文件是哪个软件包提供的[root@clear log]# rpm -qc rsyslog-8.1911.0-3.el8.x86_64 /etc/logrotate.d/syslog/etc/rsyslog.conf #一般服务文件以.conf结尾，改文件是日志服务的配置文件/etc/sysconfig/rsyslog[root@clear log]# vim /etc/rsyslog.conf 10.1.2 记录日志的规则12345678#日志文件配置格式: mail.info /var/log/vsftpd.log #.点代表包含后面级别及以上级别AAAA.BBBB CCCCAAAA 产生日志的设备（类别） #如何产生的日志BBBB 日志的级别 #日志有不同安全级别，类似轻重缓急的严重程度，发出警告CCCC 保存日志的位置 #在系统中保存日志文件的路径 10.1.3 rsyslog配置文件类别 类别 说明 Kern 内核 authpriv 授权和安全 cron 计划任务 mail 邮件 daemon 系统守护进程 user 普通用户级别的 syslog 由rsyslog生成的信息 loca10\\~loca17 自定义本地策略 \\* 所有类别 10.1.4 日志级别 等级 解释 EMERG（紧急） 会导致主机系统不可用的情况 ALERT（警告） 必须马上采取措施解决的问题 CRIT（严重） 比较严重的情况 ERR（错误） 运行出现错误 WARNING（提醒） 可能会影响系统功能的事件 NOTICE（注意） 不会影响系统但值得注意 INFO（信息） 一般信息 DEBUG（调试） 程序或系统调试信息等 \\* 所有等级 none 不记录日志 1234*.info;mail.none;authpriv.none;cron.none /var/log/messages*.info # *代表所有级别 .点代表后面的等级及以上等级，也就是info以上的等级全记录；分号是不同设备等级的分隔符号-/var/log/maillog # - 代表先记录缓存，再记录硬盘，减轻硬盘i/o读写压力 10.1.5 logger发送测试日志123456789101112131415161718192021221 查看rsyslog服务是否开启 (默认系统已开启)[root@servera ~]# systemctl status rsyslog2 编辑rsyslog配置文件[root@servera ~]# vim /etc/rsyslog.conf# Save boot messages also to boot.loglocal7.* /var/log/boot.log..*.debug /var/log/messages.debug3 重启rsyslog日志服务让配置生效[root@servera ~]# systemctl restart rsyslog4 开另一个窗口 ctrl+shift+t[root@servera ~]# tail -n 0 -f /var/log/messages.debug 5 使用logger命令生成一个user类别，debug级别的日志内容为“Debug test messages” #（考点）[root@servera ~]# logger -p user.debug &quot;Debug test messages&quot; 6 在第4步的窗口中查看新生成日志信息[root@servera ~]# tail -n 0 -f /var/log/messages.debugJun 18 14:45:31 servera root[29174]: messages haha 10.2 journalctl123456789101112131415# 传统的日志服务是rsyslog# 新的日志服务是systemd-journal，它也是一个日志管理服务，可以收集来自内核、系统早期启动阶段的日志，以及系统进程在启动和运行中的一些标准输出与错误输出# systemd-journal一旦重启既消失，因为保存在了/run/log/journal/*/*.journal结尾，该文件是一个二进制日志文件，需要用journalctl命令查看journalctl #查看系统日志journalctl -n #通过q或ctrl接触观看 ，此命令显示方式类似与tail -njournalctl -n 5 journalctl -p err 日志等级journalctl -f journalctl -p err journalctl -p info （deubg、info、notice、warning、err、crit、alert、emerg）journalctl --since &quot;2020-02-28 22:53:35&quot; --until &quot;2020-02-28 22:53:40&quot; 10.2.1 journalctl常用字段 常用字段 含义 \\_COMM 命令名称 \\_EXE 进程的可执行文件的路径 \\_PID 进程的PID \\_UID UID \\_SYSTEM_UNIT 启动该进程的systemd单元 123journalctl -o verbose journalctl _HOSTNAME=localhostjournalctl _HOSTNAME=localhost _PID=1 102.2 保存journal服务文件12345678910111213mandbman -k journalman 5 journald.conf ， Storage=[root@clear journal]# ll -d /run/log/journal[root@clear journal]# cp -a /run/log/journal/ /var/log/journal[root@clear journal]# ll -d /var/log/journaldrwxr-sr-x. 4 root systemd-journal 86 Nov 19 04:30 /var/log/journal[root@clear journal]# systemctl restart systemd-journald[root@clear journal]# ll /var/log/journal/3a2b4da8dabb4729935c193e58ad052d/ #字符串目录名字每个人的可能不一样。不要复制我的笔记。total 8192-rw-r-----. 1 root root 8388608 Nov 19 04:30 system.journal[root@clear journal]# journalctl 10.3 准确的系统时间1234567891011121314# RHEL6 ntp服务# RHEL8 chrony服务# 还是使用同样的协议标准ntp（network time protocol）UTC：通用协调时 （UTC时间0点是北京时间8点,因为中国、新加坡、马来西亚、菲律宾等国的时间与UTC的时差均为+8,也就是UTC+8,所以当UTC时间0点,北京时间即为0+8=8点）GMT：格林威治标准时间CST：中国标准时间 (China Standard Time） （中国大陆、中国香港、中国澳门、中国台湾、蒙古国、新加坡、马来西亚、菲律宾、西澳大利亚州的时间与UTC的时差均为＋8，也就是UTC+8）RTC：(Real-Time Clock)也称为硬件时间：RTC是芯片内置的硬件时钟，只要芯片不断电，即使操作系统关机的时候，RTC时钟也是正常在 走的，所以当操作系统关机重启后，可通过读取RTC时间来更新系统时间 （可通过hwclock命令来获取具体的时间 -r 查看硬件时间 -s 硬件时间设置到系统 -w 系统设置到硬件） 10.3.1 timedatectl命令1234567891011121314151617181920212223242526[root@servera log]# timedatectl Local time: Sat 2020-02-29 08:51:49 CST Universal time: Sat 2020-02-29 00:51:49 UTC RTC time: Sat 2020-02-29 08:11:07 Time zone: Asia/Shanghai (CST, +0800)System clock synchronized: yes NTP service: active RTC in local TZ: no [root@servera log]# timedatectl list-timezones [root@servera log]# timedatectl set-timezone Asia/Hong_Kong [root@servera log]# timedatectl Local time: Sat 2020-02-29 08:55:48 HKT Universal time: Sat 2020-02-29 00:55:48 UTC RTC time: Sat 2020-02-29 08:15:06 Time zone: Asia/Hong_Kong (HKT, +0800)System clock synchronized: yes NTP service: active RTC in local TZ: no# 修改时间方法timedatectl set-time &quot;2020-02-30 10:00:00&quot;Failed to set time: NTP unit is activetimedatectl set-ntp false timedatectl set-time &quot;2020-02-30 10:00:00&quot;timedatectl set-ntp true 10.3.2 chrony命令1234567891011server===server选项格式===server host [ key n ] [ version n ] [ prefer ] [ mode n ] [ minpoll n ] [ maxpoll n ] [ iburst ]其中host是上层NTP服务器的IP地址或域名，随后所跟的参数解释如下所示：◆ key： 表示所有发往服务器的报文包含有秘钥加密的认证信息，n是32位的整数，表示秘钥号。◆ version： 表示发往上层服务器的报文使用的版本号，n默认是3，可以是1或者2。◆ prefer： 如果有多个server选项，具有该参数的服务器有限使用。◆ mode： 指定数据报文mode字段的值。◆ minpoll： 指定与查询该服务器的最小时间间隔为2的n次方秒，n默认为6，范围为4-14。◆ maxpoll： 指定与查询该服务器的最大时间间隔为2的n次方秒，n默认为10，范围为4-14。◆ iburst： 当初始同步请求时，采用突发方式接连发送8个报文，时间间隔为2秒。 1234567[root@servera ~]# systemctl enable --now chronyd #--now启动服务 enable 开机自动[root@servera ~]# systemctl status chronyd[root@servera ~]# vim /etc/chrony.conf #大概第7行 server 后面添加服务器'地址'或'域名'。server classroom.exmaple.com iburst[root@servera ~]# systemctl restart chronyd.service[root@servera ~]# chronyc sources -v 11 RHEL网络管理11.1 IPv4地址1234567891011121314IP/(NETMASK\\|PREFIX) 172.25.0.9/255.255.0.0 \\| 172.25.0.9/16--- ---------------------- --------------------------------------------GATEWAY 172.25.x.xDNS 正向解析 \\# host servera， 反向解析 \\# host私有地址：A １－１２７B １２８－１９１C １９２－２２３IP地址分类默认对应的子网掩码掩码：Ａ：２５５．０．０．０ 11111111.00000000.00000000.00000000 /8Ｂ：２５５．２５５．０．０ 11111111.11111111.00000000.00000000 /16Ｃ：２５５．２５５．２５５．０ 11111111.11111111.11111111.00000000 /24 11.1.1 IP与掩码二进制与运算123网络地址 172.25.0.0 主机位全0---------- ---------------- -----------广播地址 172.25.255.255 主机位全1 11.1.2 查看ipv4与ipv612345678# 查看ip地址方法1：[root@servera ~]#ifconfig [root@servera ~]#ifconfig eth0# 查看ip地址方法2：[root@servera ~]# ip addr show eth0[root@servera ~]# ip a s eth0[root@servera ~]# ip -s link show enp1s0 11.1.3 端口与服务1234567891011121314# 查看服务端口是否被占用lsof -i:80 或 netstat-n：显示接口和端口编号-t：tcp信息-u：udp信息-l：监听状态信息-a：显示所有信息-p：显示协议名称而不是端口netstat -ntlp | grep 80# 参考ss和netstat区别：https://blog.csdn.net/qq_37863891/article/details/107283415# 标准服务端口 /etc/services 11.2 网络管理工具11.2.1 nmcli的概念1234567891011121.使用nmcli管理网络服务NetworkManager2.nmcli工具的功能：查看网络设备、创建网络连接、修改网络配置3.nmcli的特点及概念： nmcli工具可以对网卡或网卡配置文件操作 device ---- 网卡设备 connection --- 连接 指的就是网卡配置文件 一个device可以拥有多个connection，同一时间只能启用一个connection，且一个connection只能属于一个device4.举例： device-----eth0 connection1 ---- dhcp 自动获取IP connection2 ---- static 静态IP 11.2.2 nmcli管理网络1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253mandbman -k nmclinmcli (1) nmcli-examples (7)man nmcli | grep -A 2 'nmcli connection add'-basic[root@servera ~]# nmcli connection show [root@servera ~]# nmcli connection show --active [root@servera ~]# nmcli device status -add---添加 dhcp方式: #创建一个名为default的手动链接，绑定至eth0网卡[root@servera ~]# nmcli connection add con-name 'default' type ethernet ifname eth0 autoconnect yes [root@servera ~]# nmcli con show static方式：#创建一个名为static的静态链接，绑定至eth1网卡[root@servera ~]# nmcli connection add con-name static type ethernet ifname eth0 autoconnect yes ipv4.addresses 192.168.0.1/24 ipv4.gateway 192.168.0.254 ipv4.dns 8.8.8.8 ipv4.method manual [root@servera ~]# nmcli connection showNAME UUID TYPE DEVICE Wired connection 1 1f5ad5ae-e926-3f54-9805-33174e63af47 ethernet eth0 static 980f6712-86b7-4d92-bc84-62e677ccabfc ethernet eth1 #此处dhcp 82c4a93f-1ca2-432b-94da-59a6c4f5aaca ethernet -- Wired connection 2 e801f880-78a6-3344-857f-588f7495bb26 ethernet -- [root@servera /]# nmcli connection up static 启动static网卡[root@servera /]# ip a s eth1 | grep -w inet-modify---修改#将链接static网络信息更改： IP：192.168.0.2 mask：/24 gw：192.168.0.200 dns：114.114.114.114[root@servera ~]# nmcli connection modify static ipv4.addresses 192.168.0.2/24 ipv4.gateway 192.168.0.200 ipv4.dns 114.114.114.114 autoconnect yes ipv4.method manual[root@servera ~]# nmcli connection up static[root@servera ~]# ip a s eth0 inet 192.168.0.2/24 brd 192.168.0.255 scope global noprefixroute eth1 [root@servera ~]# route -nDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.25.250.254 0.0.0.0 UG 100 0 0 eth00.0.0.0 192.168.0.200 0.0.0.0 UG 101 0 0 eth1[root@servera ~]# cat /etc/resolv.conf # Generated by NetworkManagersearch lab.example.com example.comnameserver 172.25.250.254nameserver 114.114.114.114 [root@serverb ~]# nmcli connection delete static #删除一个链接Connection 'static' (b85e6a57-b8f7-421f-8d15-9ff5e27cbb85) successfully deleted.-up_down---启动与关闭网卡[root@servera /]# nmcli connection down static 关闭static网卡-off---关闭网络服务[root@servera /]# nmcli networking off 关闭网络服务，慎重使 11.2.3 图形化管理工具nmtui12345nmtui-edit图形化管理配置通过点击设置--network--网卡设置，ipv4address netmask dns gatewaynmtui-edit[root@servera /]# nmcli connection up static 11.2.4 网卡配置文件12345678910111213141516171819202122-RHEL8 版本# grep -r IPADDR /usr/share/ #找到下面手册的指令vim /usr/share/doc/initscripts/sysconfig.txt 帮助手册修改配置文件方式修改IP[root@servera ~]# vim /etc/sysconfig/network-scripts/ifcfg-Wired_connection_1BOOTPROTO=none #获取IP的方式 static--静态 none--不设置 dhcp--自动获取 ，手动配IP选前两个中任意一个ONBOOT=yes #开机自动连接IPADDR=172.25.250.100 #ip地址PREFIX=24 #子网掩码 PREFIX=24(等效于255.255.255.0)，mask=255.255.255.0 netmask=255.255.255.0GATEWAY=172.25.250.254 #网关DNS1=xxxx #dns，dns可以有三个 DNS1= DNS2= DNS3=加载网卡配置文件方法一：[root@serverb network-scripts]# nmcli connection reload ifcfg-Wired_connection_1 或 nmcli connection reload[root@serverb network-scripts]# nmcli connection up staticsystemctl restart NetworkManager-RHEL9版本 网卡配置文件位置[root@node1 system-connections]# vim /etc/NetworkManager/system-connections/System\\ eth0.nmconnection 11.3 更改网络信息11.3.1 主机名12345[root@servera ~]# hostnameservera.lab.example.com[root@servera ~]# hostnmae www.example.com 临时[root@servera ~]# vim /etc/hostname 永久（重启系统:reboot、init 6)[root@servera ~]# hostnamectl set-hostname hostname 永久 11.3.2 网关(gateway)123456789# 使用nmclinmcli con add con-name xxx ipv4.gateway xxx.xxx.xxx.xxx 配置网关nmcli con mod xxx ipv4.gateway xxx.xxx.xxx.xxx 修改网关以上两种改完之后，需要nmcli con up xxx# 修改配置文件vim /etc/sysconfig/network-scripts/ifcfg-xxxxGATEWAY=xxx.xxx.xxx.xxx 修改完后要nmcli con reload ，再nmcli con up xxxx 11.3.3 查看路由及网关123456789101112131415161718[root@servera ~]# ip route default via 172.25.250.254 dev enp1s0 proto static metric 100 172.25.250.0/24 dev enp1s0 proto kernel scope link src 172.25.250.10 metric 100[root@servera ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.25.250.254 0.0.0.0 UG 100 0 0 enp1s0172.25.250.0 0.0.0.0 255.255.255.0 U 100 0 0 enp1s0[root@servera ~]# nmcli connection show Wired\\ connection\\ 1 | grep ipv4.gaipv4.gateway: 172.25.250.254[root@servera ~]# netstat -nrKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface0.0.0.0 172.25.250.254 0.0.0.0 UG 0 0 0 enp1s0172.25.250.0 0.0.0.0 255.255.255.0 U 0 0 0 enp1s0 11.3.4 指定DNS1234567891011121314vim /etc/resolv.conf #该文件指定dns和域名的/etc/sysconfig/network-scripts/ifcfg-xxx中的DNS字段会同步到/etc/resolv.conf，后者优先。nameserver 172.25.250.254# 测DNS域名解析是否正常[root@servera ~]# host classroom.example.com classroom.example.com has address 172.25.254.254[root@servera ~]# nslookup classroom.example.comServer: 172.25.250.254Address: 172.25.250.254#53Name: classroom.example.comAddress: 172.25.254.254[root@servera ~]# dig classroom.example.com 12 归档与系统间复制文件12.1 归档及压缩12345678910# 语法：tar 选项 归档文件名 源文件 源文件2 源文件N-c 创建-t 查看-f 指定文件名-v 显示详细信息-x 解包-C 指定解包路径man tartar -cvf /root/etc.tar /etc/ 12.1.1 文件及目录打包、解包123456789101112131415161718192021222324# 文件打包归档[root@servera opt]#etc.tar file1 file2 file3[root@servera opt]# tar -cvf file.tar file1 file2 file3[root@servera opt]# lsetc.tar file1 file2 file3 file.tar# 文件解包[root@servera opt]# tar -xvf file.tar -C /tmp/[root@servera opt]# ls /tmp/file1 rclocal.logfile2 rht-bastionfile3 rht-defaultNIC1 rht-vm-hostsNIC2 systemd-private-ef2feb022cd2465c9dd920878a1d962b-chronyd.service-kRKFp0# 目录打包归档[root@servera opt]# tar -cvf etc.tar /etc[root@servera opt]# lsetc.tar file1 file2 file3 file.tar# 目录解包[root@servera opt]# tar -xvf etc.tar 12.1.2 文件压缩12345678910# 只压缩文件[root@servera opt]# gzip file1[root@servera opt]# file file1.gz file1.gz: gzip compressed data, was &quot;file1&quot;, last modified: Sun Mar 1 05:54:06 2020, from Unix, original size 0[root@servera opt]# bzip2 file2[root@servera opt]# file file2.bz2 file2.bz2: bzip2 compressed data, block size = 900k[root@servera opt]# xz file.tar [root@servera opt]# lsetc.tar file1.gz file2.bz2 file3 file.tar.xz 12.1.3 tar打包并压缩123456789101112# tar的压缩选项 man tar | grep gzip-z gzip -j bzip2 -J xz# 打包并压缩 tar -zcvf /root/etc.tar.gz /etc/ file etc.tar.gz tar -jcvf /opt.tar.bz2 /opt/ tar -Jcvf /root/etc.tar.gz /etc/# 解压缩并指定路径tar -zxvf etc.tar.gz -C /opt/tar xf etc.tar.gz -C /opt/ 12.2 远程传输12.2.1 远程文件传输1234567891011121314151617181920# scp实现远程文件传输scp servra.txt root@bastion:/opt/scp root@bastion:/opt/bastion.txt .# sftp实现远程文件传输ID app roles---- ------- ----------------1 ftp client2 sftp ssh SubService3 vsftp service[root@servera opt]# touch put.txt[root@servera opt]# sftp instructor@classroom.example.cominstructor@classroom.example.com's password: Connected to instructor@classroom.example.com.sftp&gt; cd /tmp/sftp&gt; put /opt/put.txt Uploading /opt/put.txt to /tmp/put.txt/opt/put.txt 100% 0 0.0KB/s 00:00 sftp&gt; ls 12.2.2 同步文件内容12345678910# rsync -v 显示详细信息 -a 相当于存档模式 # 本地同步[root@servera tmp]# rsync -av /var/log/* /tmp# 远程同步[root@servera tmp]# rsync -av /var/log/* serverb:/tmp# 将serverb上的/var/log/同步到，servera当前目录下[root@servera tmp]# rsync -av serverb:/var/log/ . 13 安装和升级软件包13.1 RPM包管理13.1.1 rpm包语法1234567891011121314151617181920#软件的获取方式： 1、互联网(下载光盘镜像.iso)、直接使用网络yum源 2、光盘#rpm包语法:rpm 选项 包名-i 安装-v 显示过程-h 以易读方式显示进度条-e 卸载rpm -ivh xxx.rpm#示例:1. 在f0中进入软件包的存储位置[root@foundation0 /]# cd /content/rhel8.0/x86_64/dvd/AppStream/Packages/[root@foundation0 Packages]# pwd/content/rhel8.0/x86_64/dvd/AppStream/Packages2.安装软件[root@foundation0 Packages]# rpm -ivh lftp-4.8.4-1.el8.x86_64.rpm 13.1.2 rpm包查询命令123456789101112131415#语法:rpm -q 软件包名称-q: query 查询，和其他参数配合-l：list 列出软件包安装后给系统带来的所有文件-a：all 查看所有已安装的软件包-c: configure 查看软件包提供的配置文件rpm -qa | grep telnetrpm -qc openssh-server-8.0p1-4.el8_1.x86_64#卸载RPM包[root@node1 /]# rpm -q telnettelnet-0.17-73.el8.x86_64[root@node1 /]# rpm -e telnet-0.17-73.el8.x86_64[root@node1 /]# rpm -q telnetpackage telnet is not installed 13.2 YUM工具13.2.1 管理yum源文件12345678910111213141516171819202122# yum源软件配置方式:[root@servera /]# cd /etc/yum.repos.d/[root@servera yum.repos.d]# mkdir old[root@servera yum.repos.d]# mv * old #将系统默认的yum源文件移动到old中，可以在该文件中查看原来的yum源路径[root@servera yum.repos.d]# man 5 yum.conf[root@servera yum.repos.d]# vim rhel.repo[AppStream] #id名称自定义name=AppStream #描述自定义，和id不必一样baseurl=http://content.example.com/rhel8.4/x86_64/dvd/AppStream #file:///中://是url格式，第三个/是根目录gpgcheck=0 #gpgchek=1 要进行公钥验证，需要再添加选项gpgkey=http://content.example.com/rhel8.4/x86_64/dvd/RPM-GPG-KEY-redhat-releaseenabled=1[BaseOS]name=BaseOSbaseurl=http://content.example.com/rhel8.4/x86_64/dvd/BaseOSgpgcheck=0enabled=1[root@servera yum.repos.d]# yum clean all #清除缓存，避免沿用之前缓存的软件[root@servera yum.repos.d]# yum makecache #和当前yum源建立缓存关联[root@servera yum.repos.d]# yum repolist all #查看当前yum源状态[root@servera yum.repos.d]# yum install -y telnet #测试安装软件telnet[root@servera yum.repos.d]# rpm -q telnet #使用rpm方式查询测试 13.2.2 yum源命令配置12345678910111213141516171819202122231 找到提供yum-config-manager命令的软件包名称[kiosk@foundation0 ~]$ yum provides yum-config-manager2 安装yum-utils软件 #打开浏览器输入yum源仓库地址，找到yum-utils的软件包，并且通过rpm命令安装网络上的yum-utils软件包，来提通yum-config-manager命令[root@servera ]# rpm -ivh http://content.example.com/rhel8.4/x86_64/dvd/BaseOS/Packages/yum-utils-4.0.18-4.el8.noarch.rpm3 通过yum-config-manager命令部署yum源[root@servera ]# yum-config-manager --help[root@servera ]# yum-config-manager --add-repo=http://content.example.com/rhel8.4/x86_64/dvd/AppStream[root@servera ]# yum-config-manager --add-repo=http://content.example.com/rhel8.4/x86_64/dvd/BaseOS4 命令制作的yum源中没有gpgcheck选项，如何配置？可以通过以下方法： 1、此处可以在/etc/yum.repos.d/xx.repo文件里添加 gpgcheck=0 2、rpm --import ‘公钥地址’ 导入公钥` [root@servera yum.repos.d]# rpm --import http://content.example.com/rhel8.4/x86_64/dvd/RPM-GPG- KEY-redhat-release 5.开启或关闭[root@servera /]# yum-config-manager --disable rhel-8.0-for-x86_64-appstream-rpms（yum池ID）[root@servera /]# yum repolist all[root@servera /]# yum-config-manager --enable rhel-8.0-for-x86_64-appstream-rpms[root@servera /]# yum repolist all 13.2.3 YUM常见的命令123456789101112131415yum list http*yum search httpdyum info httpd-manual[root@servera /]# yum provides /var/www/html# yum update# yum install 包名 yum remove 包名[root@servera /]# yum install -y autofs[root@servera /]# yum remove -y autofs[root@servera /]# yum history# yum clean all yum repolist yum repolist all# yum group list yum groupinfo 'Server with GUI'# yum groupinstall -y 'Server with GUI'# startx 切图形 13.3 第三方YUM源123456789101.虚拟机联网 虚拟机设置里面NAT选择已连接 nmcli connection up ens192 2.百度搜索（阿里源、华为源...），将.repo文件下载到系统curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo3.验证cd /etc/yum.repos.d/;lsyum repolist allyum install -y vsftpd 14 访问Linux文件系统14.1 存储管理14.1.1 文件系统、存储和块设备 块设备 含义 /dev/sda、/dev/sdb STAT/SAS（新SCSI技术）/USB 附加存储 /dev/vda、/dev/vdb virtio-blk 超虚拟化存储（部分虚拟机） /dev/nvme0，/dev/nvme1 附加存储 （SSD） /dev/mmcblk0、/dev/mmcblk1 SD卡 14.1.2 磁盘分区123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# 分区--格式化--挂载--使用# mount命令挂载是临时的，意味着重启系统后将取消挂载。需要手动重新挂载# 永久挂载需要将挂载项记入/etc/fstab中1 分区，gpt方案 ，分2个区，每个1G[root@servera ~]# fdisk /dev/vdbCommand (m for help): m d delete a partition #删除分区 n add a new partition #创建分区 p print the partition table #打印分区表 w write table to disk and exit #保存并退出 Create a new label g create a new empty GPT partition table #指定分区方案gpt o create a new empty DOS partition table #指定分区方位mbrCommand (m for help): g #指定分区方案gptCreated a new GPT disklabel (GUID: D29B3E19-BA51-1042-BFE6-0FD975D1B7DB).Command (m for help): nPartition number (1-128, default 1): #回车First sector (2048-10485726, default 2048): #回车Last sector, +sectors or +size{K,M,G,T,P} (2048-10485726, default 10485726): +1G #指定分区大小1GCreated a new partition 1 of type 'Linux filesystem' and of size 1 GiB.Command (m for help): pDisklabel type: gpt #查看分区方案Device Start End Sectors Size Type #分区表/dev/vdb1 2048 2099199 2097152 1G Linux filesystem #/dev/vdb 分区为1GCommand (m for help): n #创建第二个分区Partition number (2-128, default 2): First sector (2099200-10485726, default 2099200): Last sector, +sectors or +size{K,M,G,T,P} (2099200-10485726, default 10485726): +1GCommand (m for help): pDevice Start End Sectors Size Type/dev/vdb1 2048 2099199 2097152 1G Linux filesystem/dev/vdb2 2099200 4196351 2097152 1G Linux filesystemCommand (m for help): w #保存退出[root@servera ~]# fdisk -l /dev/vdb #查看/dev/vdb分区表Device Start End Sectors Size Type/dev/vdb1 2048 2099199 2097152 1G Linux filesystem/dev/vdb2 2099200 4196351 2097152 1G Linux filesystem[root@servera ~]# lsblk /dev/vdb #lsblk查看块设备NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvdb 252:16 0 5G 0 disk ├─vdb1 252:17 0 1G 0 part └─vdb2 252:18 0 1G 0 part 2 格式化，将两个分区分别格式化为ext4和xfs文件系统 # mkfs 选项 设备名 -t 指定文件系统类型mkfs -t ext4 /dev/vdb1 mkfs.ext4 /dev/vdb1 [root@servera /]# mkfs -t ext4 /dev/vdb1 #mkfs格式化 ext4是文件系统类型 /dev/vdb1是要格式化的磁盘分区[root@servera /]# echo $? $?是看上一条命令返回值，0为正确，非0为错误0[root@servera /]# mkfs.xfs /dev/vdb2[root@servera /]# echo $?0[root@servera ~]# lsblk -f[root@servera ~]# lsblk -f /dev/vdb #查看文件系统类型，NAME FSTYPE LABEL UUID MOUNTPOINTvdb ├─vdb1 ext4 ecb332da-5bf4-4b86-b92e-d9da25f22a07 └─vdb2 xfs b538bf38-2b33-4d53-a785-372627587c52 3 挂载 创建挂载点 mkdir /mnt/disk1 文件系统：格式化后的设备或分区 挂载点：linux中的空目录挂载：mount 文件系统 挂载点 mount /dev/vdb1 /mnt/disk1卸载：umount 文件系统/挂载点 umount /dev/vdb1 or umount /mnt/disk1#挂载[root@servera ~]# lsblk -f /dev/vdbNAME FSTYPE LABEL UUID MOUNTPOINTvdb ├─vdb1 ext4 ecb332da-5bf4-4b86-b92e-d9da25f22a07 └─vdb2 xfs b538bf38-2b33-4d53-a785-372627587c52 [root@servera ~]# mkdir /mnt/{disk1,disk2} # 创建挂载点[root@servera ~]# ls /mntdisk1 disk2[root@servera ~]# mount /dev/vdb1 /mnt/disk1 #将/dev/vdb1 挂载到/mnt/disk1目录上[root@servera ~]# df [root@servera ~]# df -Th #-T 显示文件系统，-h以易读单位显示Filesystem Type Size Used Avail Use% Mounted on/dev/vdb1 ext4 976M 2.6M 907M 1% /mnt/disk1/dev/vdb2 xfs 1014M 40M 975M 4% /mnt/disk2[root@servera ~]# tree /mnt//mnt/├── disk1│ ├── haha.txt│ └── lost+found└── disk2 └── heihei.txt 卸载 [root@servera ~]# cd /mnt/disk2[root@servera disk2]# umount /dev/vdb2 #使用时不能卸载umount: /mnt/disk2: target is busy.[root@servera disk2]# cd / #需要退出挂载点[root@servera /]# umount /dev/vdb2 #卸载[root@servera /]# df -h | tail -2tmpfs 183M 0 183M 0% /run/user/0/dev/vdb1 976M 2.6M 907M 1% /mnt/disk1 14.1.3 检查文件系统12345678910111213141516# df查看系统挂载状态-T 查看文件系统类型-h 以易读方式列出容量单位du查看文件大小[root@servera /]# du /etc/[root@servera /]# du -h /etc/[root@servera /]# du -sh /etc/24M /etc/[root@servera /]# du /etc/man_db.conf 8 /etc/man_db.conf[root@servera /]# du /etc/man_db.conf -h #占用了的块大小，linux默认一个块4k8.0K /etc/man_db.conf[root@servera /]# ll /etc/man_db.conf -rw-r--r--. 1 root root 5165 Nov 7 2018 /etc/man_db.conf 14.2 文件查找1234567891011121314151617181920212223242526272829# locateupdatedb #收集所有文件元数据locate passwdlocate -i imagelocate -n 5 image #显示前5行# findfind 查找范围 查找条件 动作(可选)find / -name passwd-name： 以文件名的形式查找-size： 根据文件大小 -size 1k ：大小为1k的文件，+1k大于1k的文件，-1k小于1k的文件-user / -uid： 文件所有者 -user studnet ：student是用户名，查找student拥有的文件-group / -gid-perm： 权限查找 -perm 700 ：搜索权限为700的文件-type： 按文件类型 -type f ： f表示文件，d表示目录-exec： 选项后接Linux指令，操作查找到的文件 command {} ; find /etc -name sshd_configfind / -user student | xargs ls -lfind / -perm 700 -type d -user student | xargs ls -ldfind /etc -type f -size +3k -and -size -10k find / -perm -4000 | xargs ls -ld将系统中student用户的文件复制到/root/studentdir目录中，并且保留权限[root@servera ~]# find / -user student -exec cp -a {} /root/studentdir/ \\;将系统中student用户的文件列表保存到/root/studentdir文件中find / -user student &gt; /root/studentdir 14.3 ln 软链接与硬链接12345678910# 软链接(符号链接)：使用范围广，方便访问源文件# 文件链接创建方式：ln -s 源文件 链接文件ln -s dir1 linkdir1#硬链接：节省系统空间 ln 源文件 链接文件ln /opt/sou_file.txt /opt/link_file.txt# 取消链接[root@servera opt]# unlink /opt/link_file.txt 14.4 软链接和硬链接区别1234561.命令相同 参数不同2.硬链接的权限和源文件完全一致、软链接的链接文件权限永远是777，和源文件权限不同3.硬链接可以删除，移动源文件、软链接不可以删除，移动源文件4.软链接inode和源文件不同、硬链接的inode的源文件相同5.软链接可以对目录及文件生效、硬链接只可以对文件操作6.软链接可以跨文件系统、硬链接不可以跨文件系统","link":"/2025/04/18/Linux/Linux%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"title":"Shell脚本","text":"1 什么是Shell脚本","link":"/2025/05/10/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/Shell%E8%84%9A%E6%9C%AC/"},{"title":"Ansible的基本使用","text":"1 介绍Ansible &emsp;&emsp;Ansible是python开发的、开源的批量自动化运维工具 &emsp;&emsp;官方网站:https://www.ansible.com https://docs.ansible.com/ 1.1 Ansible的概念和架构&emsp;&emsp;通过inventor定义Managed node，并由SSH与python进行连通 &emsp;&emsp;&emsp;1.管理机上管理被管机 &emsp;&emsp;&emsp;2.inventory(清单)分组被管机 &emsp;&emsp;&emsp;3.基于SSH、PowerShell进行PUSH ad-hoc或基于YML文件Playbook &emsp;&emsp;&emsp;4.Python模块、插件、API 1.2 Absible的优势&emsp;&emsp;简单明了：Ansible playbook，更好的对任务进行排序编写 &emsp;&emsp;功能强大：3000多个模块，配置管理、工作流自动化、网络自动化 &emsp;&emsp;无需代理：通过SSH管理，而puppet、saltstack需要装客户端 &emsp;&emsp;与其他系统轻松集成，如jenkins &emsp;&emsp;跨平台支持：win、linux、unix、网络设备，虚拟机、物理机、云主机、容器 1.3 Ansible自动化平台2&emsp;&emsp;1.Ansible自动化平台2-Ansible Core &emsp;&emsp;&emsp;包含多个不同的组件，共同提供了一整套集成的自动化工具和资源 &emsp;&emsp;&emsp;提供用于运行Ansible Playbook的基本功能 &emsp;&emsp;&emsp;定义了用于在YAML文本文件中编写Ansible Playbook的自动化语言 &emsp;&emsp;&emsp;提供了自动化代码所需的关键功能，如循环、条件和其他Ansible命令 &emsp;&emsp;&emsp;提供了驱动自动化所需的框架和基本命令行工具 &emsp;&emsp;&emsp;在ansible-core RPM软件包及其ee-minimal-rhel8和ee-supported-rhel8自动化执行环境中提供Ansible Core 2.13 &emsp;&emsp;2.Ansible自动化平台2-Ansible内容集合 &emsp;&emsp;&emsp;在过去，Ansible提供了大量模块作为核心软件包的一部分;这种方法在Ansible社区中被称为“自带电池” &emsp;&emsp;&emsp;Ansible中包含的模块数量呈指数级增长。这导致了支持方面的一些挑战，特别是因为用户有时希望使用比Ansible特定版本中附带模块版本更早或更高的模块 &emsp;&emsp;&emsp;开发人员决定将大多数模块重新整理为单独的Ansible collection(内容集合)，这些资源collection相关的模块角色和插件构成，由同一组开发人员提供支持 &emsp;&emsp;&emsp;Ansible Core本身仅限于由ansible.builtin、Ansible collection(内容集合)提供的一小组模块，该集合始终是Ansible Core的一部分 &emsp;&emsp;&emsp;订阅红帽Ansible自动化平台2后，可获得红帽提供的120多个认证内容集合的访问权限，另外还可通过AnsibleGalaxy获得很多受社区支持的集合 &emsp;&emsp;3.Ansible自动化平台2概述-自动化内容导航器 &emsp;&emsp;&emsp;红帽Ansible自动化平台2还可提供新的顶级工具(自动化内容导航(ansble-navigator))来开发和测试Ansible Playbook。该工具可取代并扩展多个命令行实用程序的功能，包括ansible-playbook、ansible-inventory、ansible-config &emsp;&emsp;&emsp;通过在容器中运行playbook，将运行Ansible的控制节点与运行它的自动化执行环境分隔开来。这样一来，可以更轻松地为自动化代码提供完整的工作环境，以部署到生产环境中 &emsp;&emsp;4.Ansible自动化平台2-自动化执行环境 &emsp;&emsp;&emsp;自动化执行环境是一种容器镜像，包含Ansible Core、Ansible内容集合以及运行playbook所需的任何Python库、可执行文件或其他依赖项 &emsp;&emsp;&emsp;使用ansible-navigator运行playbook时，可选择用于运行该playbook的自动化执行环境 &emsp;&emsp;&emsp;当代码运行时，可以向自动化控制器提供playbook和自动化执行环境，并且也知道其具有正确运行playbook所需的一切 &emsp;&emsp;5.Ansible自动化平台2-准备控制节点 &emsp;&emsp;&emsp;要运行Ansible Playbook，需在控制节点安装自动化内容导航器(ansible-navigator)，然后下载执行环境 &emsp;&emsp;&emsp;由Ansible托管的主机无需安装ansible-navigator，该工具只需安装在运行Ansible Playbook的控制节点 &emsp;&emsp;&emsp;安装ansible-core软件包前，先要在控制节点上安装Python3.8或更高版本 &emsp;&emsp;&emsp;需要有效的红帽Ansible自动化平台订阅，才能在控制节点上安装自动化内容导航器 &emsp;&emsp;&emsp;如果已在红帽客户门户中为您的组织激活了简单内容访问，则无需再将订阅连接到系统 2 部署Ansible2.1 安装Ansible2.1.1 安装自动化内容导航器123456789101112131415161718192021222324# 登录workstation后默认为student普通用户，根据实际使用和考试结合建议使用普通用户操作# 方法1:[kiosk@foundation0 ~]$ ssh student@workstation# 方法2:[kiosk@foundation0 ~]$ ssh root@workstation[root@workstation ~]$ ssh student@localhost# 控制节点上安装自动化内容导航器 - workstation[student@workstation ~]$ sudo dnf search ansible #搜索ansible关键字的软件包，方便查找ansible的软件[student@workstation ~]$ sudo dnf -y install ansible-navigator.noarch ansible-core.x86_64[student@workstation ~]$ rpm -q ansible-navigatoransible-navigator-2.1.0-1.el9ap.noarch[student@workstation ~]$ rpm -q ansible-coreansible-core-2.13.0-2.el9ap.x86_64[student@workstation ~]$ rpm -qc ansible-core/etc/ansible/ansible.cfg # 默认全局配置文件/etc/ansible/hosts # 默认全局清单文件[student@workstation ~]$ rpm -qc ansible-navigator 2.1.2 检查Ansible版本1234567891011121314151617181920# 检查ansible的版本[student@workstation ~]$ ansible --versionansible [core 2.13.0] config file = /etc/ansible/ansible.cfg...[student@workstation ~]$ ansible-navigator --versionansible-navigator 2.1.0# 安装ansible后会提供导航器配置文件[student@workstation ~]$ ls -a. .ansible .ansible-navigator.yml .bash_logout .bashrc .config Documents .grading .jupyter Music Pictures .ssh .venv.. ansible-navigator.log .bash_history .bash_profile .cache Desktop Downloads .ipython .local .npm Public Templates Videos[student@workstation ~]$ cat .ansible-navigator.yml---ansible-navigator: execution-environment: image: utility.lab.example.com/ee-supported-rhel8:latest pull: policy: missing 2.1.3 登录镜像仓库1234567891011121314151617181920212223# 配置镜像仓库不进行https验证[student@workstation ~]$ mkdir ~/.config/containers # ~/.config/containers等价于/home/student/.config/containers[student@workstation ~]$ cp /etc/containers/registries.conf ~/.config/containers[student@workstation ~]$ vim ~/.config/containers/registries.confunqualified-search-registries = [&quot;utility.lab.example.com&quot;] # 22行[[registry]] # 24行insecure = true # 37行blocked = false # 40行location = &quot;utility.lab.example.com&quot; # 56行# 登录容器镜像服务器，为下载自动化执行环境容器镜像做准备[student@workstation ~]$ podman --versionpodman version 4.0.2[student@workstation ~]$ podman login -u admin -p redhat utility.lab.example.comLogin Succeeded!# 下载自动化内容导航器或使用ansible-navigator命令自动下载[student@workstation ~]$ ansible-navigator collections [student@workstation ~]$ ansible-navigator images # 查看下载的镜像环境 2.2 构建Ansible清单2.2.1 默认的清单文件123456789101112131415161718192021222324252627282930# 清单定义Ansible管理的主机集合。主机可以分配到组中进行集中管理。组可以包含子组，主机也可以是多个组的成员# 清单还可以设置应用到它所定义的主机和组的变量# 定义主机清单可采用两种方式: 1.使用文本文件定义静态主机清单 2.通过外部信息提供程序，使用Ansible插件按需生成动态主机清单# 系统默认清单在/etc/ansibile/hosts# 举例:# [嵌套组名:children]，嵌套组内只能包含组，不包含主机[test]servera[web]serverb.example.com[webservers]web[1:50].example.com # 表示从web1到web50，共计50台主机[servers:children] # 表示servers组是一个超级组，包含test和web两个组(嵌套)testwebwebservers# 指定清单范围格式：通配符或正则表达式的方法[START:END] #开始:结束范围192.168.[0:15].[0:255] #表示 192.168.0.0-192.168.15.255server[a:c].example.com #表示 a-cserver[01:15].example.com #表示 server01.example.com-server15.example.comall： #表示 所有主机ungrouped: #表示 指定组/未分配组的主机 2.2.2 自定义清单文件&emsp;&emsp;1.创建工作目录 1234567891011# 当使用特权用户管理Ansible时，可以在家目录中为其创建工作目录# 目录名称可以根据业务自定义名称,后续的所有文件都放到此目录，包括配置文件、清单文件、playbook等[student@workstation ~]$ mkdir ~/ansible[student@workstation ~]$ cd ~/ansible/[student@workstation ansible]$ pwd/home/student/ansible# 查看清单实例文件[student@workstation ansible]$ ansible-doc -t inventory -l[student@workstation ansible]$ ansible-doc -t inventory ini[student@workstation ~]$ ansible-navigator doc -t inventory ini &emsp;&emsp;2.编辑自定义清单文件 123456789101112131415[student@workstation ansible]$ vim inventoryservera # 未在组内的主机[web] # 主机组，主机组名称需要使用中括号括起来[]，web是组名称server[b:c] # 主机组成员，web组内的主机 表示两个主机serverb至serverc172.25.250:[10:15][db]server[d:z].lab.example.com# 嵌套组名servers是自定义的，:children是固定语法，表示web、db在servers组中，嵌套组成员应为组，不应为主机[servers:children] webdb# 不要在清单里书写无用的符号，及一些特殊符号。主机名称不要和主机组冲突，组名尽量不要用数字开头 &emsp;&emsp;3.验证清单 123456789101112131415161718# 使用ansible-navigator或ansible-inventory命令验证计算机是否存在于清单中 1.ansible-navigator inventory以stdout模式运行 2.第一条传统Ansible的验证方式，第二条使用Ansible自动化平台2的方式-RHEL8 &amp; 9-i inventory # 指定清单文件的位置--list-hosts # 列出清单中的主机 --graph # 通过ansible-inventory命令列出清单整个pattern$ ansible all -i inventory --list-hosts$ ansible-inventory --graph -i inventory [student@workstation ansible]$ ansible-inventory --graph -i /home/student/ansible/inventory-RHEL 9$ ansible-navigator inventory -i inventory -m stdout --list$ ansible-navigator inventory -i inventory -m stdout --graph$ ansible-navigator inventory -i inventory -m stdout --graph webservers重要：清单中含有名称相同的主机和主机组，ansible命令显示警告并以主机作为其目标，组被忽略 2.2.3 覆盖清单位置1234567891011121314# 安装ansible软件后，在/etc/ansible/hosts位置提供一个默认主机清单，属于全局管理范围# 特权用户管理时可以为其在工作目录中创建清单，在工作目录中使用ansible时，且优先度最高/home/student/ansible/inventory # 用于针对某个用户设置清单/etc/ansible/hosts # 用于全局设置[student@workstation /]$ ansible-inventory --graph@all: |--@ungrouped:[student@workstation /]$ pwd/[student@workstation ansible]$ ansible-inventory --graph@all: |--@ungrouped:[student@workstation ansible]$ pwd/home/student/ansible 2.2.4 多清单12345678910111213141516171819202122232425262728# 一个用户可以有一个或多个清单文件，若同时使用，可将多个清单文件放置在清单目录中，清单目录需自行创建，名称自定义[student@workstation ansible]$ vim inventoryservera[web]serverbserverc[db]serverd[servers:children]webdb[student@workstation ansible]$ ansible-inventory --graph -i inventory[student@workstation ansible]$ cp inventory inventory2 #额外创建一份清单名为inventory2# [student@workstation ansible]$ echo servere &gt; inventory2 #inventory2内指定一个主机servere[student@workstation ansible]$ vim inventory2 servere[student@workstation ansible]$ mkdir invdir #创建清单存储目录，名称自定义[student@workstation ansible]$ mv inv* invdir/ #将所有清单移动至清单存储目录mv: cannot move 'invdir' to a subdirectory of itself, 'invdir/invdir'[student@workstation ansible]$ ls invdir/inventory inventory2[student@workstation ansible]$ ansible-inventory --graph -i invdir/ #查看主机模式结构 2.3 管理Ansible配置文件2.3.1 配置Ansible123456789101112# 安装Ansible软件后，自动生成配置文件 /etc/ansible/ansible.cfg 用于配置多个Ansible工具的行为 ~/.ansible-navigator.yml 用于更改ansible-navigator命令默认选项[student@workstation ansible]$ rpm -qc ansible-core/etc/ansible/ansible.cfg/etc/ansible/hosts[student@workstation ~]$ ls -a. ansible .bash_history .bashrc Desktop .grading .lesshst .npm .ssh Videos.. ansible-navigator.log .bash_logout .cache Documents .ipython .local Pictures Templates .viminfo.ansible .ansible-navigator.yml .bash_profile .config Downloads .jupyter Music Public .venv[student@workstation ~]$ ls -a ~/.ansible-navigator.yml/home/student/.ansible-navigator.yml 2.3.2 管理Ansible设置 指令 描述 inventory 指定清单文件的路径。 remote_user 指定Ansible用于连接受管主机的用户名。如果未指定，则使用当前用户的名称。(在由ansible-navigator 运行的基于容器的自动化执行环境中，始终为 root。) ask_pass 指示是否提示输入SSH 密码。 (可以是 false，如果使用SSH公钥身份验证，则此为默认值。) become 指定连接后是否在受管主机上自动切换用户(一般切换为root)。这也可以通过play来指定 become_method 指定如何切换用户(通常为 sudo，此为默认值，但也可选择su) become_user 指定要在受管主机上切换到哪个用户(通常为 root，此为默认值) become_ask_pass 指示是否提示输入become_method参数密码。默认为false 2.3.3 管理配置文件&emsp;&emsp;1.自定义配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 在工作目录中生成ansible的配置文件[student@workstation ~]$ cat /etc/ansible/ansible.cfg | grep ansible.cfg# $ ansible-config init --disabled &gt; ansible.cfg...[student@workstation ~]$ ansible-config init --disabled &gt; /home/student/ansible/ansible.cfg[student@workstation ~]$ cd /home/student/ansible/[student@workstation ansible]$ lsansible.cfg inventory [student@workstation ansible]$ ansible --versionansible [core 2.13.0] config file = /home/student/ansible/ansible.cfg# Ansible的配置文件(ansible.cfg)由几部分组成，含有以键值对形式定义的设置、标题以方括号括起# 对于基本操作，使用以下两部分:1.[defaults]---用于设置Ansible操作的默认值2.[privilege_escalation]---用于配置Ansible如何在受管主机上执行特权升级 [student@workstation ansible]$ vim ansible.cfg[defaults]inventory=/home/student/ansible/inventory #139行 工作目录清单位置remote_user=student #222行 远程用户可选root或普通用户 host_key_checking=false #318行 不进行公钥记录[privilege_escalation] #搜/become，n向下查找 或 输入:430become=true #430行 开启特权功能，#become_ask_pass=False #433行 远程免特权密码，需要对端添加sudo免密 #become_method=sudo #442行 远程功能启用sudo#become_user=root #445行 特权用户为root备注:剪切操作 dd+p# 验证配置文件是否书写正确[student@workstation ansible]$ ansible-inventory --graph # 取消对清单的指定 #为servera~d主机设置sudo免密。[workstation][student@workstation ansible]$ for i in {a..d};do ssh root@server$i 'sed -i s/^%wheel.*$/&quot;%wheel ALL=(ALL) NOPASSWD: ALL&quot;/ /etc/sudoers';done[student@workstation ansible]$ for i in {a..d};do ssh root@server$i 'grep ^%wheel /etc/sudoers';done# 测试远程部署[student@workstation ansible]$ ansible all -m pingserverd | SUCCESS =&gt; { &quot;ansible_facts&quot;: { &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; }, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;}... &emsp;&emsp;2.配置文件的优先级 12345678$ /etc/ansible/ansible.cfg #默认路径$ ~/.ansible.cfg #家目录$ ~/ansible/ansible.cfg #ansible为工作目录(练习和考试时使用)$ grep ANSIBLE_CONFIG /etc/profile #环境变量export ANSIBLE_CONFIG=/opt/ansible.cfg （此时/opt下需要有ansible.cfg配置文件）source /etc/profile 加载# 优先级 ：变量＞当前目录＞用户家目录＞/etc &emsp;&emsp;3.指定远程用户 12345678910111213# 其他指定远程用户及密码的方法# 方法1：ansible.cfgremote_user=rootinventory[all:vars]ansible_password=redhat# 方法二：inventory[all:vars]ansible_user=rootansible_password=redhat 2.3.4 管理自动化内容导航器&emsp;&emsp;1.创建自动化内容导航器 123456789101112#可以为ansible-navigator创建配置文件以覆盖其配置设置的默认值，采用JSON(json)或YAML(yml或yam)格式#自动化内容导航器按以下顺序查找配置文件，并使用它找到的第一个文件:1.如果设置了ANSIBLE NAVIGATOR_CONFIG环境变量，则使用所指定位置处的配置文件2.当前Ansible项目目录中的ansible-navigator.yml文件3.~/.ansible-navigator.yml文件(主目录中)。请注意，其文件名开头有一个“点”#与Ansible配置文件一样，每个项目都可以有自己的自动化内容导航器设置文件 #创建导航器配置文件[student@workstation ~]$ rpm -ql ansible-navigator | grep temp[student@workstation ~]$ vim /usr/lib/python3.9/site-packages/ansible_navigator/package_data/settings-sample.template.yml &emsp;&emsp;2.配置自动化内容导航器 12345678910111213141516171819[student@workstation ~]$ vim ~/.ansible-navigator.yml---ansible-navigator: execution-environment: image: utility.lab.example.com/ee-supported-rhel8:latest pull: policy: missing # 容器镜像下载策略 missing系统里有就不下载，否则下载$ vim ~/.ansible-navigator.yml ---ansible-navigator: execution-environment: image: utility.lab.example.com/ee-supported-rhel8:latest pull: arguments: - &quot;--tls-verify=false&quot; policy: missing playbook-artifact: enable: false #关闭运行playbook时生成日志 执行剧本所产生的日志，执行一次记录一次 2.3.5 ansible的远程连接123456#anisble远程连接时的必要条件1.配置链接：如何选择远程的用户2.清单位置：相对路径、绝对路径、多清单等。3.链接设置: remote_user= ，~/.ansible-navigator.yml中playbook-artifact：4.ssh免密: ssh-key-gen，ssh-copy-id5.升级特权: visudo，/etc/sudoers 3 编写和运行Playbook&emsp;&emsp;Play是针对清单中选定的主机运行的一组有序任务 &emsp;&emsp;Playbook是一个文本文件，其中包含由一个或多个按特定顺序运行的play组成的列表 &emsp;&emsp;临时命令可以作为一次性命令对一组目标主机运行一项简单的任务 &emsp;&emsp;Playbook可以通过轻松重复的方式对一组目标主机执行多项复杂的任务 3.1 Playbook-yaml语法&emsp;&emsp;1.playbook是使用YAML语法编写的文本文件，格式为.yml &emsp;&emsp;2.严格缩进，空格 &emsp;&emsp;3.YAML文件以—开头，…. 结束，可以省略 &emsp;&emsp;4.使用”-“(减号加一个或多个空格)作为列表项 &emsp;&emsp;5.#注释 1234567891011$ ansible-doc -l | grep yum$ ansible-doc yum #进入之后，搜索/EXAMPLE$ vim ~/ansible/web.yml--- #yaml语法，---开头- name: install httpd #play任务的描述：描述部分name字段是可选的选项 hosts: servera #主机模式：任务目标主机 tasks: #任务列表：下面通常为任务模块，有两格缩进 - name: Install Apache #任务模块：注意和tasks有两格缩进，name是可选字段，模块描述 ansible.builtin.yum: #模块名称： name: httpd #模块选项1 state: latest #模块选项2 3.2 关闭运行playbook时的日志123456789[student@workstation ansible]$ vim ~/.ansible-navigator.yml---ansible-navigator: execution-environment: image: utility.lab.example.com/ee-supported-rhel8:latest pull: policy: missing playbook-artifact: enable: false #关闭playbook时生成的日志 3.3 调整tab键缩进 &emsp;&emsp;整体缩进–方便对齐yaml语法文本缩进 123456789101112131415161718vim ~/.vimrcset tabstop=2 #将vim的tab键缩进调至两格set nu #设置行号set autoindent #回车时调整至上一行文本缩进 set cursorcolumn #设置竖坐标线 简写set cucset cursorline #设置横坐标线 简写set cul:set all #末行模式下set all 查看所有环境设置帮助#整体缩进 视图模式ctrl+v ， jjj(+G) ，I，(空格、空格)，esc#分解：1.光标放在需要调节的行上2.按ctrl+v，用方向键或G选定需要调节的列3.输入I进入插入模式4.空格空格，调节需要的缩进5.按esc同步所有列 3.4 查找用于任务的模块&emsp;&emsp;用Ansible进行部署任务时，可针对管理员任务需求，选择对应功能模块，通过以下方法可以查询模块帮助，以便编写临时命令及playbook 123456789101112131415-RHEL&lt;=8[student@workstation ansible]$ ansible-doc -l[student@workstation ansible]$ ansible-doc -l | grep yumyum Manages packages with the `y...yum_repository Add or remove YUM repositori...# 查看某个模块的帮助[student@workstation ansible]$ ansible-doc yum-RHEL=9$ ansible-navigator collections$ ansible-navigator doc ansible.posix.firewalld$ ansible-navigator doc ansible.posix.firewalld -m stdout[student@workstation ansible]$ ansible-navigator collections -m stdout | grep firewalld 3.5 运行Playbook&emsp;&emsp;运行前检查语法错误或执行空运行： &emsp;&emsp;&emsp;1.检查语法错误 ansible-navigator run web.yml –syntax-check file.yml &emsp;&emsp;&emsp;2.执行空运行 ansible-navigator run web.yml -C &emsp;&emsp;通常直接运行ansible-navigator run web.yml进入交互模式或添加-m stdout可将任务打印到标准输出 123#运行playbook$ ansible-navigator run web.yml # 交互式$ ansible-navigator run web.yml -m stdout # 非交互式 3.6 实施多个Play&emsp;&emsp;1.Playbook是一个YAML文件，含有由一个或多个play组成的列表 &emsp;&emsp;2.如果一个playbook中含有多个 play，每个play可以将其任务应用到单独的一组主机 &emsp;&emsp;3.编排涉及对不同主机执行不同任务的复杂部署时会大有帮助 &emsp;&emsp;4.可以这样编写playbook：对一组主机运行一个play，完成后再对另一组主机运行另一个pla &emsp;&emsp;5.Playbook中的各个play编写为playbook中的顶级列表项 3.6.1 编写多个Play1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[student@workstation ansible]$ vim install_yum.yml---- name: install yum repo hosts: servera,serverb - name: AppStream_REPO ansible.builtin.yum_repository: name: Red Hat Enterprise Linux 9 for x86_64 - AppStream (RPMs) description: AppStream file: rhel_AppStream baseurl: http://content.example.com/rhel9.0/x86_64/dvd/AppStream gpgcheck: no- name: install yum repo hosts: serverc tasks: - name: BaseOS_REPO ansible.builtin.yum_repository: name: Red Hat Enterprise Linux 9 for x86_64 - BaseOS (RPMs) description: BaseOS file: rhel_BaseOS baseurl: http://content.example.com/rhel9.0/x86_64/dvd/BaseOS gpgcheck: no #ansible-navigator collections,ansible-navigator doc ansible.posix.firewalld -m stdout[student@workstation ansible]$ ansible-navigator collections -m stdout | grep firewalld[student@workstation ansible]$ ansible-navigator doc ansible.posix.firewalld -m stdout--syntax-check [student@workstation ansible]$ ansible-navigator run install_yum.yml -m stdout --syntax-checkplaybook: /home/student/ansible/install_yum.yml-v -vv -vvv[student@workstation ansible]$ ansible-navigator run install_yum.yml -m stdout --syntax-check -v[student@workstation ansible]$ ansible-navigator run install_yum.yml -m stdout# 测试运行剧本后的结果[student@workstation ansible]$ ansible -m shell all -a 'yun -y install ftp'[student@workstation ansible]$ ansible all -m shell -a 'rpm -q ftp'# 用ansible-doc查询所有模块yum 安装软件service 管理服务shell模块 管理防火墙copy 拷贝，1有拷贝功能，2 可以将一段文本，复制到某个文件中，如文件不存在，则生成文件。uri 网站连接测试 3.6.2 选择模块 类别 模块 文件 ansible.builtin.copy: 将本地文件复制到受管主机ansible.builtin.file: 设置文件的权限和其他属性ansible.builtin.lineinfile: 确保特定行是否在文件中ansible.posix.synchronize: 使用rsync 同步内容 软件 ansible.builtin.package: 使用操作系统自带的自动检测软件包管理器管理软件包。ansible.builtin.dnf: 使用DNF软件包管理器管理软件包ansible.builtin.apt: 使用APT 软件包管理器管理软件包ansible.builtin.pip: 从PyPI管理Python 软件包。 系统 ansible.posix.firewalld: 使用firewalld 管理任意端口和服务ansible.builtin.reboot: 重新启动计算机。ansible.builtin.service: 管理服务ansible.builtin.user: 添加、删除和管理用户帐户 网络工具 ansible.builtin.get_url: 通过HTTP、HTTPS或FTP 下载文件ansible.builtin.uri: 与Web 服务交互 4 管理变量4.1 变量概述4.1.1 Ansible变量简介&emsp;&emsp;Ansible支持利用变量来存储值，并在Ansible项目的所有文件中重复使用这些值 &emsp;&emsp;可以简化项目的创建和维护，并减少错误的数量 1234567# key：vaule # 变量可以重复的应用到项目中，简化管理，应用对象可以是：1.要创建的用户2.要安装的软件包3.要重新启动的服务4.要删除的文件5.互联网的文档等 4.1.2 命名变量&emsp;&emsp;变量名称必须以字母开头，并且只能含有字母、数字和下划线 无效变量名称 有效变量名称 web server web_server remote.file remote_file 1st file file_1，file1 remoteserver $1 remote_server_1,remote_server1 4.2 定义变量&emsp;&emsp;可在Ansible项目中的多个位置定义变量 &emsp;&emsp;如果在两个位置设置了同名变量，并且变量值不同，则通过优先级来决定要使用哪个值 &emsp;&emsp;可以设置会影响一组主机的变量，也可以设置只会影响个别主机的变量 &emsp;&emsp;有些变量是Ansible可以根据系统配置来设置的事实 &emsp;&emsp;有些变量可在playbook中设置，然后影响该playbook中的一个play，或者仅影响该play中的一项任务 &emsp;&emsp;可通过–extra-vars或-e选项并指定变量值 &emsp;&emsp;Ansible的变量可以定义在不同位置，根据需要设定，其中也有优先度 应用场景 描述 优先度 全局范围 命令行执行临时命令时指定的变量 -e key=vaule 高 play范围 playbook的Play部分或模块内部指定变量信息key: vaule 中 主机范围 清单中主机或主机组指定变量（主机 优先 主机组） 低 4.2.1 全局范围-命令行&emsp;&emsp;清单变量可被playbook中设置的变量覆盖，这两种变量又可通过在命令行中传递参数到ansible-navigatorrun 命令来覆盖 &emsp;&emsp;在命令行上设置的变量称为额外变量 1234567#命令行使用变量优先级最高[student@workstation ansible]$ ansible servera -m shell -a whoami -e ansible_user=root -e ansible_password=redhatservera | CHANGED | rc=0 &gt;&gt;root#在执行playbook时指定变量[student@workstation ansible]$ ansible-navigator run install_yum.yml -m stdout -e ansible_user=root -e ansible_password=redhat 4.2.2 PlAY范围-Playbook&emsp;&emsp;变量在Ansible Playbook中发挥着重要作用，可以简化playbook中变量数据的管理 &emsp;&emsp;编写play时，可以定义自己的变量，然后在任务中调用这些值 &emsp;&emsp;例如，可以使用值httpd来定义名为web_package的变量，任务可以使用ansible.builtin.dnf模块调用该变量来安装httpd软件包 12345678910111213141516171819202122232425262728293031323334353637383940414243#playbook中可以在play位置使用vars直接定义变量，也可以通过vars_files加载包含变量的文件。#1.在playbook的play部分使用vars直接定义变量-vars[student@workstation ansible]$ vim web.yml---- name: PLAY1 hosts: servera vars: #vars关键字就是在playbook中设置自定义变量 - package: httpd #key：vaule 键值间：冒号隔开，冒号后有一个空格 tasks: - name: install {{ package }} ansible.builtin.yum: name: &quot;{{ package }}&quot; #使用变量时，变量两边有空格，并且用双大括号括起来，变量开头要加“”双引号，非变量开头不用加双引号 state: latest [student@workstation ansible]$ ansible-navigator run web.yml -m stdout --syntax-checkplaybook: /home/student/ansible/web.yml[student@workstation ansible]$ ansible-navigator run web.yml -m stdout#2.生成变量文件-vars_files [student@workstation ansible]$ vim /home/student/ansible/var.yml ---package: httpd #定义变量[student@workstation ansible]$ vim /home/student/ansible/httpd.yml---- name: PLAY2 hosts: serverb vars_files: - /home/student/ansible/var.yml #vars_files 加载变量文件到剧本中 tasks: - name: install {{ package }} ansible.builtin.yum: name: &quot;{{ package }}&quot; state: latest [student@workstation ansible]$ ansible-navigator run httpd.yml -m stdout --syntax-checkplaybook: /home/student/ansible/httpd.yml[student@workstation ansible]$ ansible-navigator run httpd.yml -m stdout 4.2.3 主机范围-清单中&emsp;&emsp;直接应用于主机的清单变量分为两大类： &emsp;&emsp;&emsp;1.主机变量：应用于特定主机 &emsp;&emsp;&emsp;2.组变量：应用于一个主机组或组主机组中的所有主机。 &emsp;&emsp;主机变量优先于组变量，但playbook中定义的变量的优先级比这两者更高 &emsp;&emsp;若要定义主机变量和组变量，一种方法是直接在清单文件中定义 123456789101112131415161718[student@workstation ansible]$ vim /home/student/ansible/inventory172.25.250.9 ansible_password=redhat #给主机定义变量[test]172.25.250.10 [test:vars] #主机组变量中vars是固定语法ansible_password=redhat #给主机组定义变量 [prod]172.25.250.[11:12][balancers]172.25.250.13[all:vars] #给所有主机和主机组组定义变量ansible_user=rootansible_password=redhat 4.2.4 目录填充主机和组变量&emsp;&emsp;定义主机和主机组变量的首选做法是在清单文件或目录相同的工作目录中创建group_vars和host_vars两个目录，这两个目录分别包含用于定义组变量和主机变量文件 &emsp;&emsp;建议在host_vars和group_vars目录定义清单变量，而不是直接在清单文件中定义它们 4.3 字典形式表示变量&emsp;&emsp;除了将与同一元素相关的配置数据分配到多个变量外，管理员也可以使用字典 &emsp;&emsp;字典是一个包含键值对的数据结构，其中的值也可以是字典 1234567891011vim vari.yml---users: user1: A_name: zhang B_name: san C_name: /home/zhangsan user2: A_name: li B_name: si C_name: /home/lisi 4.4 调用变量12345678910111213141516171819#方法1： users.user1.A_name users.user2.B_name#方法2： 应用方法2：python字典 users['user1']['A_name'] # [student@bastion ansible]$ vim user.yml---- name: useradd hosts: dev vars_files: - vari.yml tasks: - name: Add the user ansible.builtin.user: name: &quot;{{ users.user1.A_name }}{{ users.user1.B_name }}&quot; home: &quot;{{ users['user1']['C_name'] }}&quot; 4.5 已注册变量捕获命令输出&emsp;&emsp;register用来捕获命令输出或有关模块执行的其他信息，输出会保存至一个变量中 12345678910111213141516$ vim register.yml---- name: install a packages hosts: servera tasks: - name: install the latest version of Apache ansible.builtin.yum: name: httpd state: latest register: install_result #register字段负责收集变量 install_result自定义变量名被收集变量名 - name: message ansible.builtin.debug: var: install_result #debug模块var选项打印register获取install_result变量值#验证$ ansible-navigator run -m stdout register.yml 5 管理机密-vault5.1 介绍Ansible-vault&emsp;&emsp;Ansible可能需要访问密码或API密钥等敏感数据，以配置受管主机 &emsp;&emsp;通常信息会以纯文本形式存储在清单变量或其他Ansible文件中。但若如此，任何有权访问Ansible文件的用户或存储这些Ansible文件的版本控制系统都能够访问此敏感数据，这显然存在安全风险 &emsp;&emsp;使用Ansible的Ansible Vault可以加密和解密任何由Ansible使用的数据文件 &emsp;&emsp;可通过一个名为ansible-vault的命令行工具创建、编辑、加密、解密和查看文件 &emsp;&emsp;Ansible Vault可以加密任何由Ansible使用的数据文件，这可能包括清单变量、playbook中含有的变量文件、在执行playbook时作为参数传递的变量文件或者Ansible角色中定义的变量 123456789101112[student@workstation ~]$ ansible-vault --helpusage: ansible-vault [-h] [--version] [-v] {create,decrypt,edit,view,encrypt,encrypt_string,rekey} ...encryption/decryption utility for Ansible data filespositional arguments: {create,decrypt,edit,view,encrypt,encrypt_string,rekey} create Create new vault encrypted file #创建密码文件 decrypt Decrypt vault encrypted file #解密现有密码文件 edit Edit vault encrypted file #编辑现有密码文件 view View vault encrypted file #查看加密文件 encrypt Encrypt YAML file #加密现有文件 encrypt_string Encrypt a string rekey Re-key a vault encrypted file #更改加密文件的密码 5.2 创建与查看加密文件123456789101112#创建加密文件-create[student@workstation ansible]$ ansible-vault create sec1.txt #默认sec1.txt不存在，通过该命令生成New Vault password: redhatConfirm New Vault password: redhat[student@workstation ansible]$ cat sec1.txt#查看加密文件-view[student@workstation ansible]$ ansible-vault view sec1.txtVault password: redhat---This is a encrypted file! 5.3 编辑现有的加密文件1234567891011#编辑加密文件-edit[student@workstation ansible]$ pwd/home/student/ansible[student@workstation ansible]$ ansible-vault edit sec1.txtVault password:[student@workstation ansible]$ ansible-vault view sec1.txtVault password:---This is a encrypted file!password: redhat321 5.4 加密现有的文件123456789101112131415[student@workstation ansible]$ echo China &gt; sec2.txt #创建文件[student@workstation ansible]$ cat sec2.txt China#加密现有文件-encrypt[student@workstation ansible]$ ansible-vault encrypt sec2.txt New Vault password: redhatConfirm New Vault password: redhatEncryption successful[student@workstation ansible]$ cat sec2.txt[student@workstation ansible]$ ansible-vault view sec2.txtVault password:China 5.5 解密现有的文件12345678#解密现有的文件-decrypt[student@workstation ansible]$ pwd/home/student/ansible[student@workstation ansible]$ ansible-vault decrypt sec2.txtVault password: redhatDecryption successful[student@workstation ansible]$ cat sec2.txtChina 5.6 更改加密文件的密码123456789101112131415#更改加密文件的密码-rekey[student@workstation ansible]$ pwd/home/student/ansible[student@workstation ansible]$ ansible-vault rekey sec1.txtVault password: redhat #旧密码 redhatNew Vault password: redhat321 #新密码 redhat321 Confirm New Vault password: redhat321 #重复新密码 redhat321Rekey successful[student@workstation ansible]$ ansible-vault view sec1.txtVault password: redhat321---This is a encrypted file!password: redhat321 5.7 使用密码文件12345678910111213141516171819#查看帮助[student@workstation ansible]$ pwd/home/student/ansible[student@workstation ansible]$ ansible-vault view sec1.txt --help---省略--- --vault-id VAULT_IDS the vault identity to use --ask-vault-password, --ask-vault-pass ask for vault password --vault-password-file VAULT_PASSWORD_FILES, --vault-pass-file VAULT_PASSWORD_FILES vault password file---省略---[student@workstation ansible]$ echo redhat321 &gt; secret.txt #创建密码文件[student@workstation ansible]$ cat secret.txtredhat321[student@workstation ansible]$ ansible-vault view sec1.txt --vault-id=secret.txt #通过变量文件指定密码---This is a encrypted file!password: redhat321 5.8 密码文件记录到ansible.cfg配置文件&emsp;&emsp;密码文件记录在ansible.cfg配置文件中的好处是，当执行一个使用了加密文件的playbook时，不必手工指定加密文件密码 1234567891011121314151617181920212223[student@workstation ansible]$ vim ansible.cfg #第一次填写vault路径时，搜索`vault`关键字找该选项vault_password_file=/home/student/ansible/secret.txt #配置文件中指定密码文件位置 [student@workstation ansible]$ ansible-vault view sec1.txt #自动调用配置文件中密码文件---This is a encrypted file!password: redhat321#密码文件记录在配置文件中后1.加密现有文件时会直接引用[student@workstation ansible]$ ansible-vault encrypt sec2.txtEncryption successful2.更改密码时会直接覆盖[student@workstation ansible]$ ansible-vault rekey sec2.txtRekey successful3.更改密码需使用--ask-vault-password参数指定[student@workstation ansible]$ ansible-vault rekey sec2.txt --ask-vault-passwordVault password: redhatNew Vault password: redhatConfirm New Vault password: redhatRekey successful 6 管理事实facts&emsp;&emsp;Ansible事实是Ansible在受管主机上自动检测到的变量 &emsp;&emsp;事实中含有与主机相关的信息，可以像play中的常规变量、条件、循环或依赖于从受管主机收集的值的任何其他语句那样使用 &emsp;&emsp;为受管主机收集的一些事实可能包括：主机名称、内核版本、网络接口名称、网络接口IP地址、操作系统版本、CPU数量提供的或可用的内存、存储设备的大小和可用空间 &emsp;&emsp;借助事实可以方便地检索受管主机的状态，并根据该状态确定要执行的操作。例如: &emsp;&emsp;&emsp;1.根据含有受管主机当前内核版本的事实运行条件任务，以此来重新启动服务器 &emsp;&emsp;&emsp;2.根据通过事实报告的可用内存来定义MySQL配置文件 &emsp;&emsp;&emsp;3.根据事实的值设置配置文件中使用的IPv4地址 &emsp;&emsp;每个play在执行第一个任务之前会先自动运行setup模块来收集事实，这在Ansible2.3中报告为GatheringFacts任务或者更早版本中报告为setup。默认情况下，无需具有在play中运行setup的任务，通常会自动运行 &emsp;&emsp;查看受管主机收集的事实的方式是： &emsp;&emsp;&emsp;1.使用ad-hoc命令运行setup模块 &emsp;&emsp;&emsp;2.使用playbook运行debug模块并提取变量var: ansible facts 6.1 收集事实1234# 收集事实常用两种手段常用是临时命令ad-hoc及Playbook，事实以josn语法格式列出。收集时要找ansible_开头的事实名称# 如果变量值为散列/字典，则可以用两种语法来检索该值:ansible_default_ipv4.hostnameansible_default_ipv4.[ 'hostname' ] 6.1.1 临时命令 ad-hoc12345671.使用ad-hoc方式收集事实[student@workstation ansible]$ ansible -m setup all #收集清单中所有主机的事实[student@workstation ansible]$ ansible -m setup servera -a filter=ansible_nodename #过滤[student@workstation ansible]$ ansible -m setup serverc -a filter=*ipv4* #过滤&amp;模糊匹配[student@workstation ansible]$ ansible -m setup serverc &gt; fact.txt #将事实记录到fact.txt文件中，方便查找 6.1.2 PLAYBOOK 收集事实1234567891011121314[student@workstation ansible]$ vim debug.yml---- name: debug hosts: servera tasks: - ansible.builtin.debug: #简化后可将ansible_facts去掉，二级变量开头，要保留ansible_,如：ansible_default_ipv4.address msg: servera ip address &quot;{{ ansible_default_ipv4.address }}&quot; - ansible.builtin.debug: var: ansible_hostname [student@workstation ansible]$ ansible-navigator run debug.yml -m stdout --syntax-checkplaybook: /home/student/ansible/debug.yml[student@workstation ansible]$ ansible-navigator run debug.yml -m stdout 6.1.3 关闭事实123456789101112[student@workstation ansible]$ vim debug.yml---- name: debug message hosts: dev gather_facts: on/off true/false # 是否关闭事实 tasks: - debug: msg: &quot;{{ ansible_facts.default_ipv4.address }}&quot; [greg@bastion ansible]$ ansible-playbook debug.yml#如果playbook内容和事实收集没有关系，关闭可以大量减少playbook执行时间 6.2 魔法变量&emsp;&emsp;实时变量通常收集的是受管节点的信息，而魔法变量收集的是本机的变量值 1234567891011121314#inventory_hostname 列出组在清单中的主机名[student@workstation ansible]$ ansible web -m debug -a var=inventory_hostname#group_names 列出当前主机归属于哪个组[student@workstation ansible]$ ansible serverd -m debug -a var=group_names#groups 列出清单中的所有主机名称。以及所在组[student@workstation ansible]$ ansible all -m debug -a var=groups#hostvars 列出系统中所有魔法变量及所有事实变量[student@workstation ansible]$ ansible all -m debug -a var=hostvars$ https://docs.ansible.com/ansible/latest/reference_appendices/special_variables.html #官网文档# dosc.ansible.com中搜索magic或facts 6.2.1 临时命令收集魔法变量值 ad-hoc12345ansible servera -m debug -a var=inventory_hostnameansible servera -m debug -a var=groupsansible all -m debug -a var=group_namesansible all -m debug -a var=hostvarsansible servera -m debug -a var=groups.all 6.2.2 PLAYBOOK收集事实+魔法变量12345678910111213141516[student@workstation ansible]$ vim hoc_debug.yml---- name: debug hosts: servera gather_facts: on tasks: - ansible.builtin.debug: var: hostvars[student@workstation ansible]$ ansible-navigator run hoc_debug.yml -m stdout --syntax-checkplaybook: /home/student/ansible/hoc_debug.yml[student@workstation ansible]$ ansible-navigator run hoc_debug.yml -m stdout#重定向到文件中的意义是方便在hoc_fact.txt文件中搜索需要的值[student@workstation ansible]$ ansible-navigator run hoc_debug.yml -m stdout &gt; hoc_fact.txt 6.2.3 魔法变量hostvars&emsp;&emsp;临时命令与playbook收集hostvars变量值是不同的 &emsp;&emsp;临时命令不会执行setup模块，所以收集不到事实，PLAYBOOK方法则可以收集到事实和魔法变量 1[student@workstation ansible]$ ansible all -m setup &gt; all_host.txt 7 实施任务控制7.1 利用循环迭代任务&emsp;&emsp;利用循环，管理员无需编写多个使用同一模块的任务 &emsp;&emsp;Ansible支持使用loop关键字对一组项目迭代任务，可以配置循环以利用列表中各个项目、列表中各个文件的内容、生成的数字序列或更为复杂的结构来重复任务 123#loop字段通常在同一缩进的模块下面，对该模块生效，通过item来加载loop循环中的值或变量#帮助：搜索loop可以搜到相应语法$ https://docs.ansible.com/ansible/latest/user_guide/playbooks_loops.html 7.1.1 简单循环&emsp;&emsp;简单循环对一组项目迭代任务 &emsp;&emsp;loop 关键字添加到任务中，将应对其迭代任务的项目列表取为值 &emsp;&emsp;循环变量item保存每个迭代过程中使用的值 1234567891011121314151617[student@workstation ansible]$ vim loop.yml---- name: service hosts: servera tasks: - name: Start service nfs-server&amp;chronyd ansible.builtin.service: name: &quot;{{ item }}&quot; state: started loop: - nfs-server - chronyd [student@workstation ansible]$ ansible-navigator run loop.yml -m stdout --syntax-checkplaybook: /home/student/ansible/loop.yml[student@workstation ansible]$ ansible-navigator run loop.yml -m stdout 7.1.2 循环使用变量&emsp;&emsp;在playbook中通过vars或vars files方式加载变量servers中包含循环列表，模块通过loop字段加载servers变量列表中的值 123456789101112131415161718192021222324252627282930313233343536373839# vars字段[student@workstation ansible]$ vim loop.yml---- name: service loop hosts: servera vars: servers: - nfs-server - chronyd tasks: - name: Start service ansible.builtin.service: name: &quot;{{ item }}&quot; state: stopped loop: &quot;{{ servers }}&quot; # vars字段也可以替换为vars_files，将变量保存至文件中，加载到Playbook [student@workstation ansible]$ mkdir /home/student/ansible/vars/[student@workstation ansible]$ vim /home/student/ansible/vars/var.ymlservers:- nfs-server- chronyd[student@workstation ansible]$ vim loop.yml---- name: service loop hosts: servera vars_files: - /home/student/ansible/vars/var.yml tasks: - name: Start service ansible.builtin.service: name: &quot;{{ item }}&quot; state: stopped loop: &quot;{{ servers }}&quot;[student@workstation ansible]$ ansible-navigator run loop.yml -m stdout --syntax-checkplaybook: /home/student/ansible/loop.yml[student@workstation ansible]$ ansible-navigator run loop.yml -m stdout 7.1.3 循环字典列表&emsp;&emsp;1.循环字典列表保存在loop字段中 1234567891011121314151617181920[student@workstation ansible]$ vim loop.yml---- name: service loop hosts: servera tasks: - name: Start service ansible.builtin.user: name: &quot;{{ item.name }}&quot; comment: &quot;{{ item.comment }}&quot; state: present loop: - name: jane comment: tom - name: joe comment: harry [student@workstation ansible]$ ansible-navigator run loop.yml -m stdout --syntax-checkplaybook: /home/student/ansible/loop.yml[student@workstation ansible]$ ansible-navigator run loop.yml -m stdout &emsp;&emsp;2.循环字典列表保存在play的vars中 12345678910111213141516171819202122[student@workstation ansible]$ vim loop.yml---- name: service loop hosts: servera vars: users: - name: jane comment: tom - name: joe comment: harry tasks: - name: Start service ansible.builtin.user: name: &quot;{{ item.name }}&quot; comment: &quot;{{ item.comment }}&quot; state: present loop: &quot;{{ users }}&quot;[student@workstation ansible]$ ansible-navigator run loop.yml -m stdout --syntax-checkplaybook: /home/student/ansible/loop.yml[student@workstation ansible]$ ansible-navigator run loop.yml -m stdout &emsp;&emsp;3.循环字典列表保存在play的vars_files中 1234567891011121314151617181920212223242526[student@workstation ansible]$ vim /home/student/ansible/vars/var.ymlservers:users:- name: jane comment: tom- name: joe comment: harry[student@workstation ansible]$ vim loop.yml---- name: service loop hosts: servera vars_files: - /home/student/ansible/vars/var.yml tasks: - name: Start service ansible.builtin.user: name: &quot;{{ item.name }}&quot; state: present comment: &quot;{{ item.comment }}&quot; loop: &quot;{{ users }}&quot; [student@workstation ansible]$ ansible-navigator run loop.yml -m stdout --syntax-checkplaybook: /home/student/ansible/loop.yml[student@workstation ansible]$ ansible-navigator run loop.yml -m stdout 7.2 有条件地运行任务 &emsp;&emsp;使用conditionals在符合特定条件时运行任务或play &emsp;&emsp;条件句可帮助区分不同的受管主机，并根据所符合的条件来分配功能角色 &emsp;&emsp;Playbook变量、注册的变量和Ansible事实都可通过条件句来进行测试，可以使用比较字符串、数字数据和布尔值的运算符 &emsp;&emsp;1.通常主机模式为多个节点时，可以让符合when条件的主机执行模块任务。符合条件则为真，则执行模块。否则为假，跳过模块任务 &emsp;&emsp;&emsp;when判断对象是模块，和模块在同一下列层次 &emsp;&emsp;&emsp;when判断当前模块是否执行，而不是它下面模块是否执行 &emsp;&emsp;&emsp;When中引用变量、facts，不需加大括号 &emsp;&emsp;&emsp;用于测试条件中相等的==运算符不可与变量赋值的=运算符混淆 &emsp;&emsp;2.一个when语句可用于评估多个值。 可以使用and和or关键字组合条件，或使用括号分组条件 ​ 12345678910111213141516171.or是或的关系，任意一个条件为真即可when: ansible_distribution == &quot;RedHat&quot; or ansible_distribution == &quot;Fedora&quot;2.and是与的关系，多个条件需同时为真when: ansible_distribution version == &quot;7.5&quot; and ansible_kernel == &quot;3.10.0-327.el7.x86_64&quot;3. When语句多条件的另外方式:when: -ansible_distribution_version == &quot;7.5&quot; -ansible_kernel == &quot;3.10.0-327.el7.x86_64&quot;4.使用括号分组条件来表达更复杂的条件语句:when: &gt; ( ansible_distribution == &quot;RedHat&quot; and ansible_distribution_major_version == &quot;7&quot;) or ( ansible_distribution == &quot;Fedora&quot; and ansible_distribution_major_version == &quot;28&quot;) 7.2.1 简单的有条件任务1234567891011121314151617[student@workstation ansible]$ vim when.yml---- name: repository hosts: all tasks: - name: install the latest version of Apache ansible.builtin.yum: name: httpd state: latest when: ansible_default_ipv4.address == '172.25.250.10' [student@workstation ansible]$ ansible-navigator run when.yml -m stdout --syntax-checkplaybook: /home/student/ansible/when.yml[student@workstation ansible]$ ansible-navigator run when.yml -m stdout#如果使用inventory_hostname这个魔法变量，要参考清单中的主机名称。node1位置，单双引号都可以识别为字符串 7.2.2 组合循环和有条件任务1234567891011121314151617[student@workstation ansible]$ vim when_loop.yml---- name: 安装软件包 hosts: all tasks: - name: install the latest version of Apache ansible.builtin.yum: name: &quot;{{ item }}&quot; state: latest loop: - php - mariadb when: ansible_hostname == 'servera' or ansible_hostname == 'serverc' [student@workstation ansible]$ ansible-navigator run when_loop.yml -m stdout --syntax-checkplaybook: /home/student/ansible/when_loop.yml[student@workstation ansible]$ ansible-navigator run when_loop.yml -m stdout 7.2.3 常用when条件语句123456789101112131415161718192021222324252627[student@workstation ansible]$ ansible all -m debug -a var=groups[student@workstation ansible]$ ansible all -m debug -a var=group_names#变量值 == '字符串'inventory_hostname == 'node1'inventory_hostname != 'node1''&quot;52:54:00:00:fa:0b&quot; in ansible_default_ipv4.macaddress'ansible_default_ipv4.address == '172.25.250.11'#变量值存在 in 第二个变量inventory_hostname in groups.dev #可以匹配组'&quot;dev&quot; in group_names' #可以匹配[student@workstation ansible]$ vim when_loop3.yml---- name: repository hosts: all tasks: - name: install the latest version of Apache ansible.builtin.yum: name: httpd state: latest when: '&quot;db&quot; in group_names'[student@workstation ansible]$ ansible-navigator run when_loop3.yml -m stdout --syntax-checkplaybook: /home/student/ansible/when_loop3.yml[student@workstation ansible]$ ansible-navigator run when_loop3.yml -m stdout#default变量查询方法#搜索引擎中搜索：filters --- Using filters to --- 搜索admin---default('admin', true) 8 实施处理程序&emsp;&emsp;handlers是处理程序的一种实现，当对一个Playbook模块改动时，通过监控发现改动，并自动执行后续处理动作 &emsp;&emsp;比如一种场景，当修改了服务配置文件时，需要对服务进行重启，可以在配置文件模块位置用notify监视是否修改后，用handlers中的处理程序如:service模块对其重启服务，达到修改文件便自动重启服务的效果 8.1 Ansible处理程序123456789101112131415161718192021222324$ vim handlers.yml---- name: hosts: servera tasks: - name: install the latest version of Apache ansible.builtin.yum: name: httpd state: latest - name: Copy using inline content ansible.builtin.copy: content: 'heihei' dest: /var/www/html/index.html notify: restart #notify字段冒号后的名称restart，指向handlers中的描述为restart的模块 - name: Start firewalld ansible.builtin.service: name: firewalld state: started handlers: #handlers是缩进和tasks对齐 - name: restart ansible.builtin.service: name: httpd state: restarted 8.2 执行中对错误的处理&emsp;&emsp;Ansible评估各任务的返回代码，从而确定任务是成功还是失败 &emsp;&emsp;通常而言，当任务失败时，ansible将立即在该主机上终止play的其余部分并且跳过所有后续任务 &emsp;&emsp;有些时候，可能希望即使在任务失败时也继续执行play 8.2.1 ignore_errors&emsp;&emsp;忽略任务失败 ignore_errors 123456789101112131415161718192021222324252627[student@workstation ansible]$ vim ignore_errors.yml---- name: test error hosts: servera tasks: - name: touch directory ansible.builtin.shell: mkdir /a/b ignore_errors: yes - name: Add the user ansible.builtin.user: name: johnd [student@workstation ansible]$ ansible-navigator run ignore_errors.yml -m stdout --syntax-check[student@workstation ansible]$ ansible-navigator run ignore_errors.yml -m stdout#也可以在任务失败时强制执行处理程序,详见教材---- name: test error force_handlers: yes tasks: - xxxx handlers: - name: haha ansible.builtin.service: xxx xxx 8.3 Ansible块和错误的处理8.3.1 block、rescue、always&emsp;&emsp;block：定义要运行的主要任务 &emsp;&emsp;rescue：定义要在block子句中定义的任务失败时运行的任务 &emsp;&emsp;always：定义始终都在独立运行的任务 123456789101112131415161718192021---- name: block hosts: all tasks: - block: - name: ansible.builtin.yum: name: http state: present rescue: - name: ansible.builtin.yum: name: httpd state: present when: inventory_hostname == 'serverb' always: - name: Start service httpd, if not started ansible.builtin.service: name: httpd state: started# Ansible官方文档：搜索：rescue https://docs.ansible.com/ansible/latest/user_guide/playbooks_blocks.html 8.3.2 block的使用123456789101112131415161718192021222324252627#未使用block时：---- name: test error hosts: all tasks: - name: touch file ansible.builtin.shell: mkdir -p /a/b when: inventory_hostname == &quot;servera&quot; - name: Add the user ansible.builtin.user: name: johnd when: inventory_hostname == &quot;servera&quot;#使用block时---- name: test error hosts: all tasks: - block: - name: touch file ansible.builtin.shell: mkdir -p /a/b - name: Add the user ansible.builtin.user: name: johnd when: inventory_hostname == &quot;servera&quot; 9 文件部署到受管主机9.1 将文件复制到主机9.1.1 文件模块&emsp;&emsp;1.在被管机上创建文件和目录 &emsp;&emsp;2.复制文件(或内容)到被管机 control –&gt; node1 &emsp;&emsp;3.从被管机复制文件到管理机 node1 –&gt; control &emsp;&emsp;4.修改文件内容 &emsp;&emsp;5.查看文件状态 &emsp;&emsp;6.修改文件属性(所有者、权限、selinux) &emsp;&emsp;7.文件同步 ansible.builtin-描述文件模块： 模块名 说明 blockinfile 插入、更新 、删除，自定义标记的多行文本块 file 设置权限、所有者、SElinux上下文及常规文件、符号连接、硬链接等 copy 远程copy，类似file，可以设置文件属性、SElinux上下文 fetch 和copy类似，相反工作方式，从远端拷贝到控制节点 lineinfile 改文件某一行时使用 stat 检测文件状态，类似linux中stat命令 synchronize 围绕rsync一个打包程序 12345#查找模块可使用命令$ ansible-doc -l | grep file$ ansible-navigator collections -m stdout | grep file$$ ansible-doc file#注意：很多模块已经不在ansible.builtin集合中了，所以需要通过ansible-navigator collections命令搜索。 1.ansible.builtin.file 12345678910111213- name: Change file ownership, group and permissions ansible.builtin.file: path: /var/www/html/index.html owner: apache group: apache mode: '0644' state: touch setype: default_t #(Choices: absent, directory, file, hard, link, touch)[Default: file]#file：修改文件内容，无该文件则不修改#touch：创建文件#touch，mkdir，cp，mv，rm，ln，chmod，chown，chcon 2.ansible.builtin.copy 123456789101112131415#复制本机文件到受管节点- name: Copy file with owner and permissions ansible.builtin.copy: src: /srv/myfiles/foo.conf dest: /var/www/html/index.html owner: foo group: foo mode: '0644' setype: httpd_sys_content_t #复制文本内容testweb至目标文件，文件不存在则创建 - name: Copy using inline content ansible.builtin.copy: content: &quot;testweb&quot; dest: /var/www/html/index.html 3.ansible.builtin.lineinfile 1234567891011121314151617181920212223[student@workstation ansible]$ ansible-doc -l | grep linelineinfile Manage lines in text files[student@workstation ansible]$ ansible-doc lineinfile[student@workstation ansible]$ vim lineinfile.yml---- name: lineinfile hosts: all tasks: - name: Ensure SELinux is set to enforcing mode ansible.builtin.lineinfile: path: /etc/selinux/config regexp: '^SELINUX=' line: SELINUX=enforcing[student@workstation ansible]$ ansible-navigator run lineinfile.yml -m stdout --syntax-checkplaybook: /home/student/ansible/lineinfile.yml[student@workstation ansible]$ ansible-navigator run lineinfile.yml -m stdout#验证:[student@workstation ansible]$ ansible servera -m shell -a 'cat /etc/selinux/config'#docs.ansible.com 搜索引擎中搜索：filters --- Using filters to --- 搜索admin---default('admin', true) 4.ansible.builtin.blockinfile 12345678- name: Insert/Update HTML surrounded by custom markers after &lt;body&gt; line ansible.builtin.blockinfile: path: /opt/index.html marker: &quot;&lt;!-- {mark} ANSIBLE MANAGED BLOCK --&gt;&quot; insertafter: &quot;&lt;body&gt;&quot; block: | &lt;h1&gt;Welcome to {{ ansible_hostname }}&lt;/h1&gt; &lt;p&gt;Last updated on {{ ansible_date_time.iso8601 }}&lt;/p&gt; 5.ansible.builtin.template 12345678910- name: Template a file to /etc/files.conf ansible.builtin.template: src: /mytemplates/foo.j2 dest: /etc/file.conf- name: Download foo.conf ansible.builtin.get_url: #该模块可以将网络上的文件，直接下载至受管节点上。 url: http://materials/hosts.j2 #源文件 dest: /opt/host.txt #目标文件位置 9.2 Jinja2模板部署自定义文件9.2.1 jinja2简介&emsp;&emsp;ansible中使用jinja2模板对文件进行部署，再用template模块同步jinja2模板文件至受管节点，该模块和copy模块作用基本一样，都是把某个文件复制到被管主机上，但是区别在于template模块可以获取变量的值和使用循环 &emsp;&emsp;&emsp;1.管理文件通常会使用一些模块，copy，file，blockinfile，lineinfile &emsp;&emsp;&emsp;2.更好的配置文件管理方式是使用jinja2语法制作模板文件来生成最终使用的配置文件 &emsp;&emsp;&emsp;3.jinja2模板文件内，可以通过多种方式编辑或构成，比如魔法变量、事实变量、普通字符、控制语句语法… &emsp;&emsp;&emsp;4.使用jinja2模板的方法是，先构建jinja2模板，再通过template模块将j2模板同步至受管节点 &emsp;&emsp;&emsp;5.构建模板文件通常名称自定义，以.j2结尾，类似shell脚本的.sh、python脚本的.Py 9.2.2 使用分隔符1234567891011121314151617181920212223241、构建jinja2模板$ vim jin.j2 #文件名用.j2结尾127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6{# host file #} #描述，为管理员做提示作用。用户不可见haha {{ ansible_hostname }}2、通过templete模块，同步模板文件至受管主机，同时收集事实变量值，将结果生成至相应文件中。$ vim temp.yml---- name: sync file hosts: servera tasks: - name: Template a file to /etc/files.conf ansible.builtin.template: src: jin.j2 dest: /etc/myhosts$ ansible-navigator run temp.yml -m stdout3、在受管节点上查看文件结果$ ansible servera -m shell -a 'cat /etc/myhosts'servera | CHANGED | rc=0 &gt;&gt;hahaservera heihei servera.lab.example.com 9.2.3 管理模板文件1234567891011121、在配置文件中定义ansible_managed功能，添加描述信息：“Ansible hahaha”[greg@control ansible]$ vim ansible.cfg[defaults]ansible_managed = Ansible hahaha变量名 = 变量值2、在jinja2模板中调用该功能#vim jinja.j2{# host file #} #{##}注释客户端生成文件是不显示{{ ansible_managed }} #{{}}描述，客户端生成文件时会显示127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 9.2.4 控制结构-使用循环123456789101112131415161718-shellfor user in `ls /`;do echo $userdone-jinja2{% for user in users %} #user变量替换为users变量中的所有值，一行一个值。{{ user }}{% endfor %}#示例$ vim jinja2.j2{% for host in groups.all %} #使用for 或if 时控制结构使用{% %}{{ host }}{% endfor %}#查看语法帮助:官网或关键词搜索示例[student@workstation ansible]$ grep -r '{%' /etc 1.文件的生成方法 12345678$ vim debug.yml---- name: hosts: all tasks: - ansible.builtin.debug: var: hostvars$ ansible-navigator run debug.yml -m stdout &gt; 1.txt 2.生成/etc/hosts 1234567891011121314151617181920212223242526# 方法1：$ vim /home/student/ansible/temp.yml127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6{% for host in groups.all %}{{ hostvars[host].ansible_default_ipv4.address }} {{ hostvars[host].ansible_nodename }} {{ hostvars[host].ansible_hostname }} {% endfor %}#方法2：vim /home/student/ansible/temp.yml---- name: sync file hosts: all tasks: - name: Template a file to /etc/files.conf ansible.builtin.template: src: hosts.j2 dest: /etc/myhosts when: inventory_hostname in groups.dev #匹配dev组 $ ansible-navigator run temp.yml -m stdout$ ansible servera -m shell -a 'cat /etc/myhosts'# 以json格式查看：{{ hostvars[i]['ansible_facts'] | to_nice_json }} 10 管理大项目10.1 引用清单主机10.1.1 使用主机模式的方式12345678#使用主机模式的常见方式：1.[ad-hoc:]ansible dev -m shell 2.[playbook:]- name: hosts: dev 10.1.2 通配符匹配多个主机12345678# 清单里使用通配符匹配多个主机[START:END]192.168.[0:15].[0:255] 表示 192.168.0.0-192.168.15.255server[a:c].example.com 表示 a-cserver[01:15].example.com 表示 server01.example.com-server15ipv6也可以通过[a:f]这种方式all： 所有主机ungrouped ： 不属于任何一个组的所有主机 10.1.3 主机模式其他方式1234567891011121314#使用特殊字符时，必须添加单引号，否则不生效hosts： '*' 和all相同 hosts：'*.example.com'hosts：'datacenter*'#列表形式hosts：servera,serverb hosts：webserver,serverchosts：'devops,server*'#冒号：取代逗号hosts：lab,&amp;datacenter 匹配lab组同时也属于datacenter组，顺序无所谓&amp;符号时同时也属于的意思hosts：datacenter,!test2.example.com 表示datacenter组，但不包括test2.。。这个主机hosts：all,!datacenter1 所有组，但不包含datacenter1组 10.2 包含和导入文件10.2.1 管理大型Playbook123456# 如果playbook很长很复杂，可以拆分成较小的文件便于管理，以模块化管理# 可以将多个不同功能的play，组合成一个主要的playbook，将文件中的任务列表插入play，这样可将这种模块化的play应用到不同场景playbook- httpd- php- mysql 10.2.2 导入Playbook123456789101112131415161718192021222324252627282930313233343536371.将两个playbook加入到主playbook$ vim one.yml ---- name: play1 hosts: node1 tasks: - name: Install the latest version of Apache ansible.builtin.yum: name: httpd state: latest $ vim two.yml ---- name: play2 hosts: node1 tasks: - name: Make sure a service unit is running ansible.builtin.systemd: state: started name: httpd enabled: yes 2.在主playbook中和其他play交替使用$ vim main.yml ---- name: four hosts: node1 tasks: - ansible.builtin.debug: msg: haha - name: one # 因为加载的是playbook所以需要顶头写无缩进 import_playbook: one.yml- name: two import_playbook: two.yml 10.2.3 包含与导入&emsp;&emsp;Ansible可以支持两种方法将文件放入playbook中：&emsp;&emsp;&emsp;包含：内容是一个动态操作。在playbook运行期间，Ansible会在内容到达时处理所包含的内容&emsp;&emsp;&emsp;导入：内容是一个静态操作。在运行开始之前，Ansible在最初解析playbook时预处理导入的内容 1.包含 12345671.使用include_tasks功能时，包含时设置的when等条件语句将确定任务是否包含在play中2.如果运行ansible-playbook --list-tasks以列出playbook中的任务，则不会显示已包含任务文件中的任务。将显示包含任务文件的任 务。相比之下，import_tasks功能不会列出导入任务文件的任务，而列出已导入任务文件中的各个任务 #（[greg@control ansible]$ ansible-playbook playbook.yml --start-at-task webserver）3.不能使用ansible-playbook --start-at-task从已包含任务文件中的任务开始执行playbook #（[greg@control ansible]$ ansible-playbook playbook.yml --start-at-task webinstall）4.不能使用notify语句触发已包含任务文件中的处理程序名称 可以在包含整个任务文件的主playbook中触发处理程序，在这种情况下，已包含文件中的所有任务都将运行 2.导入 1231.使用import_tasks功能时，导入时设置的when等条件语句将应用于导入的每个任务2.无法将循环用于import_tasks功能3.如果使用变量来指定要导入的文件的名称，则将无法使用主机或组清单变量 3.使用示例 12345678910111213141516171819202122232425第一个tasks任务[greg@bastion ansible]$ mkdir tasks #tasks目录是自定义的，创建的目的只是方便存储管理tasks任务文件。[greg@bastion ansible]$ vim tasks/apache.yml #tasks任务文件，文件中没有主机模式---- name: ansible.builtin.yum: name: httpd state: latest第二个tasks任务[greg@bastion ansible]$ vim tasks/service.yml ---- name: ansible.builtin.service: name: httpd enabled: yes state: started 包含和导入的方式：[greg@bastion ansible]$ vim main.yml #此处main.yml是一个playbook，有主机模式---- name: hosts: node1 tasks: - include_tasks: tasks/apache.yml - import_tasks: tasks/service.yml 11 角色和collections简化Playbook11.1 描述角色结构123# Ansible中的角色是结构化的目录组成，可以根据业务需要创建不同的角色，apache的、mysql的等# 角色的优势是类似将playbook分割成更小的模块，进行模块化管理，简化playbook# 除了自行编写、使用、重用和共享角色外，也可以从其他来源获取角色以及使用分发包(如Ansible内容集合)查找角色 11.2 创建角色1234567# 在playbook的项目目录中创建一个角色，并将其作为playbook中某个play的一部分来运行1.yum install -y httpd yum模块2.index.html, fcontext copy模块3.httpd service service模块4.firewalld.service service模块5.firewalld permit apache firewalld模块# 使用web角色一键部署以上服务 &emsp;&emsp;创建和使用角色分四步进行： &emsp;&emsp;&emsp;1.创建角色目录存储路径 &emsp;&emsp;&emsp;2.创建角色 &emsp;&emsp;&emsp;3.定义角色内容 &emsp;&emsp;&emsp;4.在playbook中使用角色 11.2.1 角色目录存储路径1234567891011121314151617181920212223242526# 配置ansible.cfg文件中的roles-path,默认ansible会在roles子目录中查找角色# 可以将自己的角色安装在~/ansible/roles子目录中1.默认路径：[student@workstation ansible]$ ansible-galaxy --help[student@workstation ansible]$ ansible-galaxy role --help #查看角色的子命令帮助[student@workstation ansible]$ ansible-galaxy role list #新版本查看角色指令[student@workstation ansible]$ ansible-galaxy list #旧版本查看角色指令# /usr/share/ansible/roles -- 系统角色# /etc/ansible/roles -- 全局角色路径[WARNING]: - the configured path /home/student/.ansible/roles does not exist. --默认该目录不存在，需要的话可以根据需求创建`如果ansible无法找到该位置角色，会按照ansible.cfg中roles_path指定的目录中查找`2.创建角色目录存储路径 使用自定义工作目录时，创建自定义roles(角色)目录，并使用ansible.cfg中roles_path=字段指定角色路径[student@workstation ansible]$ pwd #进入自己的工作目录/home/student/ansible[student@control ansible]$ mkdir roles #创建存放角色的目录[student@control ansible]$ vim ansible.cfg roles_path=/home/student/ansible/roles #指定角色路径[student@workstation ansible]$ ansible-galaxy list #要在工作目录中使用查看命令，调用当前工作目录配置文件中的角色路径# /home/student/ansible/roles #查询结果应和配置文件的roles_path字段一致 11.2.2 创建角色12345678910111213141516171819202122232425262728[student@workstation ansible]$ ansible-galaxy role init roles/apache #创建角色- Role roles/apache was created successfully[student@workstation ansible]$ ansible-galaxy role list #列出所有角色# /home/student/ansible/roles- apache, (unknown version)[student@workstation ansible]$ tree roles/apache/roles/apache├── defaults #角色默认变量│ └── main.yml├── files #引用的静态文件，可以是一些文件、网页模板等├── handlers #处理程序，通常通过模块完成的│ └── main.yml├── meta #作者，许可、兼容性│ └── main.yml├── README.md├── tasks #任务，任务的组成就是模块应用，也是角色的主要功能│ └── main.yml├── templates #模板文件，通常使用jinja2模板├── tests #测试│ ├── inventory│ └── test.yml└── vars #角色变量 └── main.yml8 directories, 8 files#用不到的目录可以删除，如defaults、vars、tests 11.2.3 定义角色内容12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[student@control ansible]$ tree roles/apache (可选，方便查看路径信息)*[student@control ansible]$ vim roles/apache/tasks/main.yml #在tasks目录中完善角色内容---- name: install apache #编写角色时，任务中只有模块任务，没有PLAY ansible.builtin.yum: name: httpd state: latest- name: Start service httpd ansible.builtin.service: name: httpd state: started enabled: yes- name: create web page ansible.builtin.template: src: jin2.j2 #jin2.j2需要手动创建 dest: /var/www/html/index.html notify: - restart #需要时将调用handlers处理程序，需要手动创建，restart通知中的处理程序名要和handlers/main.yml中处理程序描述一致- name: Start service firewalld ansible.builtin.service: name: firewalld state: started enabled: yes - name: permit apache ansible.posix.firewalld: service: http permanent: yes state: enabled immediate: yes #ansible-navigator collections可以查看posix集合 #ansible-navigator doc ansible.posix.firewalld -m stdout #以上25-30行客替换为如下:- name: permit apache ansible.builtin.shell: firewall-cmd --permanent --add-service=http - name: permit apache ansible.builtin.shell: firewall-cmd --reload [student@control ansible]$ vim roles/apache/templates/jin2.j2 #编写模板文件ipadd={{ ansible_default_ipv4.address }} hostname={{ ansible_hostname }}[student@control ansible]$ vim roles/apache/handlers/main.yml #编写处理程序---- name: restart ansible.builtin.service: name: httpd state: restarted 11.2.4 playbook中使用角色12345678910[student@bastion ansible]$ vim roles.yml---- name: hosts: servera roles: #roles字段用来调用角色 - apache #被调用角色的名称 - xxx[student@bastion ansible]$ ansible-navigator run -m stdout roles.yml [student@control ansible]$ curl serveraipadd=172.25.250.10 hostname=servera 11.3 外部内容源部署角色&emsp;&emsp;从Git存储库或AnsibleGalaxy等外部资源中选择和检索角色，并在playbook中使用 &emsp;&emsp;https://galaxy.ansible.com 11.3.1 外部内容来源1234567891011121314#角色有多种获取方式：1.本地tar包安装 2.通过网络地址安装 3.通过文件同时安装网络中多个地址角色roles/requirements.yml- src： #角色网址 varsion：#角色版本 name：#安装在本地的角色名- src： name：ansible-galaxy role install -r roles/requirements.yml -p roles-r 指定文件路径-p 指定角色安装路径roles/requirements.yml 角色安装信息，包括地址，角色名称等 11.3.2 角色部署1234567891011121314151617181920212223#示例：1.将phpinfo.tar,haproxy.tar上传到Linux，kiosk家目录下，用kiosk登录2.将两个角色包拷贝到foundation0的rhel9.0目录中$ ssh root@localhost cp /home/kiosk/{phpinfo.tar,haproxy.tar} /content/courses/rh294/rhel9.0/materials3.打开foundation0的浏览器输入http://172.25.254.254/materials/就可以看到两个软件包[student@workstation ansible]$ mkdir roles[student@workstation ansible]$ $ vim ansible.cfg[defaults]roles_path=/home/student/ansible/roles[student@workstation ansible]$ vim roles/requirements.yml---- src: http://172.25.254.254/materials/haproxy.tar name: balancer- src: http://172.25.254.254/materials/phpinfo.tar name: phpinfo[student@workstation ansible]$ ansible-galaxy install -r roles/requirements.yml [student@workstation ansible]$ ansible-galaxy list# /home/student/ansible/roles- apache, (unknown version)- balancer, (unknown version)- phpinfo, (unknown version) 11.4 内容集合获取角色和模块&emsp;&emsp;从Ansible内容集合中获取一组相关角色、补充模块和其他内容，并在playbook中使用 11.4.1 Ansible内容集合&emsp;&emsp;1.随着模块数量增加，管理困难。模块需要唯一名称，并保持更新 &emsp;&emsp;2.借助Ansible内容集合，Ansible代码可以与模块和插件分开更新 &emsp;&emsp;3.Ansible 内容集合可提供一组在playbook中使用的相关模块、角色和其他插件，便于供应商和开发人员按照自己的节奏维护和分发集合，而不受Ansible版本的影响 11.4.2 Ansible内容集合的命名空间&emsp;&emsp;1.命名空间是集合名称的第一部分 &emsp;&emsp;2.由Ansible社区维护的所有集合可能会放入community命名空间中名称类似于： &emsp;&emsp;&emsp;community.crypto &emsp;&emsp;&emsp;community.postgresql &emsp;&emsp;&emsp;community.rabbitmq &emsp;&emsp;3.红帽直接维护和支持的Ansible内容集合可能使用redhat命名空间有：&emsp;&emsp;&emsp;redhat.rhv&emsp;&emsp;&emsp;redhat.satellite&emsp;&emsp;&emsp;redhat.insights等名称 11.4.3 Ansible内容集合的来源&emsp;&emsp;无论是将ansible-navigator 用于最小自动化执行环境，还是在裸机Ansible Core上使用ansible-playbook，始终至少有一个可用Ansible内容集合:ansible.builtin &emsp;&emsp;自动化执行环境可能还内置了其他自动化执行环境，例如，红帽Ansible 自动化平台2.2使用的默认执行环境ee-supported-rhel8 &emsp;&emsp;如果需要其他Ansible内容集合，可以将其添加到Ansible项目的collections子目录中，可以从多个来源获取Ansible内容集合： &emsp;&emsp;&emsp;1.自动化中心 &emsp;&emsp;&emsp;2.私有自动化中心 &emsp;&emsp;&emsp;3.Ansible Galaxy &emsp;&emsp;&emsp;4.第三方Git存储库或存档文件 11.4.4 安装Ansible内容集合1234567#配置文件ansible.cfg中设置了collections_paths选项来指定集合的默认路径：~/.ansible/collections;/usr/share/ansible/collections #如果消除这两个目录的指定，则ansible-navigator无法在这些目录的其版本中找到自动化执行环境内提供的Ansible内容集合$ ansible-galaxy collection install 集合 -p collections-p collections 选项会将该集合安装到本地collections子目录中或者不适用-p，而在ansible.cfg文件中collections_paths选项来指定集合的默认路径#集合的来源可以是：本地、互联网、git仓库 1.使用要求文件安装Ansible内容集合 12345678910111213141516171819202122232425# 1.将三个集合包上传到Linux，kiosk家目录下，用kiosk登录# 2.将三个集合包拷贝到foundation0的rhel9.0目录中 $ ssh root@localhost cp /home/kiosk/{community-general-5.5.0.tar.gz,redhat-insights-1.0.7.tar.gz,redhat-rhel_system_roles-1.19.3.tar.gz} /content/courses/rh294/rhel9.0/materials# 3.打开foundation0的浏览器输入http://172.25.254.254/materials/就可以看到两个软件包【workstation】1.创建存储集合的位置$ mkdir /home/student/ansible/mycollections $ vim ansible.cfg[defaults]... #集合默认路径不删除，额外添加当前用户工作目录中集合路径collections_path=/home/student/ansible/mycollection:~/.ansible/collections:/usr/share/ansible/collections 2.部署requirements.yml文件$ vim /home/student/ansible/mycollections/requirements.yml---collections:- name: http://172.25.254.254/materials/community-general-5.5.0.tar.gz- name: http://172.25.254.254/materials/redhat-insights-1.0.7.tar.gz- name: http://172.25.254.254/materials/redhat-rhel_system_roles-1.19.3.tar.gz3.安装集合$ ansible-galaxy collection install -r requirements.yml -p /home/student/ansible/collections/mycollections$ ansible-galaxy collection list 11.5 利用系统角色重用内容&emsp;&emsp;编写利用红帽帽企业Linux的系统角色执行标准操作的 playbook 11.5.1 内容集合安装系统角色12345678910111213#内容集合 redhat.rhel_system_roles$ mkdir /home/student/ansible/collections$ vim /home/student/ansible/collections/requirements.yml---collectionsname: redhat.rhel system roles$ ansible-galaxy collection install -p collections/ -r home/student/ansible/collections/requirements.yml$ sudo find ./mycollection/ -name selinux-playbook.yml #不安装角色包可以使用该条命令搜索$ find mycollection/ -name '*.yml' | grep selinux$ cp /usr/share/ansible/collections/ansible_collections/redhat/rhel_system_roles/docs/selinux/selinux-playbook.yml /home/student/ansible/selinux.yml 11.5.2 rpm包安装系统角色1.系统角色使用流程 123451.通过软件安装获得系统角色（rhel-system-roles.noarch）2.定义系统角色路径（存放角色的位置），并将其记录配置文件cd /;ansible-galaxy list , vim ~/ansible/ansible.cfg/;roles_path=xxxx:xxxx3.使用系统角色，将系统角色添加至playbook，并修改内容4.应用 2.安装系统角色 12345678910111213141516171819202122232425262728293031323334353637383940414243441.安装系统帮助我们定义了一些角色，有不同的功能，需要通过安装软件包。[greg@control ansible]$ cd /[greg@control /]$ sudo yum search roles[greg@control /]$ sudo yum install -y rhel-system-roles.noarch [greg@control /]$ ansible-galaxy list #在根目录下查看角色路径，该路径为默认系统角色路径# /usr/share/ansible/roles #系统角色路径- linux-system-roles.kdump, (unknown version)....- rhel-system-roles.timesync, (unknown version)# /etc/ansible/roles [WARNING]: - the configured path /home/greg/.ansible/roles does not exist. 2.定义角色路径[greg@control /]$ cd ~/ansible/ #在ansible工作目录中查看时属于greg用户定制的角色路径[greg@control ansible]$ vim ansible.cfgroles_path = /home/greg/ansible/roles:/usr/share/ansible/roles #:冒号分割，并添加系统角色路径[greg@control ansible]$ ansible-galaxy list# /home/greg/ansible/roles #之前定义好的自定义角色路径- apache, (unknown version)# /usr/share/ansible/roles #系统角色路径- linux-system-roles.kdump, (unknown version)......- rhel-system-roles.timesync, (unknown version)3.使用系统角色，将系统角色添加至playbook并应用[greg@control ansible]$ rpm -ql rhel-system-roles.noarch | grep timesync #看README.md[greg@control ansible]$ cp /usr/share/doc/rhel-system-roles/timesync/example-timesync-playbook.yml /home/greg/ansible/timesync.yml #找例子，并复制到工作目录中[greg@control ansible]$ vim timesync.yml #编辑角色---- hosts: all vars: timesync_ntp_servers: - hostname: 172.25.254.254 iburst: yes roles: - rhel-system-roles.timesync4.使用系统角色[greg@control ansible]$ ansible-playbook timesync.yml查询验证ansible all -a 'chronyc sources -v' 11.5.3 通过内容集合安装使用系统角色123456789101112131415161718192021222324252627# 1.timesync$ cp /usr/share/ansible/collections/ansible_collections/redhat/rhel_system_roles/docs/timesync/multiple-ntp-servers.yml timesync.yml$ vim timesync.yml- hosts: all vars: timesync_ntp_servers: - hostname: 172.25.254.254 iburst: yes roles: - redhat.rhel_system_roles.timesync$ ansible-navigator run timesync.yml -m stdout$ ansible all -m shell -a 'chronyc sources -v '# 2.selinux$ cp /usr/share/ansible/collections/ansible_collections/redhat/rhel_system_roles/docs/selinux/selinux-playbook.yml selinux.yml$ vim selinux.yml---- hosts: all vars: selinux_policy: targeted selinux_state: permissive roles: - redhat.rhel_system_roles.selinux$ ansible-navigator run selinux.yml -m stdout# 验证$ ansible all -m shell -a 'grep ^SELINUX= /etc/selinux/config' 11.6 通过文件安装角色123456789101112131415161718Ansible官网：https://docs.ansible.com/ansible/2.9/galaxy/user_guide.html#installing-roles-from-galaxyhttps://galaxy.ansible.com/nginxinc/nginx_core #该网站可以查看角色包信息#实验需要在模拟考试环境下做，普通环境没有角色包[greg@control ansible]$ vim roles/requirements.yml #制作角色部署文件---- src: http://materials/phpinfo.tar #角色在网络中的路径 name: phpinfo #指定安装在系统中的角色名称- src: http://materials/haproxy.tar name: balancer [greg@control ansible]$ ansible-galaxy install --help[greg@control ansible]$ ansible-galaxy install -r roles/requirements.yml #-r 指定角色文件[greg@control ansible]$ ansible-galaxy list #检测# /home/greg/ansible/roles- phpinfo, (unknown version) #查看结果- balancer, (unknown version) 12 对Ansible进行故障排除12.1 对playbook进行故障排除12.1.1 调试Playbook12345-rhel8ansible-playbook debug.yml -v #显示详细信息-v -vv -vvv -vvvv-rhel9ansible-navigator run playbook.yml -m stdout -v 12.1.2 Debug12345678910111213141516171819202122232425262728#通过debug模块打印收集到的变量信息，帮助管理员了解模块执行过程及结果---- hosts: serverb tasks: - name: install the latest version of Apache ansible.builtin.yum: name: vsftpd state: latest register: web_install #注册变量收集模块的执行信息 - ansible.builtin.debug: var: web_install #debug模块可以帮助打印出执行信息#可以一次性将hostvars收集到的所有魔法及事实变量打印出来，并可以记录到文件中[greg@control ansible]$ vim debug.yml---- name: test hosts: node1 tasks: - ansible.builtin.debug: var: hostvars[greg@control ansible]$ ansible-navigator run -m stdout debug.yml &gt; 2.txt 关闭事实收集如果要关闭收集，可以编辑配置文件gathering = explicit或者在playbook里gather_facts: yes/no true/false 12.1.3 检查Playbook的错误1234567-RHEL8$ ansible-playbook playbook.yml --syntax-checkplaybook: playbook.yml-RHEL9greg@control ansible]$ ansible-navigator run -m stdout debug.yml --syntax-checkplaybook: /home/greg/ansible/debug.yml 12.1.4 检查Playbook的问题123456#常见语法问题name： 冒号后面要空格，内容随意hosts： all 指定的主机不在清单中，报错无parttensyntax-errors：注意格式缩进，2格缩进变量设置错误，或调用时错误（变量名写错，变量开头要加双引号） when条件： （逻辑判断思路错误，或书写错误） 12.1.5 检查Playbook工件和日志文件1234567891011121314151617# ansible-navigator可以生成playbook工件，以JSON格式存储与playbook的运行相关的信息# 可以将与playbook运行相关的信息记录到系统上您可写入位置处的文本文件中# RHEL9中的ansible自动开启日志功能，可通过配置文件设置及手工设置# 配置文件$ vim ~/.ansible-navigator.yml---ansible-navigator: execution-environment: image: utility.lab.example.com/ee-supported-rhel8:latest pull: policy: missing playbook-artifact: enable: false # false/true 关闭/打开# 手工指定--pae --playbook-artifact-enable 12.1.6 输出记录到文本文件1234-RHEL8&amp;91.Ansible提供了一个内置日志基础架构，可通过ansible.cfg配置文件[defaults]部分中的log_path参数或$ANSIBLE_LOG_PATH环境变 量进行配置。在进行上面一项或两项配置后，Ansible会以文本形式将ansible-navigator命令的输出存储到所配置的日志文件中。此机制 也适用于ansible-playbook等早期的工具2.如果将Ansible配置为将日志文件写入/var/log，红帽建议您配置logrotate来管理文件 12.2 对Ansible受管主机进行故障排除12.2.1 Ansible运行临时命令123456789# ad-hoc命令可以用于简单部署场景，比如对一组目标部署一个任务，或检索受管节点的运行情况、状态等ansible all -m ping# RHEL9中默认不允许root用户登录，需要修改配置文件允许root登录。所建议使用普通用户作为远程管理用户# 解决方法,服务器端：$ vim /etc/ssh/sshd_config# PermitRootLogin prohibit-passwordPermitRootLogin yes$ systemctl reload sshd 12.2.2 手动控制执行1234567891011$ ansible-playbook playbook.yml --step$ ansible-navigator run playbook.yml -m stdout --stepPLAY [PLAY1] ********************************************************************************************Perform task: TASK: Gathering Facts (N)o/(y)es/(c)ontinue: n#手动输入n y c来控制playbook中执行的步骤1.n 不执行该步骤2.y 执行该步骤3.c 继续自动执行到结束# 通过该方法可以让有问题的步骤执行，而无关紧要的步骤可以跳过不执行。达到排错的目的 12.2.3 从指定步骤执行任务123[greg@control ansible]$ ansible-playbook playbook.yml --start-at-task='Start service httpd'--start-at-task #指定具体执行步骤，等号后面指定模块的描述，如果描述内容中有空格，建议用单或双引号引起来，表示为一个参数 12.2.4 烟雾测试12345678910111213141516171819202122232425262728293031# 烟雾测试：在管理节点执行剧本，显示剧本的真实执行结果，但是不在受管节点上部署$ ansible-playbook playbook.yml --help | grep \\\\-C$ ansible-navigator run playbook.yml -m stdout -C[greg@control ansible]$ vim playbook.yml---- name: PLAY1 hosts: node2 tasks: - name: install apache ansible.builtin.yum: name: httpd state: latest - name: Copy using inline content ansible.builtin.copy: content: 'test web page' dest: /var/www/html/index.html - name: Start service httpd ansible.builtin.service: name: httpd state: started - name: Start service firewalld ansible.builtin.service: name: firewalld state: started - name: permit apache ansible.posix.firewalld: service: http permanent: yes state: enabled immediate: yes[greg@control ansible]$ ansible-playbook playbook.yml -C 12.2.5 发送脚本解决问题12345678910111213141516$ vim test.sh #自定义脚本名及内容#!/bin/bashdate$ vim debug.yml #剧本名自定义---- name: hosts: node1 tasks: - name: Run a script ansible.builtin.script: /home/greg/ansible/test.sh #使用script模块，加脚本文件路径 register: haha #register收集上面模块执行结果至 haha（自定义名称）的这个变量中 - ansible.builtin.debug: var: haha #通过debug模块将haha变量的值打印出来。$ ansible-playbook debug.yml `如果不想写register，可以执行剧本时添加-v 类似：ansible-playbook debug.yml -v` 13 自动执行Linux管理任务13.1 管理软件和订阅13.1.1 优化多软件包安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849ansible.builtin.---- name: hosts: all tasks: - name: install the latest version of Apache ansible.builtin.yum: name: httpd #httpd state: latest - name: install the latest version of Apache ansible.builtin.yum: name: php #php state: latest---------------------------------------------- name: install the latest version of Apache ansible.builtin.yum: name: &quot;{{ item }}&quot; state: latest loop: #用loop方式简化playbook - httpd - php yum install -y httpdyum install -y php---------------------------------------------- name: install the latest version of Apache ansible.builtin.yum: name: - httpd - php state: latestyum install -y httpd php--------------------------------------------- - name: ensure a list of packages installed ansible.builtin.yum: name: &quot;{{ packages }}&quot; vars: packages: - httpd - httpd-tools--------------------------------------------- # 等同于：yum install -y httpd php或 loop的这种方式，系统会执行两次独立事务，对每个事务应用一个软件包# yum模块： state： absent删除, installed, present确保安装 latest升级到最新版本 latest 等同 yum update name： '*' 代表所有软件包 name: &quot;@RPM Development tools&quot; ansible命令里面安装包组要加@ 13.1.2 收集已安装软件包的事实12345678910111213141516---- name: hosts: node1 tasks: - name: Gather the package facts ansible.builtin.package_facts: manager: auto #- name: Print the package facts # ansible.builtin.debug: # var: ansible_facts.packages - name: Check whether a package called foobar is installed ansible.builtin.debug: msg: &quot;{{ ansible_facts.packages.zip.0.version }}&quot; 13.1.3 管理软件包的其他模块123456789- name: Install httpd on RHEL 8 and 9 ansible.builtin.dnf name: httpd state: present- name: Install httpd on RHEL 7 and earlier ansible.builtin.yum: name:httpd state:present 13.1.4 RPM软件包存储库123456789101112131415161718192021222324252627282930313233343536373839$ ansible-doc -l | grep yum---- name: hosts: node1 tasks: - name: Add multiple repositories into the same file (1/2) ansible.builtin.yum_repository: name: EX294_BASE #file字段不存在时，默认用name字段代替源文件名称 description: EX294 base software file: rhel #file选项作用是定义yum源文件名称，无该字段时会用name字段后的值代替文件名 例如：rhel.repo baseurl: http://content/rhel8.4/x86_64/dvd/BaseOS gpgcheck: yes gpgkey: http://content/rhel8.4/x86_64/dvd/RPM-GPG-KEY-redhat-release enabled: yes - name: Add multiple repositories into the same file (1/2) ansible.builtin.yum_repository: name: EX294_STREAM description: EX294 stream software file: rhel #多个模块使用同一个file字段的名称时，会将源地址写入到同一个文件中。 baseurl: http://content/rhel8.4/x86_64/dvd/AppStream gpgcheck: yes gpgkey: http://content/rhel8.4/x86_64/dvd/RPM-GPG-KEY-redhat-release enabled: yes #enabled默认值为yes，考试时要写该选项。ansible模块选项和vim配置文件内容对比 vim： ansible： rhel.repo file [rhel] * name name=rhel * description baseurl= * baseurl gpgcheck= * gpgcheck gpgeky= * gpgkey enabled= * enabled #测试方法：$ ansible all -a 'yum repolist all'# 受管节点检测 $ cat rhel.repo 1234$ ansible-doc rpm_key - ansible.builtin.rpm_key: state: present key: http://apt.sw.be/RPM-GPG-KEY.dag.txt 13.2 管理用户和身份验证13.2.1 user模块12345678910111213141516171819202122232425262728293031323334353637#在servera上创建用户tom 附加组为group1，并设置密码为password tom: tom123用sha512方式哈希，uid =2000#user这个模块类似这些功能： useradd userdel usermod$ cat group.yml - name: hosts: node1 vars: - passwordtom: tom123 tasks: - name: create group ansible.builtin.group: name: &quot;{{ item }}&quot; loop: - group1 - group2 - name: Add the ansible.builtin.user: name: tom #useradd tom comment: student #useradd -c student uid: 2000 #useradd -u 2000 password_expire_max: 10 #passwd -e 10 group: group1 #useradd -g group1 groups: group1,group2 #-G group1,group2 shell: /bin/bash #-s /bin/bash password: &quot;{{ passwordtom | password_hash('sha512') }}&quot; passwd tom #passwordtom该密码位置如果是字符串用单引号引起来，如果是变量则不需要单引号 #验证用户密码过期时间chage -l tomPassword expires : Jun 11, 2024#验证用户密码是否正确，可以从node2登录node1，测试node1上的tom用户ssh tom@node1Ansible网页搜索：password_hash--Using filter--网页中查找password_hash，查看密码哈希方式，一定注意是512Ansible官网： https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html 模块中：append: yes 如果想额外添加附加群组，此选项需要yes(usermod -a -G grouptest u1) 13.2.2 group模块1234567891011121314[greg@bastion ansible]$ cat group.yml ---- name: hosts: dev tasks: - name: Ensure group &quot;grouptest&quot; exists ansible.builtin.group: gid: 10000 name: grouptest state: present | absent #创建|删除 二选一#等同于：groupadd grouptest#等同于：groupadd -g 10000 grouptest#groupdel 13.3 管理启动过程和调度进程13.3.1 at12345678910---- name: hosts: dev tasks: - name: Schedule a command to execute in 20 minutes as root ansible.posix.at: command: ls -d / &gt;/dev/null count: 20 units: minutes# 默认是创建一个任务，给root，删除的 话使用选项state：absent 13.3.2 cron12345678910111213141516[greg@bastion ansible]$ ansible-doc cron[greg@bastion ansible]$ cat cron.yml ---- name: hosts: all tasks: - name: Ensure a job that runs at 2 and 5 exists. ansible.builtin.cron: name: &quot;check dirs&quot; #描述 minute: &quot;*/2&quot; #分钟 hour: &quot;2,5&quot; #小时 day: 1-10 #天 user: harry job: &quot;ls -alh &gt; /dev/null&quot; [greg@bastion ansible]$ ansible dev -m shell -a 'crontab -u harry -l' 13.3.3 管理服务1234567891011121314---- name: hosts: dev tasks: - name: install the latest version of Apache ansible.builtin.yum: name: httpd state: latest - name: Start service httpd, if not started ansible.builtin.service: name: httpd state: started enabled: yes #开机自启动 13.3.4 systemd1234567891011121314151617181920212223242526272829[greg@bastion ansible]$ cat systemd.yml ---- name: hosts: dev tasks: - name: install the latest version of Apache ansible.builtin.yum: name: httpd state: latest - name: Make sure a service is running ansible.builtin.systemd: name: httpd state: started enabled: yes 测试命令：ansible dev -m shell -a 'systemctl status httpd'ansible dev -m shell -a 'systemctl is-enabled httpd'# 1.reboot- name: Unconditionally reboot the machine with all defaults reboot: # 2.command &amp; shell 支持更多的特殊字符$ ansible servera -m command -a 'useradd user1'$ ansible servera -a 'useradd user2'$ ansible servera -a 'echo 123456 | passwd --stdin user1'$ ansible servera -m shell -a 'echo 123456 | passwd --stdin user1' 13.4 管理存储 模块名 linux指令 Linux实施命令 community.general.parted parted parted /dev/vdb mkpart part1 2048s 1G community.general.lvg vgcreate vgcreate -s 16M vg100 /dev/vdb{1..2} community.general.lvol lvcreate lvcreate -L 100M -n lv100 vg100 community.general.filesystem mkfs mkfs.ext4 /dev/vg100/lv100 ansible.posix.mount /etc/fstab echo “/dev/vg100/lv100 ext4 /mnt/data defaults 0 0” &gt; /etc/fstab 13.4.1 模块管理存储1.纯分区无lvm逻辑卷 1234567891011121314151617181920212223242526272829303132333435363738394041424344451-1 分区 community.general.parted #先确保community.general集合已经安装，可通过ansibl-galaxy collection list来查看# 创建两个分区每个500M---- name: parted hosts: node2 tasks: - name: part 1 2048s-500M community.general.parted: device: /dev/vdc number: 1 state: present part_end: 500MiB - name: part 2 500M-1000M community.general.parted: device: /dev/vdc number: 2 state: present part_start: 500MiB part_end: 1GiB1-2 格式化 community.general.filesystem#两个分区都格式化为ext4文件系统，如果选择不同文件系统，可以分成两个模块。 - name: Create a ext4 community.general.filesystem: fstype: ext4 dev: &quot;{{ item }}&quot; loop: - /dev/vdc1 - /dev/vdc21-3 挂载 ansible.posix.mount#自动创建挂载点/mnt/part1,再将/dev/vdc1写入/etc/fstab文件最后一行，并挂载。 - name: Mount DVD read-only ansible.posix.mount: path: /mnt/part1 #挂载点 src: /dev/vdc1 #设备 fstype: ext4 #文件系统类型 state: mounted #参考下面解释 -挂载选项解释：# absent: 卸载并删除/etc/fstab内容 # unmounted： 卸载不删除/etc/fstab内容# present： 将挂在信息写入/etc/fstab，不挂载# mounted： 将挂在信息写入/etc/fstab，并创建挂载点及挂载# remounted： 重新挂载 2.有lvm逻辑卷 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687882.分区-lvm（pv,vg,lv）-格式化-挂载2-1 分区 community.general.parted #分两个区每个大约500M，/dev/vdb---- name: lv hosts: node2 tasks: - name: part 1 2048s-500M community.general.parted: device: /dev/vdb number: 1 state: present part_end: 500MiB - name: part 2 500M-1000M community.general.parted: device: /dev/vdb number: 2 state: present part_start: 500MiB2-2 pv+vg community.general.vg#两个分区都加入vg，名称vg100#ansible-navigator collections -m stdout | grep vg$ 查看帮助 - name: Create a volume group vg100 community.general.lvg: vg: vg100 pvs: /dev/vdb1,/dev/vdb2 pesize: 322-3 lv community.general.lvol#创建一个lv名为lv100，大小800M - name: Create a logical volume lv100 size 800M community.general.lvol: vg: vg100 lv: lv100 size: 8002-4 格式化 community.general.filesystem#格式化为xfs文件系统 - name: Create a xfs community.general.filesystem: fstype: xfs dev: /dev/vg100/lv1002-5 挂载 ansible.posix.mount#lv挂载到/mnt/data，并设置为开机自启动。 - name: /mnt/data ansible.posix.mount: path: /mnt/data src: /dev/vg100/lv100 fstype: xfs state: mounted #整体配置---- name: lv hosts: node2 tasks: - name: part 1 2048s-500M community.general.parted: device: /dev/vdb number: 1 state: present part_end: 500MiB - name: part 2 500M-1000M community.general.parted: device: /dev/vdb number: 2 state: present part_start: 500MiB - name: Create a volume group vg100 community.general.lvg: vg: vg100 pvs: /dev/vdb1,/dev/vdb2 pesize: 32 - name: Create a logical volume lv100 size 800M community.general.lvol: vg: vg100 lv: lv100 size: 800 - name: Create a xfs community.general.filesystem: fstype: xfs dev: /dev/vg100/lv100 - name: /mnt/data ansible.posix.mount: path: /mnt/data src: /dev/vg100/lv100 fstype: xfs state: mounted 13.4.2 系统角色管理存储123456789101112131415161718192021222324252627282930# 逻辑卷角色[greg@control ansible]$ sudo find ./mycollection/ -name '*.yml' | grep storage|grep test.yml./mycollection/ansible_collections/redhat/rhel_system_roles/tests/storage/test.yml[greg@control ansible]$ cp ./mycollection/ansible_collections/redhat/rhel_system_roles/tests/storage/test.yml ./storage.yml[greg@control ansible]$ vim disk.yml$ vim storage.yml---- hosts: node4 vars: storage_use_partitions: true roles: - name: redhat.rhel_system_roles.storage storage_pools: - name: vg100 disks: - /dev/vdb volumes: - name: lv100 size: 200M fs_type: ext4 mount_point: '/mnt/lvm100' state: &quot;present&quot; - name: lv200 size: 300M fs_type: xfs mount_point: '/mnt/lvm200' state: &quot;absent&quot;$ ansible-navigator run storage.yml -m stdout 13.5 管理网络123456789101112131415161718192021222324252627282930[greg@control ansible]$ sudo find ./mycollection/ -name '*.md' | grep network[greg@control ansible]$ vim ./mycollection/ansible_collections/redhat/rhel_system_roles/roles/network/README.md[greg@control ansible]$ sudo find ./mycollection/ -name '*.yml' | grep network $ vim /usr/share/doc/rhel-system-roles/collection/roles/network/README.md$ vim playbook.yml---- hosts: node5 vars: network_connections: - name: eth0 interface_name: eth0 type: ethernet state: up autoconnect: yes ip: address: - 192.0.2.3/24 dns: - 192.0.2.2 dns_search: - example.com dhcp4: no gateway4: 192.0.2.1 auto6: no roles: - redhat.rhel_system_roles.network$ ansible-navigator run playbook.yml -m stdout 14 普通用户远程管理受管主机14.1 超级用户远程管理方式12345678vim /home/greg/ansible/ansible.cfgremote_user=roothost_key_checking = Falsevim /home/greg/ansible/inventory[all:vars]#ansible_user=rootansible_password=redhat 14.2 普通用户远程管理方式123456789101112131415161718192021222324# 使用greg用户远程管理受管主机【bastion】控制节点ansible.cfg[defaults]remote_user = greghost_key_checking = False[privilege_escalation]become=Truebecome_method=sudo become_user=rootbecome_ask_pass=false#管理节点ssh-keygen ssh-copy-id greg@workstationssh greg@workstation#其他受管主机上每一个主机都做如下操作[root@workstation ~]# useradd greg[root@workstation ~]# echo redhat | passwd --stdin greg[root@workstation ~]# visudogreg ALL=(ALL) NOPASSWD: ALL","link":"/2025/04/21/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/Ansible%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"},{"title":"Linux进阶","text":"1 必须掌握的Linux命令 1.1 强大好用的Shell系统内核​ 一台完整的计算机是由运算器、控制器、存储器、输入/输出等多种硬件设备共同组成的，而能让各种硬件设备各司其职且又能协同运行的东西就是系统内核 ​ Linux系统的内核负责完成对硬件资源的分配、调度等管理任务，对系统的正常运行起着十分重要的作用 ​ 与修改Windows系统中的注册表类似，直接改动内核参数的难度比较大，而且一旦”手滑”还有可能导致系统直接崩溃 ​ 不建议直接去编辑内核中的参数，而是用基于系统调用接口开发出来的程序或服务来管理计算机，以满足日常的工作需要 用户与硬件人类是无法直接控制硬件的 硬件设备由系统内核直接管理，但由于内核的复杂性太高，在访问时存在较大的风险，因此用户不能直接访问内核 虽然通过调用系统提供的API(应用程序编程接口)就能实现某个功能，但哪怕实现”将一条信息通过互联网传输给别人”这样简单的任务，都要手动调用几十次API接口，使用起来太不切实际 最外层的服务程序是最贴近于用户端的，这些服务程序集成了大量API接口的完整软件，微信、QQ就是这样的服务程序 Shell名称由来：被一层层”包裹”起来的硬件设备，感觉像一只蜗牛的壳呢？英文中的壳叫作Shell，行业中也将用户终端程序称之为Shell 翻译官：Shell就是终端程序的统称，充当了人与内核(硬件)之间的翻译官 用户把一些命令”告诉”终端程序，它就会调用相应的程序服务去完成某些工作 默认使用终端：包括红帽系统在内的许多主流Linux系统默认使用的终端是Bash(Bourne-Again SHell)解释器 Bash解释器的优势： 1.通过上下方向键来调取执行过的Linux命令 2.命令或参数仅需输入前几位就可以用Tab键补全 3.具有强大的批处理脚本 4.具有实用的环境变量功能 Shell与Bash是包含与被包含的关系： 举例来说，在社会中有翻译官这个职业，它是由许多从业者共同组成的职业名称，而Bash则是其中一个出色的成员，是Shell终端程序中的一份子 1.2 常用系统工作命令echo命令在终端设备上输出指定字符串或变量提取后的值，能够给用户一些简单的提醒信息，亦可以将输出的指定字符串内容同管道符一起传递给后续命令作为标准输入信息进行二次处理，还可以同输出重定向符一起操作，将信息直接写入文件 如需提取变量值，需在变量名称前加入$符号，变量名称一般均为大写形‍式 语法格式:echo [字符串] [$变量] 常用参数： 执行”echo 字符串”或”echo$变量”，$符号的意思是提取变量的实际值，以便后续的输出操作 命令使用示例： 1.输出指定字符串到终端设备界面(默认为电脑屏幕)： 2.使用”$变量”的方式提取出变量SHELL的值，并将其输出到屏幕上： 3.搭配转义符一起使用，输出纯字符串内容： 4.搭配输出重定向符一起使用，将字符串内容直接写入文件中： 5.搭配反引号执行命令，并将执行结果输出： 6.输出带有换行符的内容： 7.指定删除字符串中某些字符，随后将内容输出： date命令显示或设置系统日期与时间信息 可以根据想要的格式来输出系统时间信息 时间格式为MMDDhhmm[CC][YY][.ss]，其中MM为月份，DD为日，hh为小时，mm为分钟，CC为年份前两位数字，YY为年份后两位数字，ss为秒的值 语法格式：date 参数 对象 常用参数： 命令使用示例： 1.默认格式输出系统当前的日期与时间信息： 2.按照”年-月-日 小时:分钟:秒”的格式查看当前系统时间： 3.设置系统为指定的日期和时间： 4.date命令中的参数%j用来查看今天是当年中的第几天，这个参数能很好区分备份时间的早晚，数字越大，越靠近当前时间： timedatectl命令设置系统时间与日期 与使用date命令设置日期时间不同，使用timedatectl命令设置过的日期时间信息将被写入系统配置文件，从而立即且长期有效，不会随系统重启而失效 该命令能用来查看系统时间与日期，一站式搞定系统时间 语法格式:timedatectl 参数 对象 常用参数： 命令使用示例： 1.查看当前系统中的时区、日期、时间等信息： 2.关闭NTP时间服务器同步功能： 3.设置系统日期、系统时间： 4.查看可选时区： 5.设置系统时区： reboot命令用于重新启动计算机。与halt和shutdown命令相似 语法格式:reboot [参数] 常用参数： 命令使用示例： 1.重启当前系统与模拟重启当前系统： poweroff命令用于关闭操作系统 语法格式:poweroff [参数] 常用参数： 命令使用示例： 1.关闭操作系统： 2.模拟关机操作并记录过程到日志文件(没有真正关机)： 3.将所有硬件设置为备用模式，并关闭操作系统： wget命令从指定网址下载网络文件 wget命令非常稳定，即便网络发生波动也不会导致下载失败，而是不断尝试重连，直至整个文件下载完毕 wget命令支持如HTTP、HTTPS、FTP等常见协议，可以在命令行中直接下载网络文件 语法格式：wget 参数 网址URL 对象 常用参数： 命令使用示例： 1.下载指定的网络文件 2.下载指定的网络文件，并定义保存在本地的文件名称 3.下载指定的网络文件，限速最高每秒300kbit/s 4.启用断点续传技术下载指定的网络文件 5.下载指定的网络文件，将任务放至后台执行 ps命令显示当前系统的进程状态。使用ps命令可以查看到进程的所有信息，例如进程的号码、发起者、系统资源(处理器与内存)使用占比、运行状态等 ps命令可及时发现哪些进程出现”僵死”或”不可中断”等异常情‍况 ps命令经常会与kill命令搭配使用，以中断和删除不必要的服务进程，避免服务器的资源浪费 语法格式：ps 参数 常用参数： Linux系统中有5种常见的进程状态，分别为运行、中断、不可中断、僵死与停止，其各自含义如下： 1.R(运行):进程正在运行或在运行队列中等待 2.S(中断):进程处于休眠中，当某个条件形成后或者接收到信号时，则脱离该状态 3.D(不可中断):进程不响应系统异步信号，即便用kill命令也不能将其中断 4.Z(僵死):进程已终止，但进程描述符仍存在, 直到父进程调用wait4()系统函数后将进程释放 5.T(停止):进程收到停止信号后停止运行 除了上面5种常见的进程状态，还有可能是： 高优先级(&lt;)、低优先级(N)、被锁进内存(L)、包含子进程(s)以及多线程(l)这5种补充形式 ps命令可允许参数不加减号（-），因此可直接 写成ps aux的样子 命令使用示例： 1.显示系统中全部的进程信息，含详细信息： 2.结合输出重定向，将当前进程信息保留备份至指定文件： 3.结合管道操作符，将当前系统运行状态中指定的进程信息过滤出来： 4.结合管道操作符，将当前系统运行状态中指定用户的进程信息过滤出来： 5.结合管道操作符与sort命令，依据处理器使用量（第三列）情况降序排序： 6.结合管道操作符与sort命令，依据内存使用量（第四列）情况降序排序： pstree命令以树状图形式显示进程信息，Linux系统中常用ps命令查看进程状态信息，但是却无法了解进程之间的依赖关系，哪个是父进程，哪个是子进程？这些信息则可通过pstree命令进行查看 语法格式：pstree 参数 常用参数： 命令使用示例： 1.以树状图形式显示当前系统中的全部进程(默认)： 2.以树状图形式显示当前系统中的全部进程(带有进程号)： 3.以更完整、更丰富的信息样式显示每个进程： top命令实时显示系统运行状态，包含处理器、内存、服务、进程等重要资产信息 除了能看到常规的服务进程信息之外，还能够对处理器和内存的负载情况一目了然，实时感知系统全局的运行状态 语法格式:top 参数 对象 常用参数： 命令使用示例： 1.默认格式显示系统运行信息： top命令执行结果的前5行为系统整体的统计信息，其所代表的含义如下： 第1行：系统时间、运行时间、登录终端数、系统负载(3个数值分别为1分钟、5分钟、15分钟内的平均值，数值越小意味着负载越低) 第2行：进程总数、运行中的进程数、睡眠中的进程数、停止的进程数、僵死的进程数 第3行：用户占用资源百分比、系统内核占用资源百分比、改变过优先级的进程资源百分比、空闲的资源百分比等。其中数据均为CPU数据并以百分比格式显示，例如：”99.9 id”意味着有99.9%的CPU处理器资源处于空闲 第4行：物理内存总量、内存空闲量、内存使用量、作为内核缓存的内存量 第5行：虚拟内存总量、虚拟内存空闲量、虚拟内存使用量、已被提前加载的内存量 2.默认格式显示系统运行信息，但提供完整的进程路径及名称： 3.以批处理模式显示程序信息： 4.设定每5秒刷新一次信息： 5.设定总显示次数为5次，随后自动退出命令： nice命令用于调整进程的优先级，合理分配系统资源。优先级范围为-20~19，数字越小，优先级越高 语法格式:nice [参数] 命令或脚本名 常用参数： 命令使用示例： 1.以优先级别为5的方式执行指定脚本： 2.以最高优先级的方式执行指定脚本： pidof命令用于查找服务进程的PID号码 语法格式:pidof [参数] 服务名 常用参数： 命令使用示例： 1.查找某个指定服务所对应的进程PID号码： 2.查找多个指定服务所对应的进程PID号码： kill命令kill命令的功能是杀死(结束)进程 Linux系统中如需结束某个进程，既可以使用如service或systemctl这样的管理命令来结束服务，也可以使用kill命令直接结束进程信息 如使用kill命令后进程并没有结束，则可以使用信号9进行强制杀死动作 语法格式：kill 参数 进程号 常用参数： 命令使用示例： 1.列出系统支持的全部信号列表： 2.结束某个指定的进程(数字为对应的PID值)： 3.强制结束某个指定的进程(数字为对应的PID值)： killall命令用于基于服务名关闭一组进程 常使用kill命令关闭指定PID进程号的服务，暂且不论要先用ps命令找到对应的进程号才能关闭它，就说很多服务实际会发起多个进程，对应有数个不同PID进程号，要用kill命令逐一关闭也是件麻烦事 将ps和kill两个命令的执行过程合二为一，就得到了超好用的killall命令。一个只需要管理员给出要关闭的服务名，就能自动找到其所对应的全部进程信息，并关闭它们 语法格式:killall [参数] 服务名 常用参数： 命令使用示例： 1.结束指定服务所对应的全部进程： 2.打印已知信号列表： 如果在系统终端中执行一个命令后想立即停止它，可以同时按下Ctrl+C组合键，这样将立即终止该命令的进程 如果有些命令在执行时不断地在屏幕上输出信息，影响到后续命令的输入，则可以在执行命令时在末尾添加一个&amp;符号，这样命令将进入系统后台来执行 1.3 系统状态检测命令ifconfig命令显示或设置网络设备参数信息。Windows系统中类似的命令为ipconfig，同样的功能可以使用ifconfig去完‍成 不建议使用ifconfig命令配置网络设备的参数信息，因为一旦服务器重启，配置过的参数会自动失效，因此还是编写到配置文件中更稳妥 使用ifconfig命令来查看本机当前的网卡配置与网络状态等信息时，主要查看的是网卡名称、inet 参数后面的IP地址、ether参数后面的网卡物理地址(MAC地址)以及RX、TX的接收数据包与发送数据包的个数及累计流量(即下面加粗的信息内容)： 语法格式：ifconfig 参数 网卡名 动作 常用参数： 常用动作： 命令使用示例： 1.对指定网卡设备依次进行关闭和启动操作： 2.对指定的网卡设备执行IP地址修改操作： 3.对指定的网卡设备执行MAC地址修改操作： 4.对指定的网卡设备依次进行ARP协议关闭与开启操作： uname命令查看系统内核版本与系统架构等信息 语法格式:uname 参数 常用参数： 命令使用示例： 1.显示系统内核名称 2.显示系统所有相关信息(内核名称、主机名、版本号及硬件架构) 3.显示系统内核版本号 4.显示系统硬件架构 5.如果要查看当前系统版本的详细信息，则需要查看redhat-release文件，其命令以及相应的结果如下： uptime命令查看系统负载信息，可以显示当前系统时间、系统已运行时间、启用终端数量以及平均负载值等信息 平均负载值指的是系统在最近1分钟、5分钟、15分钟内的压力情况，负载值越低越好 建议负载值保持在1左右，在生产环境中不要超过5就好 语法格式：uptime 参数 命令使用示例： 1.查看当前系统负载及相关信息： 2.以更易读的形式显示系统已运行时间： 3.显示本次系统的开机时间： free命令显示系统内存使用量情况，包含物理内存和交换内存的总量、使用量、空闲量情况 语法格式：free 参数 常用参数： 命令使用示例： 1.以默认的容量单位显示内存使用量信息 2.以MB为单位显示内存使用量信息 3.以易读的单位显示内存使用量信息 4.以易读的单位显示内存使用量信息，每隔10s刷新一次 who命令用于显示当前登录用户信息，包含登录的用户名、终端、日期时间、进程等信息 语法格式:who 参数 常用参数: 命令使用示例: 1.查看当前登录用户信息 2.查看当前登录用户信息，并加上标题 3.查看当前全部的登录全部用户信息 4.查看系统的最近启动时间 last命令显示用户历史登录情况 通过读取系统登录历史日志文件(/var/log/wtmp)并按照用户名、登录终端、来源终端、时间等信息进行划分，可让用户对系统历史登录情况一目了然 语法格式：last 参数 对象 常用参数： 命令使用示例： 1.显示近期用户或终端的历史登录情况 2.仅显示最近3条历史登录情况，并不显示来源终端信息 3.显示系统的开关机历史信息，并将来源终端放到最后 ping命令测试主机间网络的连通性，它发送出基于ICMP传输协议的数据包，要求对方主机予以回复 若对方主机的网络功能没有问题且防火墙放行流量，则就会回复该信息，我们也就可得知对方主机系统在线并运行正常了 Windows系统下的ping命令会发送出去4个请求后自动结束该命令 Linux系统下ping命令不会自动终止，需要用户手动按下Ctrl+C组合键才能结束，或是发起命令时加入-c参数限定发送数据包的个数。** 语法格式：ping 参数 域名或IP地址 常用参数： 命令使用示例: 1.测试与指定域名之间的网络连通性(需手动按下Ctrl+C组合键结束命令) 2.测试与指定主机之间的网络连通性，发送请求包限定为4个 3.测试与指定主机之间的网络连通性，发送3个请求包，每次间隔0.2s，最长等待时间为3s tracepath命令追踪数据包的路由信息 tracepath命令能够追踪并显示数据包到达目的主机所经过的路由信息，以及对应的MTU值 语法格式:tracepath 参数 域名或IP地址 常用参数： 命令使用示例: 1.追踪到达域名的主机路由信息 2.追踪到达域名的主机路由信息，同时显示IP地址与主机名 3.设置追踪数据包路由的最大TTL值为20，并追踪到达域名的主机路由信息 netstat命令显示各种网络相关信息，例如网络连接状态、路由表信息、接口状态、NAT、多播成员等 语法格式：netstat 参数 常用参数： 命令使用示例: 1.显示系统网络状态中的所有连接信息 2.显示系统网络状态中的UDP连接信息 3.显示系统网络状态中的UDP连接端口号使用信息 4.显示网卡当前状态信息 5.显示网络路由表状态信息 6.找到某个服务所对应的连接信息 history命令显示与管理历史命令记录 Linux系统默认会记录用户执行过的有命令，可以使用history命令查阅它们，也可以对其记录进行修改和删除操作 ​ 执行history命令能显示出当前用户在本地计算机中执行过的最近1000条命令记录。如果觉得1000不够用，可以自定义/etc/profile文件中的HISTSIZE变量值。在使用history命令时，可以使用-c参数清空所有的命令历史记录。还可以使用”!编码数字”的方式来重复执行某一次的命令 语法格式：history 参数 常用参数: 命令使用示例: 1.显示执行过的全部命令记录 2.显示最近执行过的5条命令 3.将本次缓存区信息写入历史文件（~/.bash_history） 4.将历史文件中的信息读入当前缓冲区 5.将本次缓冲区信息追加写入历史文件（~/.bash_history） 6.清空本次缓冲区及历史文件中的信息 sosreport命令用于收集系统配置及架构信息并输出诊断文档 当Linux系统出现故障需要联系技术支持人员时，大多数时候都要先使用这个命令来简单收集系统的运行状态和服务配置信息，以便让技术支持人员能够远程解决一些小问题，抑或让他们能提前了解某些复杂问题 语法格式：sosreport [参数] 常用参数： 命令使用示例: 1.列出所有可用的插件及其选项 2.使用-e参数，启用指定apache.log插件 3.使用-n参数，禁用指定yum.yumlist插件 4.使用-a参数，启用所有列出来的插件 1.4 查找定位文件命令pwd命令显示当前工作目录的路径，即显示所在位置的绝对路‍径 语法格式：pwd 参数 常用参数： 命令使用示例： 1.查看当前所处的工作目录 cd命令更改当前所处的工作目录，路径可以是绝对路径，也可以是相对路径，若省略不写则会跳转至当前使用者的家目‍录 “cd-“返回到上一次所处的目录、”cd..”进入上级目录、”cd～”切换到当前用户的家目录、”cd～username”切换到其他用户的家目录 语法格式：cd 参数 目录名 常用参数： 命令使用示例: 1.切到指定目录 ls命令显示目录中的文件及其属性信息 所处的工作目录不同，当前工作目录下能看到的文件肯定也不同 使用ls命令的-a参数可以看到全部文件（包括隐藏文件），使用-l参数可以查看文件的属性、大小等详细信息。将这两个参数整合之后，再执行ls命令即可查看当前目录中的所有文件并输出这些文件的属性信息： 语法格式：ls 参数 文件名 常用参数： 命令使用示例： 1.显示当前目录中的文件名（默认不含隐藏文件） 2.显示当前目录中的文件名（含隐藏文件） 3.以详细信息模式输出文件名及其属性信息 4.显示指定目录中的文件列表 5.显示当前目录中的文件名及inode属性信息 6.结合通配符一起使用，显示指定目录中所有以sd开头的文件列表 7.依据文件内容大小进行排序，显示指定目录中文件名及其属性详情信息 tree命令以树状图形式列出目录内容 语法格式:tree 参数 常用参数: 命令使用示例: 1.显示当前工作目录下的文件层级 2.以文件和目录的更改时间进行排序 3.以带有相对路径的形式显示当前工作目录下的文件层级 4.只显示目录的层级关系 find命令根据给定的路径和条件查找相关文件或目录，其参数灵活方便，且支持正则表达式，结合管道符后能够实现更加复杂的功能 find命令通常进行的是从根目录(/)开始的全盘搜索，有别于whereis、which、locate等有条件或部分文件的搜索 对于服务器负载较高的情况，建议不要在高峰时期使用find命令的模糊搜索，这会相对消耗较多的系统资源 语法格式:find 路径 条件 文件名 常用参数： 命令使用示例: 1.全盘搜索系统中所有以.conf结尾的文件 2.在/etc目录中搜索所有大于1MB的文件 3.在/home目录中搜索所有属于指定用户的文件 4.在/var/log目录下搜索所有不是以.log结尾的文件 5.搜索当前工作目录中所有近7天被修改过的文件 6.全盘搜索系统中所有类型为目录且权限为1777的目录文件 7.全盘搜索系统中所有类型为普通文件且可执行的文件信息 8.全盘搜索系统中所有后缀为.mp4的文件，并删除所有查找到的文件 locate命令快速查找文件或目录 与find命令进行全局搜索不同，locate命令是基于数据文件(/var/lib/locatedb)进行的定点查找 在使用 locate 命令时，先使用updateddb命令生成一个索引库文件，这个库文件的名字是/var/lib/mlocate/mlocate.db，后续在使用locate命令搜索文件时就是在该库中进行查找操作，速度会快很多 第一次使用locate命令之前，记得先执行updatedb命令来生成索引数据库，然后再进行查找： 语法格式:locate 参数 文件名 常用参数： 命令使用示例: 1.在指定目录下搜索带有指定关键词的文件 whereis命令​ 按照名称快速搜索二进制程序(命令)、源代码以及帮助文件所对应的位置也是基于updatedb命令所生成的索引库文件进行搜索，与locate命令的区别是不关心那些相同名称的文件，仅仅是快速找到对应的命令文件及其帮助文件所在的位置 语法格式：whereis 参数 命令名 常用参数: 命令使用示例: 1.查找指定命令程序及相关文件所在的位置 2.仅查找指定命令程序文件所在的位置 3.仅查找指定命令的帮助文件所在的位置 which命令查找命令文件，能快速搜索二进制程序所对应的位置 如果既不关心同名文件(find与locate)，也不关心命令所对应的源代码和帮助文件（whereis），仅仅是想找到命令本身所在的路径，那么这个which命令就太合适了 语法格式：which 参数 文件名 常用参数： 命令使用示例: 1.查找某个指定命令文件所在位置 2.查找多个执行命令文件所在位置 1.5 文本文件编辑命令cat命令在终端设备上显示文件内容 在Linux系统中有很多用于查看文件内容的命令，如more、tail、head等，每个命令都有各自的特点 cat命令适合查看内容较少的纯文本文件。 对于内容较多的文件，用more命令‍显示 语法格式:cat 参数 文件名 常用参数： 命令使用示例: 1.查看指定文件的内容 2.查看指定文件的内容并显示行号 3.搭配空设备文件和输出重定向操作符，清空指定文件的内容 4.持续写入文件内容，直到碰到EOF终止符后结束并保存 5.搭配输出重定向操作符，将光盘设备制作成镜像文件 more命令分页显示文本文件的内容 如果文本文件中的内容较多较长，使用cat命令读取后则很难看清，这时使用more命令进行分页查看就比较合适了 该命令可以把文本内容一页一页地显示在终端界面上，用户每按一次Enter键即向下一行，每按一次空格键即向下一页，直至看完为止 语法格式:more 参数 文件名 常用参数： 命令使用示例: 1.分页显示指定的文本文件内容 2.先进行清屏操作，随后以每次10行内容的格式显示指定的文本文件内容 3.分页显示指定的文本文件内容，若遇到连续两行及以上空白行的情况，则以一行空白行显示 4.从第10行开始，分页显示指定的文本文件内容 head命令显示文件开头的内容，默认为前10行 语法格式：head 参数 文件名 常用参数： 命令使用示例: 1.默认显示文件前10行内容 2.显示指定文件前5行内容 3.显示指定文件前20个字符 tail命令tail命令用于查看纯文本文件的后N行或持续刷新文件的最新内容 如果指定了多个文件，则会在显示的每个文件内容前面加上文件名来加以区分 高阶玩法的-f参数的作用是持续显示文件的尾部最新内容 语法格式：tail 参数 文件名 常用参数： 命令使用示例: 1.默认显示指定文件尾部的后10行内容 2.指定显示指定文件尾部的后5行内容 3.指定显示指定文件尾部的后30个字符 4.持续刷新显示指定文件尾部的后10行内容 tr命令功能是转换字符 tr命令是一款批量字符转换、压缩、删除的文本工具，但仅能从标准输入中读取文本内容，需要与管道符或输入重定向操作符搭配使用 语法格式:tr 参数 字符串1 字符串2 常用参数： 命令使用示例: 1.将指定文件中的小写字母转换成大写字母后输出内容到终端界面 2.删除指定文件中所有的数字后输出内容到终端界面 3.将指定文件中的多个相邻空行去重后输出内容到终端界面 wc命令统计文件的字节数、单词数、行数等信息，并将统计结果输出到终端界面 语法格式:wc 参数 文件名 常用参数： 命令使用示例: 1.统计指定文件的单词数量、字节数量、字符数量、总行数 2./etc/passwd是用于保存所有用户信息的文件，要统计当前系统中有多少个用户，可以使用下面的命令来进行查询: stat命令显示文件的状态信息 ​ 在Linux系统中，每个文件都有3个”历史时间”:最后访问时间(ATIME)、最后修改时间(MTIME)、最后更改时间(CTIME)，用户可以使用stat命令查看到它们，进而判别有没有其他人修改过文件内容 使用touch命令可以轻易修改文件的ATIME和MTIME，因此请勿单纯以文件历史时间作为判别系统有无被他人入侵的唯一标准 语法格式：stat 参数 文件名 常用参数： 命令使用示例: 1.查看指定文件的状态信息(含ATIME、MTIME与CTIME) 2.仅查看指定文件的文件系统信息 3.以简洁的方式查看指定文件的状态信息 grep命令grep命令用于按行提取文本内容 grep命令两个最常用的参数：-n参数用来显示搜索到的信息的行号、-v参数用于反选信息(即没有包含关键词的所有信息行) ​ 与之容易混淆的是egrep命令和fgrep命令，如果把grep命令当作标准搜索命令，那么egrep则是扩展搜索命令，等价于grep-E命令，支持扩展的正则表达式 fgrep则是快速搜索命令，等价于grep -F命令，不支持正则表达式，直接按照字符串内容进行匹配 语法格式:grep 参数 文件名 常用参数： 命令使用示例: 1.搜索指定文件中包含某个关键词的内容行 2.搜索指定文件中以某个关键词开头的内容行 3.搜索多个文件中包含某个关键词的内容行 4.搜索多个文件中包含某个关键词的内容，不显示文件名称 5.搜索指定文件中包含某个关键词位置的行号及内容行 6.搜索指定文件中不包含某个关键词的内容行 cut命令按列提取文件内容 常用的grep命令仅能对关键词进行按行提取过滤，而cut命令则可以根据指定的关键词信息，针对特定的列内容进行过滤 ​ 一般而言，基于”行”的方式来提取数据是比较简单的，只需要设置好要搜索的关键词即可。但是如果按”列”搜索，不仅要使用-f参数设置需要查看的列数，还需要使用-d参数来设置间隔符号 语法格式:cut 参数 文件名 常用参数： 命令使用示例: 1.以冒号为间隔符，仅提取指定文件中第一列的内容 2.仅提取指定文件中每行的前4个字符 diff命令比较文件内容的差异 语法格式:diff 参数 文件名1 文件名2 常用参数： 使用diff命令时，不仅可以使用–brief参数来确认两个文件是否相同，还可以使用-c 参数详细比较出多个文件的差异之处： 1.先使用cat命令分别查看diff_A.txt和diff_B.txt文件的内容，然后进行比较： 2.接下来使用diff –brief命令显示比较后的结果，判断文件是否相同 3.最后使用带有-c参数的diff命令来描述文件内容具体的不同 uniq命令去除文件中的重复内容行 ​ uniq命令能够去除掉文件中相邻的重复内容行，如果两端相同内容，但中间夹杂了其他文本行，则需要先使用sort命令进行排序后再去重，这样保留下来的内容就都是唯一的了 语法格式:uniq 参数 文件名 常用参数： 命令使用示例: 1.对指定的文件进行去重操作 2.统计相同内容行在文件中重复出现的次数 3.仅显示指定文件中没有存在完全相同内容行的信息 4.仅显示指定文件中存在完全相同内容行的信息 sort命令对文件内容进行排序 语法格式:sort 参数 文件名 常用参数: 命令使用示例: 1.对指定的文件内容按照字母顺序进行排序(默认) 2.对指定的文件内容按照数字大小进行排序 3.以冒号(:)为间隔符，对指定的文件内容按照数字大小对第3列进行排序 4.sort命令无论内容行之间是否夹杂有其他内容，只要有两个一模一样的内容行，就可以使用-u 参数进行去重操作 tee命令用于读取标准输入的数据，将其内容转交到标准输出设备中，同时保存成文件 语法格式：tee [参数] 文件名 常用参数： 命令使用示例: 1.将用户输入的数据同时写入到两个文件中 2.执行某个指定的命令，并将其执行结果即输出到屏幕，又写入到文件中 1.6 文件目录管理命令touch命令创建空文件与修改时间戳 文件不存在时会创建一个空内容的文本文件；如果文件存在，会对文件的Atime(访问时间)和Ctime(修改时间)进行修改操作 语法格式:touch 参数 文件名 常用参数： 命令使用示例: 1.创建出一个指定名称的空文件 2.结合通配符，创建多个指定名称的空文件 3.修改指定文件的查看时间和修改时间 mkdir命令创建目录文件 若要创建的目标目录已经存在，则会提示已存在而不继续创建，不覆盖已有文件 若目录不存在，但具有嵌套的依赖关系时，例如/Dir1/Dir2/Dir3/Dir4/Dir5，要想一次性创建则需要加入-p参数，进行递归操‍作 语法格式:mkdir 参数 目录名 常用参数： 命令使用示例: 1.建立一个目录文件 2.创建一个目录文件并设置700权限，不让除所有主以外的任何人读、写、执行它 3.一次性创建多个目录文件 4.在系统根目录中，一次性创建多个有嵌套关系的目录文件 cp命令复制文件或目录 cp命令能够将一个或多个文件或目录复制到指定位置，亦常用于文件的备份工作 -r参数用于递归操作，复制目录时若忘记添加则会直接报错 -f参数则用于当目标文件已存在时会直接覆盖而不再询问 复制操作具 体分为3种情况： 1.如果目标文件是目录，则会把源文件复制到该目录中 2.如果目标文件也是普通文件，则会询问是否要覆盖它 3.如果目标文件不存在，则执行正常的复制操作 语法格式：cp 参数 源文件名 目标文件名 常用参数： 命令使用示例: 1.复制指定的源文件，并定义新文件的名称 2.复制指定的源目录，并定义新目录的名称 3.复制文件时，保留其原始权限及用户归属信息 4.将指定文件复制到/etc目录中，并覆盖已有文件，不进行询问 5.将多个文件一同复制到/etc目录中，如已有目标文件名称则默认询问是否覆盖 mv命令用于剪切或重命名文件 剪切不同于复制，默认会把源文件删除，只保留剪切后的文件 如果在同一个目录中将某个文件剪切后还粘贴到当前目录下，其实也就是对该文件进行了重命名操作 语法格式:mv 参数 源文件名 目标文件名 常用参数: 命令使用示例: 1.对指定文件进行剪切后粘贴(重命名)操作 2.将指定文件移动到/etc目录中，保留文件原始名称 3.将指定目录移动到/etc目录中，并定义新的目录名称 4.将/home目录中所有的文件都移动到当前工作目录中，若遇到文件已存在则直接覆盖 rm命令用于删除文件或目录 Linux系统中删除文件时，系统会默认询问是否要执行删除操作，如果不想总是看到这种反复的确认信息，可在rm命令后跟上-f参数来强制删除 要想删除一个目录， 需要在rm命令后面加一个-r参数才可以，否则删除不掉 语法格式：rm 参数 文件名 常用参数： 命令使用示例: 1.删除文件时默认会进行二次确认，敲击y进行确认 2.强制删除文件而无须二次确认 3.删除指定目录及其内的全部子文件，一并强制删除 4.强制删除当前工作目录内所有以.txt为后缀的文件 5.强制清空服务器系统内的所有文件 dd命令按照指定大小和个数的数据块来复制文件或转换文件 语法格式为：dd if= 参数值 of=参数值 count=参数值 bs=参数值 语法格式：dd 参数 对象 常用参数： 命令使用示例: 1.用dd命令从/dev/zero设备文件中取出一个大小为560MB的数据块，然后保存成名为560_file的文件 2.dd命令的功能也绝不仅限于复制文件这么简单 ​ 如果想把光驱设备中的光盘制作成iso格式的镜像文件，在 Windows系统中需要借助于第三方软件才能做到，但在Linux系统中可以直接使用dd命令来压制出光盘镜像文件，将它变成一个可立即使用的iso镜像： file命令用于查看文件的类型 语法格式：file 参数 文件名 常用参数： 命令使用示例: 1.查看指定文件的类型 2.查看指定文件的类型，但不显示文件名 3.通过MIME来分辨指定文件的类型 4.查看符号链接文件的类型，会提示实际的文件名称 5.直接查看指定符号链接文件所对应的目标文件的类型 tar命令压缩和解压缩文件，能够制作出Linux系统中常见的tar、tar.gz、tar.bz2等格式的压缩包文件 对于RHEL 7、CentOS7版本及以后的系统，解压缩时不添加格式参数（如z或j），系统也能自动进行分析并解压 语法格式:tar 参数 压缩包名 文件或目录名 常用参数： -c参数用于创建压缩文件，-x参数用于解压文件，因此这两个参数不能同时使用 -z参数指定使用gzip格式来压缩或解压文件，-j参数指定使用bzip2格式来压缩或解 压文件 用户使用时则是根据文件的后缀来决定应使用何种格式的参数进行解压 使用-v参数向用户不断显示 压缩或解压的过程 -C参数用于指定要解压到哪个指定的目录。-f参数特别重要，它必须放到参数的最后一位，代表要压缩或解压的软件包名称 一般使用”tar -czvf 压缩包名称.tar.gz 要打包的目录”命令把指定的文件进行打包压缩；相应的解压命令为”tar -xzvf压缩 包名称.tar.gz” 演示打包压缩与解压的操作： 1.先使用tar命令把/etc目录通过gzip格式进行打包压缩，并把文件命名为etc.tar.gz： 2.将打包后的压缩包文件指定解压到/root/etc目录中(先使用mkdir命令创建/root/etc目录)： 2 管道符、重定向与环境变量2.1 输入输出重定向输入重定向是指把文件导入到命令中，而输出重定向则是指把原本要输出到屏幕的数据信息写入到指定文件中 相较于输入重定向，使用输出重定向的频率更高，所以又将输出重定向分为了标准输出重定向和错误输出重定向两种： 1.标准输入重定向(STDIN，文件描述符为0):默认从键盘输入，也可从其他文件或命令中输入 2.标准输出重定向( STDOUT，文件描述符为1):默认输出到屏幕 3.错误输出重定向(STDERR，文件描述符为2):默认输出到屏幕 ​ 分别查看两个文件的属性信息，先创建出第一个文件，而第二个文件是不存在的。所以，虽然针对这两个文件的操作都分别会在屏幕上输出一些信息，但这两个操作的差异其实很大： ​ 上述命令中名为linuxprobe的文件是真实存在的，输出信息是该文件的一些相关权限、所有者、所属组、文件大小及修改时间等信息，这也是该命令的标准输出信息。而名为xxxxxx的第二个文件是不存在的，因此在执行完ls命令之后显示的报错提示信息也是该命令的错误输出信息。那么，要想把原本输出到屏幕上的数据转而写入到文件当中，就要区别对待这两种输出信息。 1.输入重定向中用到的符号及其作用 ： 2.输出重定向中用到的符号及其作用： 对于重定向中的标准输出模式，可以省略文件描述符1不写，而错误输出模式的文件描述符2是必须要写的 通过标准输出重定向将man bash命令原本要输出到屏幕的信息写入到文件readme.txt中，然后显示readme.txt文件中的内容。具体命令如下： ​ 通过覆盖写入模式向readme.txt文件写入多行数据(该文件中已包含man命令信息)。通过覆盖写入模式向文件中写入数据时，每一次都会覆盖掉上一次写入的内容，所以最终文件中只有最后一次的写入结果： 通过追加写入模式向readme.txt文件写入一次数据，然后在执行cat命令之后，可以看到如下所示的文件内容： ​ 虽然都是输出重定向技术，但是命令的标准输出和错误输出还是有区别的。例如查看当前目录中某个文件的信息，这里以linuxprobe文件为例。由于这个文件是真实存在的，因此使用标准输出即可将原本要输出到屏幕的信息写入到文件中，而错误的输出重定向则依然把信息输出到了屏幕上： ​ 如何把命令的报错信息写入到文件？当用户在执行一个自动化的 Shell脚本时，这个操作会特别有用，而且特别实用，因为它可以把整个脚本执行过程中的报错信息都记录到文件中： ​ 不区分标准输出和错误输出，只要命令有输出信息则全部追加写入到文件中。这就要用到&amp;&gt;&gt;操作符： ​ 输入重定向的作用是把文件直接导入到命令中。使用输入重定向把readme.txt文件导入给wc -l 命令，统计文件中的内容行数： ​ “wc -l /etc/passwd”是非常标准的”命令+参数+对象”的执行格式，而”wc -l &lt; readme.txt”是将readme.txt文件中的内容通过操作符导入到命令中，没有被当作命令对象进行执行，因此wc命令只能读到信息流数据，而没有文件名称的信息。 2.2 管道命令符把前一个命令原本要输出到屏幕的信息当作后一个命令的标准输入 1.找出所有被限制登录系统的用户：grep /sbin/nologin /etc/passwd 2.统计文本行数的命令则是wc –l 3.把grep搜索命令的输出值传递给wc统计命令，即把原本要输出到屏幕的用户信息列表再交给wc命令作进一步的加工，因此只需要把管道符放到两条命令之间即可，具体如下： 4.用翻页的形式查看/etc 目录中的文件列表及属性信息： 5.在修改用户密码时，通常都需要输入两次密码以进行确认，这在编写自动化脚本时将成为一个非常致命的缺陷 通过把管道符和passwd命令的–stdin参数相结合，可以用一条命令来完成密码重置操作： 6.如果需将管道符处理后的结果既输出到屏幕，又同时写入到文件中，可以与tee命令结合使用： 2.3 命令行的通配符通配符就是通用的匹配信息的符号 1.星号（*）代表匹配零个或多个字符 2.问号（?）代表匹配单个字符 3.中括号内加上数字[0-9]代表匹配0～9之间的单个数字的字符 4.中括号内加上字母[abc]则是代表匹配a、b、c三个字符中的任意一个字符 使用示例: 1.匹配所有在/dev目录中且以sda开头的文件 2.只想查看文件名以sda开头，但后面还紧跟其他某一个字符的文件的相关信息 3.除了使用[0-9]来匹配0～9之间的单个数字，也可以用[135]这样的方式仅匹配这3个指定数字中的一个 若没有匹配到数字1或2或3，则不会显示出来 4.通配符也可以放到前面。比如，搜索/etc/目录中所有以.conf结尾的配置文件有哪些 5.可以与创建文件的命令相结合，创建出很多个文件。不过在创建多个文件时，需要使用大括号，并且字段之间用逗号间隔 6.使用通配符可以输出一些指定的信息 2.4 常用的转义字符为了能更好地理解用户的表达，Shell解释器提供了丰富的转义字符来处理输入的特殊数据 反斜杠( \\ ):使反斜杠后面的一个变量变为单纯的字符 单引号(‘ ‘):转义其中所有的变量为单纯的字符串 双引号(“ “):保留其中的变量属性，不进行转义处理 反引号(` `):把其中的命令执行后返回结果 使用示例： 1.定义一个名为PRICE的变量并赋值为5，然后输出以双引号括起来的字符串与变量信息 2.输出”Price is $5”，即”价格是5美元”的字符串内容 3.将反引号与uname -a命令结合，使用echo命令查看本机的Linux版 本和内核信息 4.在大多数情况下好像加不加双引号，效果都一样 ​ 两者的区别在于无法得知第一种执行方式中到底有几个参数。因 为有可能把”AA BB CC”当作一个参数整体直接输出到屏幕，也有可能分别将AA、BB和CC输出到屏幕。如果参数中出现了空格，就加双引号；如果参数中没有空格，那就不用加双引号 2.5 重要的环境变量变量是计算机系统用于保存可变值的数据类型 在Linux系统中，变量名称一般都是大写的，命令则都是小写的，这是一种约定俗成的规范 Linux系统的环境变量用来定义系统运行环境的一些参数，比如每个用户不同的家目录、邮件存放位置等 可以直接通过变量名称来提取到对应的变量值 命令在Linux中的执行分为4个步骤： 1.判断用户是否以绝对路径或相对路径的方式输入命令(如/bin/ls)，如果是绝对路径则直接执行，否则进入第2步继续判断 2.Linux系统检查用户输入的命令是否为”别名命令”，即用一个自定义的命令名称来替换原本的命令名称 ​ 使用rm命令删除文件时，Linux系统都会要求用户确认是否执行删除操作，这是Linux系统为防止用户误删除文件而特意设置的rm别名命令：”rm-i” alias命令用于创建一个属于自己的命令别名，语法格式为”alias 别名=命令”。取消一个命令别名，则用unalias命令，语法格式为”unalias 别名” 将当前rm命令所被设置的别名取消掉，再删除文件试试： 3.Bash解释器判断用户输入的是内部命令还是外部命令 内部命令是解释器内部的指令，会被直接执行 用户在绝大部分时间输入的是外部命令，这些命令交由步骤4继续处理 4.系统在多个路径中查找用户输入的命令文件，定义这些路径的变量叫作PATH，作用是告诉Bash解释器待执行的命令可能存放的位置，然后Bash解释器就会在这些位置中逐个查找 为什么不能将当前目录(.)添加到PATH中呢? ​ 尽管可以将当前目录(.)添加到PATH变量中，从而在某些情况下可以让用户免去输入命令所在路径的麻烦。但如果黑客在比较常用的公共目录/tmp中存放了一个与ls或cd命令同名的木马文件，而用户又恰巧在公共目录中执行了这些命令，那么就极有可能中招了 Linux系统中最重要的10个环境变量： ​ Linux作为一个多用户、多任务的操作系统，能够为每个用户提供独立的、合适的工作运行环境。因此，一个相同的变量会因为用户身份的不同而具有不同的值。使用下述命令来查看 HOME变量在不同的用户身份下都有哪些值： ​ 变量是由固定的变量名与用户或系统设置的变量值两部分组成的，可以自行创建变量来满足工作需求。例如，设置一个名称为WORKDIR的变量，方便用户更轻松地进入一个层次较深的目录： 但是这样的变量不具有全局性，作用范围也有限，默认情况下不能被其他用户使用： 如果工作需要，可使用export命令将其提升为全局变量，这样其他用户也就可以使用它了： 直接在终端设置的变量能够立即生效，但在重启服务器后就会失效，需要将变量和变量值写入到.bashrc或者.bash_profile文件中，以确保永久能使用 3 Vim编辑器与Shell命令脚本3.1 Vim文本编辑器Linux系统中一切都是文件，而配置一个服务就是在修改其配置文件的参数 Vim文本编辑器默认会安装在当前所有的Linux操作系统上，是一款超棒的文本编辑器 Vim编辑器中设置了3种模式： 命令模式:控制光标移动，可对文本进行复制、粘贴、删除和查找等 末行模式:正常的文本录入 编辑模式:保存或退出文档，以及设置编辑环境 Vim编辑器模式的切换： 1.每次运行Vim编辑器时，默认进入命令模式，此时需先切换到输入模式后再进行文档编写工作 2.每次编写完文档后需要先返回命令模式，然后再进入末行模式，执行文档的保存或退出操作 3.在Vim中，无法直接从输入模式切换到末行模式 命令模式中最常用的一些命令： 末行模式主要用于保存或退出文件，以及设置Vim编辑器的工作环境，还可以让用户执行外部的Linux命令或跳转到所编写文档的特定行数 要切换到末行模式，在命令模式 输入一个冒号就可以 末行模式常用的命令： 编写简单文档​ 给文档取个名字，这里将其命名为practice.txt。如果存在该文档，则是打开它。如果不存在，则是创建一个临时的输入文件： ​ 打开practice.txt文档后，默认进入Vim编辑器的命令模式。此时只能执行该模式下的命令，而不能随意输入文本内容 ​ 需要切换到输入模式才可以编写文档，可以分别使用a、i、o这3个键从命令模式切换到输入模式。其中，a键与i键分别是在光标后面一位和光标当前位置切换到输入模式，而o键则是在光标的下面再创建一个空行，此时可敲击a键进入编辑器的输入模式： ​ 进入输入模式后，可随意输入文本内容，Vim编辑器不会把输入的文本内容当作命令而执行： 在编写完之后，要想保存并退出，必须先敲击键盘的 Esc键从输入模式返回命令模式： 再输入”:wq!”切换到末行模式才能完成保存退出操作： 当在末行模式中输入”:wq!”命令时，就意味着强制保存并退出文档。然后可以用cat命令查看保存后的文档内容： 配置主机名称​ 在局域网中查找某台特定的主机，或者对主机进行区分，除了要有IP地址外，还要为主机配置一个主机名，主机之间可以通过这个类似于域名的名称来相互访问在Linux 系统中，主机名大多保存在/etc/hostname文件中，接下来将/etc/hostname 配置文件的内容修改为”linuxprobe.com”，步骤如下： 1.使用Vim编辑器修改/etc/hostname主机名称文件 2.把原始主机名称删除后追加”linuxprobe.com 3.保存并退出文档，然后使用hostname命令检查是否修改成功 ​ hostname命令用于查看当前的主机名称，但有时主机名称的改变不会立即同步到系统中，如果发现修改完成后还显示原来的主机名称，可重启虚拟机后再行查看： 配置网卡信息现在有一个名称为ifcfg-ens160的网卡设备，将其配置为开机自启动，并且IP地址、子网、网关等信息由人工指定，其步骤如下 第一步:切换到/etc/sysconfig/network-scripts目录中(存放网卡的配置文件) 第二步:使用Vim编辑器修改网卡文件ifcfg-ens160，逐项写入下面的配置参数并保存退出 由于每台设备的硬件及架构是不一样的，因此请使用ifconfig命令自行确认各自网卡的默认名称 12345678设备类型：TYPE=Ethernet地址分配模式：BOOTPROTO=static网卡名称：NAME=ens160是否启动：ONBOOT=yesIP地址：IPADDR=192.168.1.28子网掩码：NETMASK=255.255.255.0网关地址：GATEWAY=192.168.1.1DNS地址：DNS1=192.168.1.1 第三步:重启网络服务并测试网络是否连通 进入到网卡配置文件所在的目录，然后编辑网卡配置文件，在其中填入下面的信息： 执行重启网卡设备的命令，然后通过ping命令测试网络能否连通： 启动或停止网卡： 1234nmcli connection reload ——重载网卡nmcli connection up ens33 ——激活网卡ens33nmcli connection down ens33 ——停用网卡ens33nmcli connection down ens33 &amp;&amp; nmcli connection up ens33 ——重启网卡ens33 配置软件仓库软件仓库是一种能进一步简化RPM管理软件的难度以及自动分析所需软件包及其依赖关系的技术 可把Yum或DNF想成是一个硕大的软件仓库，里面保存有几乎所有常用的工具，只需要说出所需的软件包名称，系统就会自动为您搞定一切 Yum与DNF软件仓库的配置文件是通用的，填写好配置文件信息后，这两个软件仓库的命令都是可以正常使用 在RHEL 8中使用dnf作为软件的安装命令，因为它具备更高的效率，而且支持多线程同时安装软件 配置软件仓库的步骤： 1.进入/etc/yum.repos.d/目录(该目录存放着软件仓库的配置文件) 2.使用Vim编辑器创建一个名为rhel8.repo的新配置文件(文件名称可随意，但后缀必须为.repo），逐项写入下面的配置参数并保存退出 123456仓库名称：具有唯一性的标识名称，不应与其他软件仓库发生冲突描述信息（name）：可以是一些介绍性的词，易于识别软件仓库的用处仓库位置（baseurl）：软件包的获取方式，可以使用FTP或HTTP下载，也可以是本地的文件（需要在后面添加file参数）是否启用（enabled）：设置此源是否可用；1为可用，0为禁用是否校验（gpgcheck）：设置此源是否校验文件；1为校验，0为不校验公钥位置（gpgkey）：若上面的参数开启了校验功能，则此处为公钥文件位置。若没有开启，则省略不写 3.按配置参数中所填写的仓库位置挂载光盘，并把光盘挂载信息写入/etc/fstab文件中 4.使用”dnf install httpd -y”命令检查软件仓库是否已经可用 ​ 进入/etc/yum.repos.d目录创建软件仓库的配置文件： ​ 创建挂载点后进行挂载操作，并设置成开机自动挂载： ​ 尝试使用软件仓库的dnf命令来安装Web服务，软件包名称为httpd，安装后出现”Complete!”则代表配置正确： 3.2 编写Shell脚本​ 可将Shell终端解释器当作人与计算机硬件之间的”翻译官”，作为用户与Linux系统内部的通信媒介，除了能支持各种变量与参数外，还提供了诸如循环、分支等高级编程语言才有的控制结构特性 Shell脚本命令的工作方式有下面两种： 交互式(Interactive):用户每输入一条命令就立即执行 批处理(Batch):由用户事先编好一个完整的Shell脚本，Shell会一次性执行脚本中诸多的命令 当前系统已经默认使用Bash作为命令行终端解释器： 编写简单的脚本使用Vim编辑器把Linux命令按照顺序依次写入到一个文件中，就是一个简单的脚本 如果想查看当前所在工作路径并列出当前目录下所有的文件及属性信息，实现这个功能的脚本应该类似于下面这样： Shell脚本文件的名称可以任意，但为了避免被误以为是普通文件，建议将.sh后缀加上，以表示是一个脚本文件。 在上面的example.sh脚本中实际上出现了3种不同的元素： 第一行的脚本声明(#!)用来告诉系统使用哪种Shell解释器来执行该脚本 第二行的注释信息(#)是对脚本功能和某些命令的介绍信息 第三、四行的可执行语句是执行的Linux命令 第二种运行脚本程序的方法是通过输入完整路径的方式来执行。默认会因为权限不足而提示报错信息，需要为脚本文件增加执行权限 接收用户的参数为了让Shell脚本程序更好地满足实时需求，以便灵活完成工作，必须要让脚本程序能够像之前执行命令时那样，接收输入的参数 Linux系统的Shell脚本语言内设了用于接收参数的变量，变量之间使用空格间隔： 12345$0对应的是当前Shell脚本程序的名称$#对应的是总共有几个参数$*对应的是所有位置的参数值$?对应的是显示上一次命令的执行返回值$1、$2、$3……则分别对应着第N个位置的参数值 通过引用上面的变量参数来看一下真实效果： 判断用户的参数Shell脚本的条件测试语法可判断表达式是否成立，若条件成立则返回数字0，否则返回非值 条件测试语法的执行格式如下图所示。条件表达式两边均应有一个空格： 按照测试对象来划分，条件测试语句可分为4种：文件测试语句、逻辑测试语句、整数值比较语句、字符串比较语句 1.文件测试即使用指定条件判断文件是否存在或权限是否满足等情况的运算符，具体参数如下: 使用文件测试语句来判断/etc/fstab是否为一个目录类型的文件或是否为一般文件，通过Shell内设$?变量显示上一条命令执行后的返回值 如果返回值为0，则目录存在或为一般文件；如果返回值为非零的值，则意味着它不是目录或这个目录不存在以及不为一般文件： 2.逻辑语句用于对测试结果进行逻辑分析，根据测试结果可实现不同的效果 ​ Shell终端中逻辑”与”运算符是&amp;&amp;，表示当前面的命令执行成功后才会执行后面的命令,因此可以用来判断/dev/cdrom文件是否存在，若存在则输出Exist字样 ​ 逻辑”或”在Linux系统中的运算符号为||，表示前面的命令执行失败后才会执行后面的命令： ​ 逻辑”非”在Linux系统中的运算符号是一个叹号(!)，表示把条件测试中的判断结果取相反值： ​ 叹号应放到判断语句的前面，代表对整个的测试语句进行取反值操作，而不应该写成”$USER !=root”，因为”!=”代表的是不等于符号（≠），尽管执行效果一样，但缺少了逻辑关系 注明： 1.&amp;&amp;是逻辑”与”，只有当前面的语句执行成功的时候才会执行后面的语句 2.||是逻辑”或”，只有当前面的语句执行失败的时候才会执行后面的语句 3.!是逻辑”非”，代表对逻辑测试结果取反值；之前若为正确则变成错误，若为错误则变成正确 ​ 下面这个示例的执行顺序是，先判断当前登录用户的USER变量名称是否等于root，然后用逻辑”非”运算符进行取反操作，效果就变成了判断当前登录的用户是否为非管理员用户。最后若条件成立，则会根据逻辑”与”运算符输出user字样；若条件不满足，则会通过逻辑”或”运算符输出root字样，而只有在前面的&amp;&amp;不成立时才会执行后面的||符号： 3.可用的整数比较运算符： 先使用free -m命令查看内存使用量情况(单位为MB)，再通过”grep Mem:”命令过滤出剩余内存量的行，再用awk ‘{print $4}’命令只保留第4列： 如果把这个命令写入到Shell脚本中，建议把输出结果赋值给一个变量，以方便其他命令调用： 使用整数运算符判断内存可用量的值是否小于1024，若小 于则提示”Insufficient Memory”(内存不足)： 4.字符串比较语句用于判断测试字符串是否为空值或两个字符串是否相同。经常用来判断某个变量是否未被定义(即内容为空值) 字符串比较中常见的运算符： 通过判断String变量是否为空值，进而判断是否定义了这个变量： 当用于保存当前语系的环境变量值LANG不是英语（en.US）时，则会满足逻辑测试条件并输出”Not en.US”（非英语）的字样： 3.3 流程控制语句if条件测试语句if条件测试语句可以让脚本根据实际情况自动执行相应的命令 从技术角度来讲，if语句分为单分支结构、双分支结构、多分支结构；其复杂度随着灵活度一起逐级上升 1.单分支if条件语句 单分支结构由if、then、fi关键词组成，而且只在条件成立后才执行预设的命令： 使用单分支的if条件语句判断/media/cdrom目录是否存在，若不存在就创建这个目录，反之结束条件判断和整个Shell脚本的执行： 2.双分支if条件语句 双分支结构由if、then、else、fi关键词组成，进行一次条件匹配判断 如果与条件匹配，则去执行相应的预设命令；反之则去执行不匹配时的预设命令 使用双分支的if条件语句来验证某台主机是否在线，然后根据返回值的结果，显示主机是否在线 Linux系统的ping命令通过-c参数规定尝试的次数，使用-i参数定义每个数据包的发送间隔，使用-W参数定义等待超时时间 ​ 若前面的那条语句成功执行，则$?变量会显示数字0，反之则显示一个非零的数字(可能为1或2，取决于系统版本）。因此可以使用整数比较运算符来判断$?变量是否为0，从而获知那条语句的最终判断情况。这里的服务器IP地址为192.168.1.30，验证一下脚本的效果： 3.多分支if条件语句 多分支结构由if、then、else、elif、fi关键词组成，进行多次条件匹配判断，多次判断中的任何一项在匹配成功后都会执行相应的预设命令 使用多分支的if条件语句判断用户输入的分数在哪个成绩区间内，输出如Excellent、Pass、Fail等提示信息 Linux系统中read用来读取用户输入信息的命令，能够把接收到的用户输入信息赋值给后面的指定变量，-p参数用于向用户显示一些提示信息： for条件循环语句for循环语句允许脚本一次性读取多个信息，然后逐一对信息进行操作处理。当要处理的数据有范围时，使用for循环语句就很适合： 使用for循环语句从列表文件中读取多个用户名，然后为其逐一创建用户账户并设置密码 首先创建用户名称的列表文件users.txt，每个用户名称单独一行： ​ 脚本中使用read命令读取用户输入的密码值，然后赋值给PASSWD变量，并通过-p参数向用户显示一段提示信息，告诉用户正在输入的内容即将作为账户密码 ​ 在执行该脚本后，会自动使用从列表文件users.txt中获取到所有的用户名称，然后逐一使用”id 用户名”命令查看用户的信息，并使用$?判断这条命令是否执行成功，也就是判断该用户是否已经存在 ​ /dev/null被称作Linux黑洞，把输出信息重定向到这个文件等同于删除数据(类似于没有回收功能的垃圾箱)，可以让用户的屏幕窗口保持简洁 ​ 执行批量创建用户的Shell脚本addusers.sh，在输入为账户设定的密码后将由脚本自动检查并创建这些账户 ​ 由于已经将多余的信息通过输出重定向符转移到了/dev/null黑洞文件中，因此在正常情况下屏幕窗口除了”用户账户创建成功”(Createsuccess)的提示后不会有其他内容 ​ Linux系统中/etc/passwd是用来保存用户账户信息的文件。如果想确认这个脚本是否成功创建了用户账户，可以打开这个文件，看其中是否有这些新创建的用户信息 ​ 脚本从文本中自动读取 主机列表，自动逐个测试主机是否在线 ​ 首先创建一个主机列表文件 ipaddrs.txt： ​ 让脚本从主机列表文件 ipaddrs.txt中自动读取IP地址并将其赋值给HLIST变量，从而通过判断ping命令执行后的返回值来逐个测试主机是否在线。脚本中出现的”$（命令）”是一种类似于转义字符中反引号`命令`的Shell操作符，效果同样是执行括号或双引号括起来的字符串中的命令 while条件循环语句​ while条件循环语句是让脚本根据某些条件来重复执行命令的语句，循环结构在执行前并不确定最终执行的次数，不同于for循环语句中有目标、有范围的使用场景 ​ while循环语句通过判断条件测试的真假来决定是否继续执行命令，若条件为真就继续执行，为假就结束循环 ​ 编写一个用来猜测数 值大小的脚本Guess.sh。该脚本使用$RANDOM变量调取一个随机的数值（范围为 0～32767），然后将这个随机数对1000进行取余操作，并使用expr命令取得其结果，再用这个数值与用户通过read命令输入的数值进行比较判断。这个判断语句分为3种情况，分别是判断用户输入的数值是等于、大于还是小于使用expr命令取得的数值 ​ while条件循环语句中的条件测试始终为true，因此判断语句会无限执行下去，直到用户输入的数值等于expr命令取得的数值后，才运行exit0命令终止脚本的执行 当条件为true(真)的时候，while语句会一直循环下去，只有碰到exit才会结束 case条件测试语句case语句是在多个范围内匹配数据 若匹配成功则执行相关命令并结束整个条件测试 如果数据不在所列出的范围内， 则会去执行星号（*）中所定义的默认命令 编写脚本Checkkeys.sh，提示用户输入一个字符并将其赋值给变量KEY，然后根据变量KEY的值向用户显示其值是字母、数字还是其他字符： 3.4 计划任务服务程序计划任务分为一次性计划任务与长期性计划任务 一次性计划任务一次性计划任务只执行一次，一般用于临时的工作需求。可以用at命令实现这种功能，只需要写成”at 时间”的形式就行 查看已设置好但还未执行的一次性计划任务，可以使用at-l命令；要想将其删除，可以使用”atrm任务序号” at命令中的参数及其作用: 使用at命令设置一次性计划任务时，默认采用的是交互式方法。如，将系统设置为在今晚23:30自动重启网站服务 上面设置了两条一样的计划任务，可以使用atrm命令删除其中一条： 特殊场景：把计划任务写入Shell脚本中，当用户激活该脚本后再开始倒 计时执行 使用”at now +2 MINUTE”的方式进行操作，表示2分钟（MINUTE）后执行这个任务，也可以将其替代成小时（HOUR）、日（DAY）、月（MONTH）等： 长期性计划任务有时候希望Linux系统能够周期性地、有规律地执行某些具体的任务，那么Linux系统中默认启用的crond服务简直再适合不过了 创建、编辑计划任务的命令为crontab -e，查看当前计划任务的令为crontab -l，删除某条计划任务的命令为crontab -r 以管理员的身份登录的系统，还可以在crontab命令中加上-u参数编辑他人的计划任务 crontab命令中的参数及其作用如下： 使用crond设置任务的参数格式，如果有些字段没有被设置，则需要使用星号(*)占位： 使用crond设置任务的参数字段说明： 1.在每周一、三、五的凌晨3:25都需要使用tar命令把某个网站的数据目录进行打包使其作为一个备份文件 可以使用crontab -e命令来创建计划任务，为自己创建计划任务时无须使用-u参数 crontab –e命令的具体实现效果和crontab -l命令的结果如下所示： 用逗号（,）来分别表示多个时间段，例如”8,9,12”表示8月、9月 和12月 用减号（-）来表示一段连续的时间周期（例如字段”日”的取值为”12-15”表示每月的12～15日） 用除号（/）表示执行任务的间隔时间（例如”*/2”表示每 隔 2分钟执行一次任务） 在crond服务中需要同时包含多条计划任务的命令语句，应每行仅写一条 在crond服务的计划任务参数中，所有命令一定要用绝对路径的方式来写 使用计划服务的注意事项： 1.在crond服务的配置参数中，一般会像Shell脚本那样以#号开头写上注释信息 2.计划任务中的”分”字段必须有数值，绝对不能为空或是*号，”日”和”星期”字段不能同 时使用，会发生冲突 3.删除crond计划任务直接使用crontab -e命令进入编辑界面，删除里面的文本信息即可。也可以使用 crontab -r 命令直接进行删除 4 用户身份与文件权限4.1 用户身份与能力用户身份Linux系统的管理员之所以是root，并不是因为它的名字叫root，而是因为该用户的身份号码(UID)的数值为0 Linux系统中UID就像身份证号码一样具有唯一性，可通过用户的UID值判断用户身份。在RHEL 8系统中，用户身份有以下这些： 1.管理员UID为0: 系统的管理员用户 2.系统用户UID为1～999: Linux系统为了避免因某个服务程序出现漏洞而被黑客提权至整台服务器，默认服务程序会由独立的系统用户负责运行，进而有效控制被破坏范围 3.普通用户UID从1000开始:由管理员创建的用于日常工作的用户 注明:UID是不能冲突的，而且管理员创建的普通用户的UID默认是从1000开始的(即使前面有闲置的号码) 用户组Linux系统引入了用户组的概念。通过使用用户组号码可以把多个用户加入到同一个组中，从而方便为组中的用户统一规划权限或指定任务 Linux系统创建每个用户时自动创建一个与其同名的基本用户组，基本用户组只有该用户一人 如果该用户以后被归纳到其他用户组，则这个其他用户组称之为扩展用户组 一个用户只有一个基本用户组，但是可以有多个扩展用户组，从而满足日常的工作需要 注明： 1.基本用户组像是原生家庭，是创建账号(出生)时自动生成的 2.扩展用户组则像工作单位，为了完成工作，需要加入到各个不同的群体中，这是需要手动添加的 id命令用于显示用户的详细信息，语法格式为”id 用户名” 查看用户的基本信息，如用户ID、基本组与扩展组GID，用于判别某个用户是否已经存在，以及查看相关信息 使用id命令查看名称为linuxprobe的用户信息： useradd命令用于创建新的用户账户，语法格式为”useradd [参数] 用户名” 该命令创建用户账户时，默认的用户家目录会被存放在/home目录中，默认的Shell解释器为/bin/bash，默认会创建一个与该用户同名的基本用户组 默认设置根据useradd命令参数修改： 使用useradd命令创建一个名称为linuxcool的用户，并使用id命令确认信息： 创建一个普通用户并指定家目录的路径、用户的UID以及Shell解 释器 /sbin/nologin是终端解释器中的一员，用户的解释器被设置为nologin代表该用户不能登录到系统中： groupadd命令用于创建新的用户组，语法格式为”groupadd [参数] 群组名” 使用如下命令创建一个用户组ronny： usermod命令用于修改用户的属性，语法格式为”usermod [参 数] 用户名” 用户的信息保存在/etc/passwd文件中，可以直接用文本编辑器来修改其中的用户参数项目，也可以用usermod命令修改已经创建的用户信息，比如用户的UID、基本扩展用户组、默认终端等 usermod命令的参数以及作用如下: 将用户linuxprobe加入到root用户组中，扩展组列表中会出现root用户组的字样: 使用-u参数修改linuxprobe用户的UID号码值： 把用户的解释器终端由默认的/bin/bash修改为 /sbin/nologin： ​ 将用户的终端设置成/sbin/nologin后用户马上就不能登录了（切换身份也不行），但用户依然可以被某个服务所调用，管理某个具体的服务。这好处是当黑客通过这个服务入侵成功后，破坏的范围也仅仅局限于这个特定的服务，而不能使用这个用户身份登录到整台服务器上，从而尽可能地把损失降至最小化。默认终端是/bin/bash passwd命令用于修改用户的密码、过期时间等，语法格式为”passwd [参数] 用户名” 普通用户只能使用passwd命令修改自己的系统密码，root管理员有权限修改其他所有人的密码 root管理员在Linux系统中修改自己或他人的密码时不需要验证旧密码 root管理员能够修改其他用户的密码，就表示其完全拥有该用户的管理权限 passwd命令中的参数以及作用如下： 1.修改自己的密码，只需要输入命令后敲击回车键即可： 2.修改其他人的密码，需要先检查当前是否为root管理员权限，然后在命令后指定要修改密码的那位用户的名称： 3.可使用passwd命令禁止某用户登录系 统，而不是将其删除。既保证了系统的安全，也避免了频繁添加、删除用户带来的麻烦： 4.在解锁时也要使用管理员的身份；如果普通用户也有锁定权限，系统肯定会乱成一锅粥： userdel命令用于删除已有的用户账户，语法格式为”userdel [参 数] 用户名” 可通过userdel命令删除某用户的所有信息。在执行删除操作时，该用户的家目录默认会保留下来，此时可以使用-r参数将其删除 userdel命令的参数以及作用如下： 1.删除一个用户时，一般建议保留其家目录的数据，以免有重要的数据被误删除 使用userdel命令时可以不加参数，写清要删除的用户名称就行： 2.虽然此时该用户已被删除，但家目录数据会继续存放在/home目录中，等确认未来不再使用时将其手动删除即可： 4.2 文件权限与归属Linux系统中每个文件都有归属的所有者和所属组，规定文件所有者、所属组以及其他人对文件所拥有可读（r）、可写（w）、可执行（x）等权限 1.对于一般文件来说： “可读”表示能够读取文件的实际内容、”可写”表示能够编辑、新增、修改、删除文件的实际内容、”可执行”则表示能够运行一个脚本程序 2.对于目录文件来说： “可读”表示能够读取目录内的文件列表、”可写”表示能够在目录内新增、删除、重命名文件、”可执行”则表示能够进入该目录* 可读、可写、可执行权限对应的命令在文件和目录上是有区别的，具体参考如下： 文件的可读、可写、可执行权限简写为r、w、x，亦可分别用数字4、2、1表示，文件所有者、文件所属组及其他用户权限之间无关联，如下所示： 文件权限转换示意图(字符与数字)： 文件属性信息： ​ 上图中包含了文件的类型、访问权限、所有者(属主)、所属组(属组)、占用磁盘大小、最后修改时间和文件名称等信息。该文件的类型为普通文件，所有者权限为可读、可写（rw-），所属组权限为可读（r–），其他人也只有可读权限（r–），文件的磁盘占用大小是34298字节，最近一次的修改时间为4月2日的0:23，文件的名称为install.log ​ 排在权限前面的减号（-）是文件类型（减号表示普通文件），常见的文件类型包括普通文件（-）、目录文件（d）、链接文件（l）、管道文件（p）、块设备文件（b）以及字符设备文（c） ​ 纯文本信息、服务配置信息、日志信息以及Shell脚本等都属于普通文件 ​ 块设备文件（b）和字符设备文件（c）一般是指硬件设备，如鼠标、键盘、光驱、硬盘等 4.3 文件的特殊权限​ 文件的rwx权限无法满足对安全和灵活性的需求，因此便有了SUID、SGID与SBIT的特殊权限位。这是对文件权限进行设置的特殊功能，可与一般权限同时使用，以弥补一般权限不能实现的功能 SUIDSUID是一种对二进制程序进行设置的特殊权限，能够让二进制程序的执行者临时拥有所有者的权限(仅对拥有执行权限的二进制程序有效) ​ 所有用户都可以执行passwd命令修改自己的用户密码，用户密码保存在/etc/shadow文件中。该文件默认权限是000，即除了root管理员以外，所有用户都没有查看或编辑该文件的权限 ​ 使用passwd命令加上SUID特殊权限位，就可让普通用户临时获得程序所有者的身份，把变更的密码信息写入到shadow文件中 ​ 查看passwd命令属性时发现所有者的权限由rwx变成了rws，x改变成s意味着文件被赋予了SUID 权限。如果原权限位上没有x执行权限，那么被赋予特殊权限后将变成大写的S SGIDSGID特殊权限有两种应用场景： 1.当对二进制程序进行设置时，能够让执行者临时获取文 件所属组的权限 2.当对目录进行设置时，则是让目录内新创建的文件自动继承该目录原有用户组的名称 SGID的第一种功能是参考SUID设计的，不同在于执行程序的用户获取的不再是文件所有者的临时权限，而是获取到文件所属组的权限 在早期的Linux系统中/dev/kmem是一个字符设备文件，用于存储内核程序要访问的数据，权限为： 除了root管理员或属于system组的成员外，所有用户都没有读取上述文件的权限。为了获取进程的状态信息，可在用于查看系统进程状态的ps命令文件上增加SGID特殊权限位 下面查看ps命令文件的属性信息： 由于ps命令被增加了SGID特殊权限位，所以当用户执行该命令时，也就临时获取到了system用户组的权限，从而顺利地读取到了设备文件。 每个文件都有其归属的所有者和所属组，当创建或传送一个文件后，这个文件就会自动归属于执行这个操作的用户（即该用户是文件的所有者） ​ 如果需要在一个部门内设置共享目录，让部门内的所有人员都能够读取目录中的内容，那么就可以在创建部门共享目录后，在该目录上设置SGID特殊权限位。这样，部门内的任何人员在里面创建的任何文件都会归属于该目录的所属组，而不再是自己的基本用户组。此时，用到的就是SGID的第二个功能，即在某个目录中创建的文件自动继承该目录的用户组（只可以对目录进行设置） ​ 使用上述命令设置好目录的777权限(确保普通用户可以向其中写入文件)，并为该目录设置了SGID特殊权限位后，就可以切换至一个普通用户，然后尝试在该目录中创建文件，并查看新创建的文件是否会继承新创建的文件所在的目录的所属组名称： 1.chmod命令用于设置文件的一般权限及特殊权限，语法格 式为”chmod [参数] 文件名” ​ 把一个文件的权限设置成其所有者可读可写可执行、所属组可读可写、其他人没有任何权限，则相应的字符法表示为rwxrw—-，其对应的数字法表示为760 ​ chown命令用于设置文件的所有者和所有组，语法格式为”chown 所有者:所有组 文件名” ​ chmod和chown命令是用于修改文件属性和权限的最常用命令，它们有一个特别的共性，就是针对目录进行操作时需要加上大写参数-R来表示递归操作，即对目录内所有的文件进行整体操作 ​ 使用”所有者:所有组”的格式把前面那个文件的所属信息修改一下，变更后的效果如下： SBITSBIT特殊权限位可确保用户只能删除自己的文件，而不能删除其他用户的文件 当对某个目录设置了SBIT粘滞位权限后，该目录中的文件就只能被其所有者执行删除操作了 RHEL 8系统中的/tmp作为一个共享文件的目录，默认设置了SBIT特殊权限位，因此除非是该目录的所有者，否则无法删除这里面的文件 ​ 与SUID和SGID权限显示方法不同，当目录被设置SBIT特殊权限位后，文件其他用户权限部分的x执行权限就会被替换成t或者T—原本有x执行权限则会写成t、原本没有x执行权限则会被写成T /tmp目录上的SBIT权限默认已经存在，这体现为”其他用户”权限字段的权限变为rwt： 文件能否被删除并不取决于自身的权限，而是看其所在目录是否有写入权限 赋予了这个test 文件最大的777权限(rwxrwxrwx)： ​ 切换到一个普通用户身份下，尝试删除这个由其他人创建的文件，就会发现，即便读、写、执行权限全开，但是由于SBIT特殊权限位的缘故，依然无法删除该文件： 使用chmod命令设置特 殊权限的参数如下： 切回root管理员身份，在家目录中创建一个名为linux的新目录，随后为其设置SBIT权限： 上述代码中的o+t参数是在一般权限已经设置完毕的前提下，又新增了一项特殊权限。 如果想将一般权限和特殊权限一起设置，有什么高效率的方法么？ ​ SUID、SGID与SBIT也有对应的数字表示法，分别为4、2、1。也就是说777还不是最大权限，最大权限应该是7777，其中第1个数字代表的是特殊权限位。既然知道了数字表示法是由”特殊权限+一般权限”构成的，以上面linux目录的权限为例，梳理一下计算方法： ​ 在rwxr-xr-t权限中，最后一位是t，说明该文件的一般权限为rwxr-xr-x，并带有SBIT特殊权限。对于可读（r）、可写（w）、可执行（x）权限计算即—rwxr-xr-x，即755，而SBIT特殊权限位是1，则合并后的结果为1755 ​ 如果权限是”rwsrwSr–”呢？大写S表示原先没有执行权限，因此一般权限为rwxrw-r–，将其转换为数字表示法后结果是764。带有的SUID和SGID特殊权限的数字法表示是4和2，得出结果是6，合并后的结果为6764 ​ 以5537为例。首先，特殊权限的5由4+1组成，意味着有SUID和SBIT。SUID和SGID的写法是，原先有执行权限则是小写s，如果没有执行权限则是大写S；而SBIT的写法则是，原先有执行权限是小写t，没有执行权限是大写T。一般权限的537进行字符转换后应为r-x-wxrwx，然后在此基础上增加SUID和SBIT特殊权限，合并后的结果是r-s-wxrwt 4.4 文件的隐藏属性​ Linux系统中的文件除了具备一般权限和特殊权限外，还有一种隐藏权限，默认情况下不能直接被用户发觉。有用户曾经在生产环境和RHCE考试题目中碰到过明明权限充足但却无法删除某个文件的情况，或者仅能在日志文件中追加内容而不能修改或删除内容的情况，这在一定程度上阻止了黑客篡改系统日志的图谋，因此这种”奇怪”的文件权限也保障了Linux系统的安全性 ​ 隐藏权限的专用设置命令是chattr，专用查看命令是lsattr chattr命令用于设置文件的隐藏权限，语法格式为”chattr [参 数] 文件名称” 如果想要把某个隐藏功能添加到文件上，需要在命令后面追加”+参数”，如果想要把某个隐藏功能移出文件，则需要追加”-参数” chattr命令中可供选择的隐藏权限参数如下： 先创建一个普通文件，然后立即尝 试删除(这个操作肯定会成功)： 再次新建一个普通文件，并设置”不允许 删除与覆盖”(+a参数)权限，然后再尝试删除文件： lsattr命令用于查看文件的隐藏权限，语法格式为”lsattr [参 数] 文件名称” Linux系统中文件的隐藏权限必须使用lsattr命令查看： 按照显示的隐藏权限的类型(字母)，使用chattr命令将其去掉： ​ 一般会将-a参数设置到日志文件(/var/log/messages)上，这样可在不影响系统正常写入日志的前提下，防止黑客擦除自己的作案证据。如果希望彻底保护某个文件，不允许任何人修改和删除它的话，不妨加上-i参数试试 ​ 要想彻底删除某个文件，可以使用-s参数保证其被删除后不可恢复—硬盘上的文件数据会被用零块重新填充，那就更保险了 4.5 文件访问控制列表​ 一般权限、特殊权限、隐藏权限都有一个共性：权限是针对某一类用户设置的，能够对很多人同时生效 ​ 如果希望对某个指定的用户进行单独的权限控制，就需要用到文件的访问控制列表(ACL)了 ​ 基于普通文件或目录设置ACL其实就是针对指定的用户或用户组设置文件或目录的操作权限，更加精准地派发权限 ​ 如果针对某个目录设置了ACL，则目录中的文件会继承其ACL权限 ​ 若针对文件设置了ACL，则文件不再继承其所在目录的ACL权限 ​ 先切换到普通用户，然后尝试进入root管理员的家目录中。在没有针对普通用户为root管理员的家目录设置ACL之前，其执行结果如下所示： setfacl命令用于管理文件的ACL权限规则，语法格式为 “setfacl [参数] 文件名称” ACL权限提供的是在所有者、所属组、其他人的读/写/执行权限之外的特殊权限控制 ​ 使用setfacl命令可以针对单一用户或用户组、单一文件或目录来进行读/写/执行权限的控制。其中，针对目录文件需要使用-R递归参数；针对普通文件则使用-m参数；删除某个文件的ACL使用-b参数 setfacl命令的常用参数如下： 例如：本来是无法进入/root目录的，现在为普通用户单独设置一下权限，随后切换到普通用户身份下，可以正常进入： ls命令看不到ACL信息，但可以看到文件权限的最后一个点(.)变成了加号(+)，这意味着该文件已经设置了ACL getfacl命令用于查看文件的ACL权限规则，语法格式为 “getfacl [参数] 文件名称” 设置ACL用的是setfacl命令；查看ACL用的是getfacl命令 使用getfacl命令显示在root管理员家目录上设置的 所有ACL信息： ACL权限还可以针对某个用户组进行设置 例如，允许某个组的用户都可以读写/etc/fstab 文件： 清空所有ACL权限使用-b参数；删除某一条指定的权 限用-x参数： ​ ACL权限的设置都是立即且永久生效的，不需要再编辑什么配置文件，这一点特别方便。但是，这也带来了一个安全隐患。如果不小心设置错了权限，就会覆盖掉文件原始的权限信息，并且永远都找不回来了 ​ 在备份/home目录上的ACL权限时，可使用-R递归参数，这样不仅能够把目录本身的权限进行备份，还能将里面的文件权限也自动备份 ​ getfacl在备份目录权限时不能使用绝对路径的形式，先切换到最上层根目录，然后再进行操作 ACL权限的恢复使用的是–restore参数。由于在备份时已经指定是对/home目录进行操作，所以不需要写对应的目录名称，它能够自动找到要恢复的对象： 4.6 su命令与sudo服务​ su命令可以解决切换用户身份的需求，使得当前用户在不退出登录的情况下，顺畅地切换到其他用户，比如从root管理员切换至普通用户： ​ 上面的su命令与用户名之间有一个减号(-)，意味着完全切换到新的用户，即把环境变量信息也变更为新用户的相应信息，而不是保留原始的信息。建议在切换用户身份时添加减号(-) ​ 当从root管理员切换到普通用户时是不需要密码验证的，而从普通用户切换成root管理员需要进行密码验证；这也是一个必要的安全检查： ​ 像上面这样使用su命令后，普通用户可以完全切换到root管理员的身份来完成相应工作，但这将暴露root管理员的密码，从而增大了系统密码被黑客获取的概率，这并不是最安全的方案。 ​ 使用sudo命令把特定命令的执行权限赋予指定用户，既可保证普通用户能够完成特定的工作，也可以避免泄露root管理员密码 ​ 授权原则:在保证普通用户完成相应工作的前提下，尽可能少地赋予额外的权限 ​ sudo命令用于给普通用户提供额外的权限，语法格式为”sudo [参数] 用户名” ​ 使用sudo命令可以给普通用户提供额外的权限来完成原本只有root管理员才能完成的任务，可以限制用户执行指定的命令，记录用户执行过的每一条命令，集中管理用户与权限(/etc/sudoers），以及可以在验证密码后的一段时间无须让用户再次验证密码 ​ 常见的sudo命令的可用参数如下： ​ 可使用sudo命令提供的visudo命令来配置用户权限： ​ visudo命令用于编辑、配置用户sudo的权限文件，语法格式为”visudo [参数]“ ​ 这是一条会自动调用vi编辑器来配置/etc/sudoers权限文件的命令，能够解决多个用户同时修改权限而导致的冲突问题。不仅如此，visudo命令还可以对配置文件内的参数进行语法检查，并在发现参数错误时进行报错提醒。这要比用户直接修改文件更友好、安全、方便。 注明:使用visudo命令配置权限文件时，其操作方法与Vim编辑器中用到的方法完全一致 谁可以使用：允许使用的主机 = (以谁身份) 可执行命令的列表 谁可以使用：稍后要为哪位用户进行命令授权 允许使用的主机：可以填写ALL表示不限制来源的主机，亦可填写如192.168.10.0/24这样的网段限制来源地址，使得只有从允许网段登录时才能使用sudo命令 以谁的身份：可以填写ALL表示系统最高权限，也可以是另外一位用户的名字 可执行命令的列表：可以填写ALL表示不限制命令，亦可填写如/usr/bin/cat这样的文件名称限制命令列表，多个命令文件之间用逗号(,)间隔 ​ 在Linux系统中配置服务文件时，新增参数的位置不建议太靠上，以免新填写的参数在执行时失败，导致一些必要的服务功能没有成功加载。建议在配置文件中找一下相似的参数，然后在相邻位置进行新的修改，或者在文件的中下部位置进行添加后修改 ​ 填写完毕后记得要先保存再退出，然后切换至指定的普通用户身份，此时就可以用sudo -l命令查看所有可执行的命令(下面的命令中，验证的是普通用户的密码，而不是root管理员的密码)： ​ 普通用户肯定不能看到root管理员的家目录(/root)中的文件信息，但只需要在想执行的命令前面加上sudo命令就行了： ​ 如果让某个用户只能使用root管理员的身份执行指定的命令，需要给出该命令的绝对路径，否则系统会识别不出来。可以先使用whereis命令找出命令所对应的保存路径： ​ 使用visudo命令继续编辑权限文件，将原新增的参数作如下修改，多个命令之间用逗号(,)间隔： ​ 再次切换到指定的普通用户，然后尝试正常查看某个系统文件的内容，此时系统提示没有权限(Permission denied)。再使用sudo命令就能顺 利地查看文件内容了： ​ 每次执行sudo命令后都会要求验证一下密码。可以添加NOPASSWD参数，使用户下次再执行sudo命令时不用密码验证： ​ 当切换到普通用户后再执行命令时，就不用再频繁地验证密码了： ​ visudo命令只有root管理员才可以执行，普通用户使用时会提示权限不足 5 存储结构与管理硬盘5.1 一切从”/“开始Linux系统中目录、字符设备、套接字、硬盘、光驱、打印机等都被抽象成文件形式，即”Linux 系统中一切都是文件” 在Windows操作系统中，想要找到一个文件，要依次进入该文件所在的磁盘分区(盘符)，然后再进入该分区下的具体目录，最终找到这个文件 ​ Linux系统中并不存在C、D、E、F等盘符，Linux系统中的一切文件都是从”根”目录（/）开始的，并按照文件系统层次标准(FHS)采用倒树状结构来存放文件，以及定义了常见目录的用途 ​ Linux系统中的文件和目录名称是严格区分大小写的。root、rOOt、Root、rooT均代表不同的目录，并且文件名称中不得包含斜杠（/） ​ Linux系统中的文件存储结构如下： ​ FHS是用户在Linux系统中存储文件时需要遵守的规则，用于指导用户应该把文件保存到什么位置，以及告诉用户应该在何处找到所需的文件。 ​ Linux系统中最常见的目录以及所对应的 存放内容如下： 路径指的是如何定位到某个文件： ​ 分为绝对路径与相对路径。绝对路径指的是从根目录（/）开始写起的文件或目录名称，而相对路径则指的是相对于当前路径的写法 5.2 物理设备的命名规则Linux系统中一切都是文件，硬件设备也不例外。既然是文件，就必须有文件名称 系统内核中的udev设备管理器会自动把硬件名称规范起来，目的是让用户通过设备文件的名字可以猜出设备大致的属性以及分区信息等 udev设备管理器的服务会一直以守护进程的形式运行并侦听内核发出的信号来管理/dev目录下的设备文件 Linux系统中常见的硬件设备及其文件名称如下： ​ 一般的硬盘设备都是以”/dev/sd”开头。而一台主机上可以有多块硬盘，因此系统采用a～z来代表26块不同的硬盘(默认从a开始分配) ​ 硬盘的分区编号也很有讲究：主分区或扩展分区的编号从1开始，到4结束、逻辑分区从编号5开始 ​ /dev目录中sda设备之所以是a并不是由插槽决定的，而是由系统内核的识别顺序来决定的，恰巧很多主板的插槽顺序就是系统内核的识别顺序，因此才会被命名为/dev/sda ​ 使用iSCSI网络存储设备时就会发现主板上第二个插槽是空着的，但系统却能识别到/dev/sdb这个设备就是这个道理 ​ 分区的数字编码不一定是强制顺延下来的，也有可能是手工指定的。因此sda3只能表示是编号为3的分区，不能判断sda设备上已经存在了3个分区 ​ /dev/sda5这个设备文件名称包含的些信息如下所示： ​ /dev/目录中保存的是硬件设备文件、sd表示的是存储设备、a表示系统中同类接口中第一个被识别到的设备、5表示这个设备是一个逻辑分区。”/dev/sda5”表示的是”这是系统中第一块被识别到的硬件设备中分区编号为5的逻辑分区的设备文件” ​ 硬盘设备是由大量的扇区组成的，每个扇区的容量为512字节。其中第一个扇区最重要，它里面保存着主引导记录与分区表信息。就第一个扇区来讲，主引导记录需要占用446字节，分区表占用64字节，结束符占用2字节；分区表中每记录一个分区信息就需要16字节，这样一来最多只有4个分区信息可以写到第一个扇区中，这4个分区 就是4个主分区 ​ 第一个扇区中的数据信息如下所示： ​ 每块硬盘最多只能创建出4个分区明显不合情理也不够用。为了解决分区个数不够的问题，可以将第一个扇区的分区表中16字节(原本要写入主分区信息)的空间(扩展分区)拿出来指向另外一个分区。也就是说，扩展分区其实并不是一个真正的分区，而更像是一个占用16字节分区表空间的指针(指向另外一个分区的指针)。这样一来，用户一般会选择使用3个主分区加1个扩展分区的方法，然后在扩展分区中创建出数个逻辑分区，从而来满足多分区(大于4个)的需求 ​ 主分区、扩展分区、逻辑分区可以如下去规划： ​ 注明:扩展分区严格地讲不是一个实际意义的分区，而仅仅是一个指向其他分区的指针，这种指针结构将形成一个单向链表。因此扩展分区自身不能存储数据，用户需要在其指向的对应分区(逻辑分区)上进行操作 ​ /dev/hdc8代表这是第3块IDE设备(现在比较少见)中编号为8的逻辑分区 ​ 云主机中还会看到类似于/dev/vda、/dev/vdb这样的设备。以vd开头的设备叫作Virtio设备，简单来说就是一种虚拟化设备。像KVM、Xen这种虚拟机监控器(Hypervisor)默认就都是这种设备 5.3 文件系统与数据资料​ 用户在硬件存储设备中执行的文件建立、写入、读取、修改、转存与控制等操作都是依靠文件系统来完成的。文件系统的作用是合理规划硬盘，以保证用户正常的使用需求 Linux系统支持数十种文件系统，最常见的文件系统如下所示： ​ 1.Ext2:Linux系统的第一个商业级文件系统，基本沿袭了UNIX文件系统的设计标准。但由于不包含日志读写功能，数据丢失的可能性很大，因此能不用就不用，顶多建议用于SD存储卡或U盘 ​ 2.Ext3:一款日志文件系统，能够在系统异常宕机时避免文件系统资料丢失，并能自动修复数据的不一致与错误。然而，当硬盘容量较大时，所需的修复时间也会很长，也不能100%保证资料不会失 ​ 3.Ext4:Ext3的改进版本，RHEL 6系统中默认的文件管理系统，支持的存储容量高达1EB（1EB=1,073,741,824GB），且能够有无限多的子目录。Ext4文件系统能够批量分配block(块)，从而极大地提高了读写效率。现在很多主流服务器也会使用Ext4文件系统 ​ 4.XFS:高性能的日志文件系统，RHEL 7/8中默认的文件管理系统。它的优势在发生意外宕机后尤其明显，即可以快速地恢复可能被破坏的文件，而且强大的日志功能只需花费极低的计算和存储性能，支持的最大存储容量为18EB ​ RHEL 7/8系统比较大的变化就是使用了XFS作为文件系统，XFS虽然在性能方面比Ext4有所提升，但绝不是压倒性的，因此XFS文件系统最卓越的亮点当属可支持高达18EB的存储容量 ​ 拿到一块新的硬盘存储设备后，先需要分区，然后再格式化文件系统，最后才能挂载并正常使用。硬盘的分区操作取决于需求和硬盘大小；也可以不进行分区，但是必须对硬盘进行格式化处理 ​ 日常需要保存在硬盘中的数据太多了，因此Linux系统中有一个名为superblock的”硬盘地图”。Linux并不是把文件内容直接写入到这个”硬盘地图”里面，而是在里面记录着整个文件系统的信息。如果把所有的文件内容都写入到这里面，它的体积将变得非常大，而且文件内容的查询与写入速度也会变得很慢。Linux只是把每个文件的权限与属性记录在inode中，每个文件占用一个独立的inode表格，该表格的大小默认为128字节，里面记录着如下信息： ​ 文件的实际内容保存在block块中(大小一般是1KB、2KB或4KB)，一个inode的默认大小仅为128字节，记录一个block消耗4字节。当文件的inode被写满后，Linux系统会自动分配出一个block，专门用于像inode那样记录其他block块的信息，这样把各个block块的内容串到一起，就能够让用户读到完整的文件内容了。对于存储文件内容的block块，有下面两种常见的情况(以4KB大小的block 为例进行说明)： ​ 1.情况1：文件很小(1KB)，但依然会占用一个block，因此会潜在地浪费3KB ​ 2.情况2：文件很大(5KB)，会占用两个block(5KB−4KB后剩下的1KB也要占用一个block) ​ Linux内核中的软件层为用户程序提供了一个虚拟文件系统（Virtual FileSystem，VFS）接口，用户实际上在操作文件时就是统一对这个虚拟文件系统进行操作了 ​ 下图为VFS的架构示意图。从中可见，实际文件系统在VFS下隐藏了自己的特性和细节，这样用户在日常使用时会觉得”文件系统都是一样的”，也就可以随意使用各种命令在任何文件系统中进行各种操作了(如使用cp命令复制文件)： ​ VFS像一个翻译官。不需要知道对方的情况，只要告诉VFS想进行的操作是什么，它就会自动判断对方能够听得懂什么指令，然后翻译并交代下去。这可以让用户不用操心这些”小事情”，专注于自己的操作 5.4 挂载硬件设备mount命令​ 用于挂载文件系统，格式为”mount 文件系统 挂载目录”。可用的参数及作用如下所示： ​ 挂载是在使用硬件设备前所执行的最后一步操作。只需使用mount命令把硬盘设备或分区与一个目录文件进行关联，然后就能在这个目录中看到硬件设备中的数据了 ​ 新的Linux系统一般不需要使用-t参数来指定文件系统的类型，Linux系统会自动进行判断 ​ mount中的-a参数会在执行后自动检查/etc/fstab文件中有无被疏漏挂载的设备文件，如果有，则进行自动挂载操作 ​ 把设备/dev/sdb2挂载到/backup目录，只需要在mount命令中填写设备与挂载目录参数就行，系统会自动判断要挂载文件的类型，命令如下： ​ 工作中挂载一块网络存储设备，该设备的名字可能会变来变去，再写为sdb就不太合适了。推荐用UUID(通用唯一识别码)进行挂载操作。UUID是一串用于标识每块独立硬盘的字符串，具有唯一性及稳定性，特别适合用来挂载网络设备，使用blkid命令可得知独立硬盘的UUID blkid命令blkid命令用于显示设备的属性信息，语法格式为”blkid [设备 名]“ blkid命令查询设备UUID的示例如下： 有了设备的UUID值之后，就可以用它挂载网络设备： ​ 执行mount命令后就能立即使用文件系统了，但系统在重启后挂载就会失效，如果想让硬件设备和目录永久地进行自动关联，就必须把挂载信息按照指定的填写格式”设备文件 挂载目录 格式类型 权限选项 是否备份 是否自检”(各字段的意义如下)写入到/etc/fstab文件中 将文件系统为xfs的硬件设备/dev/sdb1在开机后自动挂载到/backup目录上，并保持默认权限且无须开机自检，需要在/etc/fstab文件中写入下面的信息： 由于后面需要使用系统镜像制作Yum/DNF软件仓库，我提前把光盘设备挂载到/media/cdrom目录中。光盘设备的文件系统格式是iso9660： 写入到/etc/fstab文件中的设备信息并不会立即生效，需使用mount-a参数进行自动 挂载： df命令用于查看已挂载的磁盘空间使用情况，语法格式为 “df -h” ​ 查看当前系统中设备的挂载情况可使用df命令，不仅能列出系统中正在使用的设备有哪些，还可以用-h参数便捷地对存储容量进行”进位”操作。例如，在遇到10240K的时候会自动进位写成10M ​ 网络存储设备建议在fstab文件挂载信息中加上_netdev参数。加上后系统会等联网成功后再尝试挂载这块网络存储设备，从而避免了开机时间过长或失败的情况： umount命令​ 挂载文件系统是为了使用硬件资源，卸载文件系统则意味不再使用硬件的设备资源。既然挂载操作就是把硬件设备与目录两者进行关联的动作，那么卸载操作只需要说明想要取消关联的设备文件或挂载目录的其中一项即可，一般不需要加其他额外的参数。 ​ umount命令用于卸载设备或文件系统，语法格式为”umount [设备文件/挂载目录]“ ​ 如果当前处于设备所挂载的目录，系统会提示该设备繁忙，此时只需退出到其他目录后再尝试一次就行了 ​ 如果系统中硬盘特别多，分区特别多，就可以用lsblk命令以树状图的形式列举一下了​ lsblk命令用于查看已挂载的磁盘的空间使用情况，输入该命令后按回车键执行即可。 5.5 添加硬盘设备​ 首先需要在虚拟机中模拟添加入一块新的硬盘存储设备，然后再进行分区、格式化、挂载等操作，最后通过检查系统的挂载状态并真实地使用硬盘来验证硬盘设备是否成功添加 具体的操作步骤如下： ​ 1.首先把虚拟机系统关机，稍等几分钟会自动返回到虚拟机管理主界面，然后单击”编辑虚拟机设置”选项，在弹出的界面中单击”添加”按钮，新增一块硬件设备 ​ 2.选择想要添加的硬件类型为”硬盘”，然后单击”下一步”按钮就可以了 ​ 3.选择虚拟硬盘的类型为SATA，并单击”下一步”按钮，这样虚拟机中的设备名称过一会儿后应该为/dev/sdb ​ 4.选中”创建新虚拟磁盘”单选按钮，再次单击”下一步”按钮 ​ 5.将”最大磁盘大小”设置为默认的20GB。这个数值是限制这台虚拟机所使用的最大硬盘空间，而不是立即将其填满，因此默认20GB就很合适了。单击”下一步”按钮** ​ 6.设置磁盘文件的文件名和保存位置(默认设置即可，无须修改)，直接单击”完成”按钮 ​ 7.将新硬盘添加好后就可以看到设备信息了。不需要做任何修改，直接单击”确定”按钮后就可以启虚拟机了 ​ 在虚拟机中模拟添加了硬盘设备后就应该能看到抽象后的硬盘设备文件了。按照udev服务命名规则，第二个被识别的SATA设备应该会被保存为/dev/sdb、第三个为/dev/sdc，这个就是硬盘设备文件了。但在开始使用该硬盘之前还需要进行分区操作，例如从中取出一个2GB的分区设备以供后面的操作使用 fdisk命令用于新建、修改及删除磁盘的分区表信息，语法格 式为”fdisk 磁盘名称” ​ 在Linux系统中，管理硬盘设备最常用的方法就当属fdisk命令。提供了集添加、删除、转换分区等功能于一身的”一站式分区服务”。不过与前面讲解的直接写到命令后面的参数不同，这条命令的参数(如下)是交互式的一问一答的形式，因此在管理硬盘设备时特别方便，可根据需求动态调整。 使用fdisk命令管理/dev/sdc硬盘设备 在看到提示信息后输入参数p来查看硬盘设备内已有的分区信息，其中包括了硬盘的容量大小、扇区个数等信息： 输入参数n尝试添加新的分区 系统会要求用户是选择继续输入参数p来创建主分区，还是输入参数e来创建扩展分区。这里输入参数p来创建一个主分区： ​ 在确认创建一个主分区后，系统要求用户先输入主分区的编号。主分区的编号范围是1～4，因此这里输入1即可。接下来系统会提示定义起始的扇区位置，这不需要改动，敲击回车键保留默认设置即可，系统会自动计算出最靠前的空闲扇区的位置。最后，系统会要求定义分区的结束扇区位置，这其实就是要去定义整个分区的大小是多少。不用去计算扇区的个数，只需要输入+2G即可创建出一个容量为2GB的硬盘分区。 ​ 再次使用参数p查看硬盘设备中的分区信息。就能看到一个名称为/dev/sdc1、起始扇区位置为2048、结束扇区位置为4196351的主分区了。敲击参数w后按回车键，分区信息写入成功 分区信息中第6个字段的Id值是一个编码，用于标识该分区的作用，可帮助用户快速了解该分区的作用，一般没必要修改 使用l参数查看一下磁盘编码都有哪些，在6.6节进行SWAP操作时再修改： ​ 上述步骤执行完后，Linux系统会自动把这个硬盘主分区抽象成/dev/sdc1设备文件。可以使用file命令查看该文件的属性，但有些时候系统并没有自动把分区信息同步给Linux内核，而且这种情况似乎还比较常见(但不能算作严重的bug) ​ 输入partprobe命令手动将分区信息同步到内核，而且一般推荐连续两次执行该命令，效果会更好。如果使用这个命令都无法解决问题，那么就重启计算机吧 如果硬件存储设备没有进行格式化，则Linux系统无法得知怎么在其上写入数据。因此，在对存储设备进行分区后还需要进行格式化操作。在Linux系统中用于格式化操作的命令是mkfs。这条命令很有意思，因为在Shell终端中输入mkfs名后再敲击两下用于补齐命令的Tab键，会有如下的效果：\\*\\* ​ mkfs命令把常用的文件系统名称用后缀的方式保存成了多个命令文件，用起来也非常简单—mkfs.文件类型名称。将分区为xfs的文件系统进行格式化，则命令应为mkfs.xfs /dev/sdc1 ​ 创建一个用于挂载设备的挂载点目录，使用mount命令将存储设备与挂载点进行关联，最后使用df -h命令来查看挂载状态和硬盘使用量 信息 du命令查看分区或目录所占用的磁盘容量大小，语法格式 为”du -sh 目录名称” 在Linux系统中可以使用du -sh /*命令查看在Linux系统根目录下所有一级目录分别占用的空间大小，在1s之内就能找到哪个目录占用的空间最多： 先从某些目录中复制过来一批文件，然后查看这些文件总共占用了多大的容量： 使用mount命令挂载的设备文件会在系统下一次重启的时候失效。如果想让这个设备文件的挂载永久有效，则需要把挂载的信息写入配置文件中： 5.6 添加交换分区​ 交换(SWAP)分区是通过在硬盘中预先划分一定的空间，然后把内存中暂时不常用的数据临时存放到硬盘中，以便腾出物理内存空间让更活跃的程序服务来使用的技术，设计目的是为了解决真实物理内存不足的问题。通俗来讲就是让硬盘帮内存分担压力。但由于交换分区毕竟是通过硬盘设备读写数据的，速度肯定要比物理内存慢，所以只有当真实的物理内存耗尽后才会调用交换分区的资源。 ​ 交换分区的创建与挂载与使用存储设备的过程非常相似。交换分区的划分建议：在生产环境中，交换分区的大小一般为真实物理内存的1.5～2倍。此处取一个大小为5GB的主分区作为交换分区资源： 在上面的操作结束后，就得到了一个容量为5GB的新分区。然后修改硬盘的标识码，将其改成82(Linux swap)以方便以后知道它的作用： 备注:敲击w参数退出分区表编辑工具 mkswap命令用于对新设备进行交换分区格式化，语法格式为 “mkswap 设备名称” swapon命令用于激活新的交换分区设备，语法格式为”swapon 设备名称” 使用swapon命令把准备好的SWAP硬盘设备正式挂载到系统中。使用free -m命令查看交换分区的大小变化(由2047MB增加到7167MB)： 为了让新的交换分区设备在重启后依然生效，需按下面的格式将相关信息写入配置文件中： 5.7 磁盘容量配额​ root管理员使用磁盘容量配额服务来限制某位用户或某个用户组针对特定文件夹可以使用的最大硬盘空间或最大文件个数，一旦达到这个最大值就不再允许继续使用。使用quota技术进行磁盘容量配额管理，从而限制用户的硬盘可用容量或所能创建的最大文件个数。quota技术还有软限制和硬限制的功能 ​ 软限制:当达到软限制时会提示用户，但仍允许用户在限定的额度内继续使用 ​ 硬限制:当达到硬限制时会提示用户，且强制终止用户的操作 ​ RHEL 8系统中已经安装了quota磁盘容量配额服务程序包，但存储设备却默认没有开启对quota技术的支持，需要手动编辑配置文件(/etc/fstab)并重启(reboot)一次系统，让系统中的启动目录(/boot)能支持quota磁盘配额技术 ​ 早期的Linux系统想让硬盘设备支持quota磁盘容量配额服务，使用的是usrquota参数，RHEL 7/8系统使用的是uquota参数。在重启系统后使用mount命令查看，即可发现/boot目录已经支持quota磁盘配额技术了： ​ 创建一个用于检查quota磁盘容量配额效果的用户tom，并针对/boot目录增加其他人的写权限，保证用户能够正常写入数据： xfs_quota命令用于管理设备的磁盘容量配额，语法格式为”xfs_quota [参数] 配额 文件系统” ​ 这是专门针对XFS文件系统来管理quota磁盘容量配额服务而设计的命令。-c参数用于以参数的形式设置要执行的命令；-x参数是专家模式，让运维人员能够对quota服务进行更多复杂的配置 ​ 使用xfs_quota命令设置用户tom对/boot目录的quota磁盘容量配额。具体的限额控制包括： 1.硬盘使用量的软限制和硬限制分别为3MB和6MB 2.创建文件数量的软限制和硬限制分别为3个和6个 ​ 上面使用的参数分为两组，分别是isoft/ihard与bsoft/bhard，Linux系统中每个文件都会使用一个独立的inode信息块来保存属性信息，一个文件对应一个inode信息块，所以isoft和ihard就是通过限制系统最大使用的inode个数来限制文件数量。bsoft和bhard则是代表文件所占用的block大小，也就是文件占用的最大容量的总统计 ​ soft是软限制，超过该限制后也只是将操作记录写到日志中，不对用户行为进行限制。hard是硬限制，一旦超过系统就会马上禁止，用户再也不能创建或新占任何的硬盘容量 ​ 当配置好上述各种软硬限制后，尝试切换到一个普通用户，分别尝试创建一个体积为5MB和8MB 的文件。可以发现，在创建8MB的文件时受到了系统限制： edquota命令用于管理系统的磁盘配额，语法格式为”edquota [参 数] 用户名” ​ 为用户设置了quota磁盘容量配额限制后，可使用edquota命令按需修改限额的数值。其中，-u参数表示要针对哪个用户进行设置；-g参数表示要针对哪个用户组进行设置，如下所示： ​ edquota命令会调用Vi或Vim编辑器来让root管理员修改要限制的具体细节 ​ 把用户tom的硬盘使用量的硬限额从5MB提升到8MB： 5.8 VDO(虚拟数据优化)​ 一种通过压缩或删除存储设备上的数据来优化存储空间的技术。VDO是红帽公司收购了Permabit公司后获取的新技术，2019-2020年前后，多次在RHEL 7.5/7.6/7.7上进行测试，最终随RHEL8系统正式公布。VDO技术的关键就是对硬盘内原有的数据进行删重操作，类似于使用的网盘服务，第一次正常上传文件时速度特别慢，第二次上传相同的文件时仅作为一个数据指针，几乎可以达到”秒传”的效果，无须再多占用一份空间，也不用再漫长等待。除了删重操作，VDO技术还可以对日志和数据库进行自动压缩，进一步减少存储浪费的情况 VDO针对各种类型文件的压缩效果如下所示： ​ VDO技术支持本地存储和远程存储，可作为本地文件系统、iSCSI或Ceph存储下的附加存储层使用。红帽公司在VDO介绍页面中提到，在部署虚拟机或容器时，建议采用逻辑存储与物理存储为10∶1的比例进行配置，即1TB物理存储对应10TB逻辑存储；而部署对象存储时(如使用Ceph)则采用逻辑存储与物理存储为3∶1的比例进行配置，即使用1TB物理存储对应3TB逻辑存！简而言之，VDO技术能省空间！ ​ 其一，公司服务器上已有的dm-crypt之类的技术是可以与VDO技术兼容的，但记得要先对卷进行加密再使用VDO。因为加密会使重复的数据变得有所不同，因此删重操作无法实现。要始终记得把加密层放到VDO之下，如下图所示： ​ 其二，VDO技术不可叠加使用，1TB的物理存储提升成10TB的逻辑存储没问题，但是再用10TB翻成100TB就不行了 ​ 把虚拟机关闭，添加一块容量为20GB的新SATA硬盘进来，开机后就能看到这块名称为/dev/sdd 的新硬盘了： ​ RHEL/CentOS8系统中默认启用了VDO技术。VDO技术是红帽公司自己的技术，兼容性自然没得说。如果所用的系统没有安装VDO，用dnf命令即可完成安装： 首先，创建一个全新的VDO卷 ​ 新添加进来的物理设备就是使用vdo命令管理的，其中name参数代表新的设备卷的名称；device参数代表由哪块磁盘进行制作；vdoLogicalSize参数代表制作后的设备大小。依据红帽公司推荐的原则，20GB硬盘将翻成200GB的逻辑存储： ​ 创建成功后，使用status参数查看新建卷的概述信息： ​ 由上可见，输出信息中包含了VDO卷创建的时间、主机名、版本、是否压缩(Compression)及是否删重(Deduplication)等关键信息。 ​ 接下来，对新建卷进行格式化操作并挂载使用。 ​ 新建的VDO卷设备会被存放在/dev/mapper目录下，并以设备名称命名，对它操作就行。另外，挂载前可以用udevadm settle命令对设备进行一次刷新操作，避免刚才的配置没有生效： 如果想查看设备的实际使用情况，使用vdostats命令即可。human-readable参数的作用是将存储容量自动进位(比如，显示20G而不是20971520K)： 这里显示的Size是实际物理存储的空间大小(即20.0GB是硬盘大小)，如果想看逻辑存储空间，可以用df命令进行查看： 随便复制一个大文件过来，看看占用了多少容量，以及空间节省率(Space saving)是多少： 原先448MB的文件这次只占用了不到100MB 的容量，空间节省率也从18%提升到了55% ​ VDO设备卷在创建后会一直存在，但需要手动编辑/etc/fstab文件后才能在下一次重启后自动挂载生效，为我们所用。对于这种逻辑存储设备，其实不太建议使用/dev/mapper/storage作为设备名进行挂载。不如试试前面所说的UUID： ​ 打开/etc/fstab文件，把对应的字段填写完整。建议再加上_netdev参数，表示等系统及网络都启动后再挂载VDO设备卷，以保证万无一失。 5.9 软硬方式链接​ 在Windows系统中，快捷方式是指向原始文件的一个链接文件，可以让用户从不同的位置来访问原始的文件；原文件一旦被删除或剪切到其他地方，会导致链接文件失效。但在Linux系统中可不太一样。 ​ Linux系统中存在软链接和硬链接两种不同的类型： ​ 软链接:(符号链接)，仅仅包含所链接文件的名称和路径，很像一个记录地址的标签。当原始文件被删除或移动后，新的链接文件也会随之失效，不能被访问。可以针对文件、目录设置软链接，跨文件系统进行链接也不是问题。从这一点来看，它与Windows系统的”快捷方式”具有一样的性质。 ​ 硬链接:可将它理解为一个”指向原始文件block的指针”，系统会创建出一个与原来一模一样的inode信息块。所以，硬链接文件与原始文件其实是一模一样的，只是名字不同。每添加一个硬链接，该文件的inode个数就会增加1；而且只有当该文件的inode个数为0时，才算彻底将它删除。换言之，由于硬链接实际上是指向原文件block的指针，因此即便原始文件被删除，依然可以通过硬链接文件来访问。由于技术的局限性，不能跨分区对目录文件进行硬链接。 ln命令用于创建文件的软硬链接，语法格式为”ln [参数]原始文件 名 链接文件名” ln命令的可用参数以及作用如下所示。在使用ln命令时，是否添加-s参数，将创建出性质不同的两种”快捷方式” 为了更好地理解软链接、硬链接的不同性质，先创建出一个文件，为其创建一个软链接： 原始文件名为old，新的软链接文件名为new。删掉原始文件后，软链接文件立刻就无法读取了： ​ 对原始文件old创建一个硬链接，相当于针对原始文件的硬盘存储位置创建了一个指针。这样一来，新创建的这个硬链接就不再依赖于原始文件的名称等信息，也不会因为原始文件的删除而导致无法读取了。同时可以看到创建硬链接后，原始文件的硬盘链接数量增加到了2 ​ 创建的硬链接文件竟然会让文件属性第二列的数字变成了2，这个数字表示的是文件的inode信息块的数量。已经非常肯定地知道，即便删除了原始文件，新的文件也会一如既往地可以读取，因为只有当文件inode数量被”清零”时，才真正代表这个文件被删除了 6 使用RAID与LVM磁盘阵列技术6.1 RAID(独立冗余磁盘阵列)​ RAID技术通过把多个硬盘设备组合成一个容量更大、安全性更好的磁盘阵列，并把数据切割成多个区段后分别存放在各个不同的物理硬盘设备上，利用分散读写技术来提升磁盘阵列整体的性能，同时把多个重要数据的副本同步到不同的物理硬盘设备上，从而起到了非常好的数据冗余备份效果 ​ 现代企业更看重的是RAID技术所具备的冗余备份机制以及带来的硬盘吞吐量的提升。也就是说，RAID不仅降低了硬盘设备损坏后丢失数据的几率，还提升了硬盘设备的读写速度，所以它在绝大多数运营商或大中型企业中得到了广泛部署和应用 ​ 出于成本和技术的考虑，需要针对不同的需求在数据可靠性及读写性能上做出权衡，制定出满足各自需求的不同方案。目前已有的RAID磁盘阵列的方案至少有十几种，RAID0、RAID 1、RAID 5 与RAID 10是4种最常见的方案。这4种方案的对比如下所示，其中 n 代表硬盘总数。 RAID 0​ RAID0技术把多块物理硬盘设备(至少两块)通过硬件或软件的方式串联在一起，组成一个大的卷组，并将数据依次写入各个物理硬盘中。这样一来，在最理想的状态下，硬盘设备的读写性能会提升数倍，但是若任意一块硬盘发生故障，将导致整个系统的数据都受到破坏。通俗来说，RAID 0技术能够有效地提升硬盘数据的吞吐速度，但是不具备数据备份和错误修复能力。如下图所示，数据被分别写入到不同的硬盘设备中，即硬盘A和硬盘B设备会分别保存数据资料，最终实现提升读取、写入速度的效果。 RAID 1​ 尽管RAID 0技术提升了硬盘设备的读写速度，但它是将数据依次写入到各个物理硬盘中。也就是说，它的数据是分开存放的，其中任何一块硬盘发生故障都会损坏整个系统的数据。因此，如果生产环境对硬盘设备的读写速度没有要求，而是希望增加数据的安全性时，就需要用到RAID 1技术了 ​ 在下图RAID 1技术示意图中可以看到，它是把两块以上的硬盘设备进行绑定，在写入数据时，是将数据同时写入到多块硬盘设备上(可以将其视为数据的镜像或备份)。当其中某一块硬盘发生故障后，一般会立即自动以热交换的方式来恢复数据的正常使用 ​ 考虑到在进行写入操作时因硬盘切换带来的开销，因此RAID 1的速度会比RAID 0有微弱地降低。但在读取数据的时候，操作系统可以分别从两块硬盘中读取信息，因此理论读取速度的峰值可以是硬盘数量的倍数。只要保证有一块硬盘稳定运行，数据就不会出现损坏的情况，可靠性较高 ​ RAID1技术虽然十分注重数据的安全性，但是因为是在多块硬盘设备中写入了相同的数据，因此硬盘设备的利用率得以下降。从理论上来说，图中所示的硬盘空间的真实可用率只有50%，由3块硬盘设备组成的RAID 1磁盘阵列的可用率只有33%左右；以此类推。而且，由于需要把数据同时写入到两块以上的硬盘设备，这无疑也在一定程度上增大了系统计算功能的负载 RAID 5​ 如下图所示，RAID5技术是把硬盘设备的数据奇偶校验信息保存到其他硬盘设备中。RAID 5磁盘阵列中数据的奇偶校验信息并不是单独保存到某一块硬盘设备中，而是存储到除自身以外的其他每一块硬盘设备上。这样的好处是，其中任何一设备损坏后不至于出现致命缺陷。图中Parity部分存放的就是数据的奇偶校验信息。RAID 5技术实际上没有备份硬盘中的真实数据信息，而是当硬盘设备出现问题后通过奇偶校验信息来尝试重建损坏的数据。RAID这样的技术特性”妥协”地兼顾了硬盘设备的读写速度、数据安全性与存储成本问题 ​ RAID 5最少由3块硬盘组成，使用的是硬盘切割(Disk Striping)技术。相较于RAID 1级别，好处就在于保存的是奇偶校验信息而不是一模一样的文件内容，所以当重复写入某个文件时，RAID 5级别的磁盘阵列组只需要对应一个奇偶校验信息就可以，效率更高，存储成本也会随之降低 RAID 10​ RAID 5技术是出于硬盘设备的成本问题对读写速度和数据的安全性能有了一定的妥协，但是大部分企业更在乎的是数据本身的价值而非硬盘价格，因此在生产环境中主要使用RAID 10技术 ​ RAID 10技术是RAID 1+RAID 0技术的一个”组合体”。如图所示，RAID 10技术需要至少4块硬盘来组建，其中先分别两两制作成RAID 1磁盘阵列，以保证数据的安 全性；然后再对两个RAID 1磁盘阵列实施RAID 0技术，进一步提高硬盘设备的读写速度。这样从理论上来讲，只要坏的不是同一阵列中的所有硬盘，那么最多可以损坏50%的硬盘设备而不丢失数据。由于RAID 10技术继承了RAID 0的高读写速度和RAID1的数据安全性，在不 考虑成本的情况下RAID 10的性能也超过了RAID 5，因此当前成为广泛使用的一种存储技术 ​ 由于RAID 10是由RAID 1和RAID 0组成的，因此正确的叫法是”RAID 一零”，而不是”RAID 十” ​ RAID 10是先对信息进行分割，然后再两两一组制作镜像。 也就是先将RAID 1作为最低级别的组合，然后再使用RAID 0技术将RAID 1磁盘阵列组合到一起，将它们视为”一整块”硬盘。而 RAID 01则相反，它是先将硬盘分为两组，然后使用RAID 0作为最低级别的组合，再将这两组RAID 0硬盘通过RAID 1技术组合到一起 ​ RAID 10技术和RAID 01技术的区别非常明显。在RAID 10中，任何一块硬盘损坏都不会影响到数据安全性，其余硬盘均会正常运作。但在RAID01中，只要有任何一块硬盘损坏， 最低级别的RAID 0磁盘阵列马上会停止运作，这可能造成严重隐患。所以RAID 10远比RAID 01常见，很多主板甚至不支持RAID 01 部署磁盘阵列​ 首先，在虚拟机中添加4块硬盘设备来制作一个RAID 10磁盘阵列，硬盘要用SCSI或SATA接口的类型， 大小默认20GB ​ mdadm命令用于创建、调整、监控和管理RAID设备，语法格式为”mdadm 参数硬盘名称” ​ mdadm命令常用参数及作用如下： 使用mdadm命令创建RAID 10，名称为”/dev/md0” udev是Linux系统内核中用来给硬件命名的服务，通过命名规则猜测到第二个SCSI存储设备的名称会是/dev/sdb，然后依此类推 ​ 如果使用10、50、100个硬盘来部署RAID磁盘阵列呢？​ 此时，就需要使用mdadm中的参数了。其中，-C参数代表创建一个RAID阵列卡；-v参数显示创建的过程，同时在后面追加一个设备名称/dev/md0，这样/dev/md0就是创建后的RAID 磁盘阵列的名称；-n 4参数代表使用4块硬盘来部署这个RAID磁盘阵列；而-l 10参数 则代表RAID 10方案；最后再加上4块硬盘设备的名称就搞定了 ​ 初始化过程大约需要1分钟左右，可以用-D 参数进行查看。也可以用-Q参数查看简要信息： ​ 4块20GB大小的硬盘组成的磁盘阵列组，可用空间只有 39.97GB ​ RAID 10技术通过两两一组硬盘组成的RAID 1磁盘阵列保证了数据的可靠性，每一份数据都会被保存两次，导致硬盘存在50%的使用率和50%的冗余率。这样一来，80GB的硬盘容量也就只有一半了 ​ 等两三分钟后，把制作好的RAID磁盘阵列格式化为Ext4格式： ​ 创建挂载点，将硬盘设备进行挂载操作： ​ 查看/dev/md0磁盘阵列设备的详细信息，确认RAID级别(Raid Level)、阵列大小(Array Size)和总硬盘数(Total Devices)都是否正确： ​ 如果想让创建好的RAID磁盘阵列一直提供服务，需将信息添加到/etc/fstab文件中，确保每次重启后RAID磁盘阵列都是有效的。 损坏磁盘阵列及修复​ 生产环境中部署RAID10磁盘阵列是为了提高存储设备的IO读写速度及数据的安全性，但因为我们的硬盘设备是在虚拟机中模拟出来的，所以对于读写速度的改善可能并不直观 ​ 在确认有一块物理硬盘设备出现损坏而不能再继续正常使用后，应该使用mdadm命令将其移除，然后查看RAID磁盘阵列的状态，可以发现状态已经改变： 使用-f参数是让硬盘模拟损坏的效果。为了能够彻底地将故障盘移除，还要再执行一步操作： ​ 在RAID 10级别的磁盘阵列中，当RAID 1磁盘阵列中存在一个故障盘时并不影响RAID 10磁盘阵列的使用。当购买了新的硬盘设备后再使用mdadm命令予以替换即可，在此期间可以在/RAID目录中正常地创建或删除文件。由于我们是在虚拟机中模拟硬盘，所以先重启系统，然后再把新的硬盘添加到RAID磁盘阵列中 ​ 更换硬盘后再次使用-a参数进行添加操作，系统默认会自动开始数据的同步工作。使用-D参数即可看到整个过程和进度(用百分比表示)： 磁盘阵列+备份盘​ RAID 10磁盘阵列中最多允许50%的硬盘设备发生故障，但极端情况是同一RAID 1磁盘阵列中的硬盘设备若全部损坏，也会导致数据丢失。在RAID 10磁盘阵列中，如果RAID 1中的某一块硬盘出现了故障，而我们正在前往修复的路上，恰巧该 RAID1磁盘阵列中的另一块硬盘设备也出现故障，那么数据就被彻底丢失了。可以使用RAID备份盘技术来预防这类事故。该技术的核心理念就是准备一块足够大的硬盘，这块硬盘平时处于闲置状态，一旦RAID磁盘阵列中有硬盘出现故障后则会马上自动顶替上去 ​ 为了避免多个实验之间相互发生冲突，需要保证每个实验的相对独立性，需要将虚拟机还原到初始状态。由于刚才已经演示了RAID 10磁盘阵列的部署方法，现在来看一下RAID 5的部署效果。部署RAID 5磁盘阵列时，至少需要用到3块硬盘，还需要再加一块备份硬盘(也叫热备盘)，所以总计需要在虚拟机中模拟4块硬盘设备 ​ 创建一个RAID 5磁盘阵列+备份盘。在下面的命令中，参数-n 3代表创建这个RAID 5磁盘阵列所需的硬盘数，参数-l 5代表RAID的级别，而参数-x1则代表有一块备份盘 ​ 当查看/dev/md0(RAID 5磁盘阵列名称)磁盘阵列的时候，能看到有一块备份盘在等待中了 将部署好的RAID 5磁盘阵列格式化为Ext4文件格式，然后挂载到目录上，之后就能够使用了： 由3块硬盘组成的RAID5磁盘阵列，其对应的可用空间是n-1，也就是40GB。热备盘的空间不计算进来，平时完全就是在”睡觉”，只有在意外出现时才会开始工作 ​ 把硬盘设备/dev/sde移出磁盘阵列，然后迅速查看 /dev/md0磁盘阵列的状态，就会发现备份盘已经被自动顶替上去并开始了数据同步。RAID中的这种备份盘技术非常实用，可以在保证RAID磁盘阵列数据安全性的基础上进一步提高数据可靠性 如果后面想再添加一块热备盘进来，使用-a参数就可以了 删除磁盘阵列生产环境中RAID磁盘阵列部署后一般不会被轻易停用，但还是可能会遇到。 1.将所有的磁盘都设置成停用状态： 2.逐一移除出去 3.如果着急，也可以用”mdadm /dev/md0 -f /dev/sde -r /dev/sde”这一条命令搞定 ​ 在早期版本的服务器中，命令中的-f和-r不能一起使用，保守起见，还是一步步地操作吧。​ 将所有的硬盘都移除后，再来查看磁盘阵列组的状态： 4.继续停用整个RAID磁盘阵列，工作就彻底完成了 在老版本的服务器中，使用–stop参数后依然会保留设备文件。这很明显是没有处理干净，再执行”mdadm -remove /dev/md0”命令即可 6.2 LVM(逻辑卷管理器)​ 随着实际需求的变化调整硬盘分区的大小时，会受到硬盘”灵活性”的限制。这时就需要用另外一项硬盘设备资源管理技术了—逻辑卷管理器(Logical Volume Manager，LVM)。LVM允许用户对硬盘资源进行动态调整 ​ LVM是Linux系统用于对硬盘分区进行管理的一种机制，理论性较强，创建初衷是为了解决硬盘设备在创建分区后不易修改分区大小的缺陷。尽管对传统的硬盘分区进行强制扩容或缩容从理论上来讲是可行的，但是却可能造成数据的丢失。LVM是在硬盘分区和文件系统之间添加了一个逻辑层，它提供了一个抽象的卷组，可以把多块硬盘进行卷组合并。这样一来，用户不必关心物理硬盘设备的底层架构和布局，就可以实现对硬盘分区的动态调整。​ LVM的技术架构如下图所示 ​ 举例说明: 比如小明家里想吃馒头，但是面粉不够了，于是妈妈从隔壁老王家、老李家、老张家分别借来一些面粉，准备蒸馒头吃。首先需要把这些面粉(物理卷[Physical Volume，PV])揉成一个大面团(卷组[Volume Group]，VG)，然后再把这个大面团分割成一个个小馒头(逻辑卷[Logical Volume，LV])，而且每个小馒头的重量必须是每勺面粉(基本单元[Physical Extent，PE])的倍数 ​ 在日常使用中，如果卷组(VG)的剩余容量不足，可以随时将新的物理卷(PV)加入到里面，进行不断地扩容。逻辑卷管理器使用流程示意图，如下图所示： 物理卷处于LVM中的最底层，可以将其理解为物理硬盘、硬盘分区或者RAID磁盘阵列。卷组建立在物理卷之上，一个卷组能够包含多个物理卷，而且在卷组创建之后也可以继续向其中添加新的物理卷。逻辑卷是用卷组中空闲的资源建立的，并且逻辑卷在建立后可以动态地扩展或缩小空间。这就是 LVM的核心理念 部署逻辑卷​ 在生产环境中无法在最初时就精确地评估每个硬盘分区在日后的使用情况，因此会导致原先分配的硬盘分区不够用。比如，伴随着业务量的增加，用于存放交易记录的数据库目录的体积也随之增加；因为分析并记录用户的行为从而导致日志目录的体积不断变大，这些都会导致原有的硬盘分区在使用上捉襟见肘。而且，还存在对较大的硬盘分区进行精简缩容的情况 ​ 可以通过部署LVM来解决上述问题。部署时，需要逐个配置物理卷、卷组和逻辑卷 ​ 常用的部署命令如下表所示： ​ 为避免多个实验之间相互发生冲突，请将虚拟机还原到初始状态，并重新添加两块新硬盘设备，如下图所示。然后开机。 ​ 在虚拟机中添加两块新硬盘设备的目的是为了更好地演示LVM理念中用户无须关心底层物理硬盘设备的特性。先对这两块新硬盘进行创建物理卷的操作，可将该操作简单理解成让硬盘设备支持LVM技术，或者理解成是把硬盘设备加入到LVM技术可用的硬件资源池中，然后对这两块硬盘进行卷组合并，卷组的名称允许由用户自定义。接下来，根据需求把合并后的卷组切割出一个约为150MB的逻辑卷设备，最后把这个逻辑卷设备格式化成Ext4文件系统后挂载使用。下文将对每一个步骤做一些简单的描述： 第一步:让新添加的两块硬盘设备支持LVM技术 第二步:把两块硬盘设备加入到storage卷组中，然后查看卷组的状态 第三步:切割出一个约为150MB的逻辑卷设备 ​ 对逻辑卷进行切割时有两种计量单位。第一种是以容量为单位，所使用的参数为-L。例如，使用-L 150M生成一个大小为150MB的逻辑卷。另外一种是以基本单元的个数为单位，所使用的参数为-l。每个基本单元的大小默认为4MB。例如，使用-l 37可以生成一个大小为37×4MB=148MB的逻辑卷。 第四步:把生成好的逻辑卷进行格式化，然后挂载使用 ​ Linux系统会把LVM中的逻辑卷设备存放在/dev设备目录中(实际上是个快捷方式)，同时会以卷组的名称来建立一个目录，其中保存了逻辑卷的设备映射文件(即/dev/卷组名称/逻辑卷名称) ​ 如果使用了逻辑卷管理器，则不建议用XFS文件系统，因为XFS文件系统自身就可以使用xfs_growfs命令进行磁盘扩容。虽然不比LVM灵活，但起码也够用。在实测阶段发现，在有一些服务器上，XFS与LVM的兼容性并不好 第五步:查看挂载状态，并写入配置文件，使其永久生效 一个小问题:刚刚明明写的是148MB，怎么这里只有140MB了呢？因为硬件厂商的制造标准是 1GB=1,000MB、1MB＝1,000KB、1KB＝1,000B，而计算机系统的算法是1GB=1,024MB、1MB＝1,024KB、1KB＝1,024B，因此有3%左右的”缩水”是正常情况 扩容逻辑卷​ 卷组是由两块硬盘设备共同组成的。用户在使用存储设备时感知不到设备底层的架构和布局，更不用关心底层是由多少块硬盘组成的，只要卷组中有足够的资源，就可以一直为逻辑卷扩容。扩容前记得卸载设备和挂载点的关联 第1步:把上一个实验中的逻辑卷vo扩展至290MB 第2步:检查硬盘的完整性，确认目录结构、内容和文件内容没有丢失。一般情况下没有报错，均为正常情况 第3步:重置设备在系统中的容量。刚刚是对LV(逻辑卷)设备进行了扩容操作，但系统内核还没有同步到这部分新修改的信息，需要手动进行同步 第4步:重新挂载硬盘设备并查看挂载状态 查看卷组大小： 缩小逻辑卷 相较于扩容逻辑卷，在对逻辑卷进行缩容操作时，数据丢失的风险更大。在生产环境中执行相应操作时，一定要提前备份好数据。另外，Linux系统规定，在对LVM逻辑卷进行缩容操作之前，要先检查文件系统的完整性(保证数据的安全)。在执行缩容操作前先把文件系统卸载掉 第1步:检查文件系统的完整性 第2步:通知系统内核将逻辑卷vo的容量减小到120MB 第3步:将LV(逻辑卷)的容量修改为120MB 缩容操作是先通知系统内核自己想缩小逻辑卷，如果在执行resize2fs命令后系统没有报错，再正式操作 第4步:重新挂载文件系统并查看系统状态 逻辑卷快照​ LVM具备”快照卷”功能，该功能类似于虚拟机软件的还原时间点功能。例如，对某一个逻辑卷设备做一次快照，如果日后发现数据被改错了，就可以利用之前做好的快照卷进行覆盖还原。LVM的快照卷功能有两个特点： 1.快照卷的容量必须等同于逻辑卷的容量 2.快照卷仅一次有效，一旦执行还原操作后则会被立即自动删除 在正式操作前，先看看VG(卷组)中的容量是否够用： 通过卷组输出信息可以看到，卷组中已经使用了120MB的容量，空闲容量还有39.88GB。用重定向往逻辑卷设备所挂载的目录中写入一个文件： ​ 第1步:使用-s参数生成一个快照卷，使用-L参数指定切割的大小，需要与要做快照的设备容量保持一致。另外，还需要在命令后面写上是针对哪个逻辑卷执行的快照操作，稍后数据也会还原到这个相应的设备上 第2步:在逻辑卷所挂载的目录中创建一个100MB的垃圾文件，然后再查看快照卷的状态。可以发现存储空间的占用量上升了 第3步:为了校验快照卷的效果，需要对逻辑卷进行快照还原操作。在此之前记得先卸载掉逻辑卷设备与目录的挂载 lvconvert命令用于管理逻辑卷的快照，语法格式为”lvconvert [参数]快照卷名称” 使用lvconvert命令能自动恢复逻辑卷的快照，在早期的RHEL/CentOS 5版本中要写全格 式:”–mergesnapshot”，从RHEL 6到RHEL8，已经允许用户只输入–merge参数进行操作 了，系统会自动分辨设备的类型 第4步:快照卷会被自动删除掉，并且刚刚在逻辑卷设备被执行快照操作后再创建出来的100MB的垃圾文件也被清除了 删除逻辑卷​ 生产环境中重新部署LVM或不再需要使用LVM时，需要执行LVM的删除操作。为此，需要提前备份好重要的数据信息，然后依次删除逻辑卷、卷组、物理卷设备，这个顺序不可颠倒 第1步:取消逻辑卷与目录的挂载关联，删除配置文件中永久生效的设备参数 第2步:删除逻辑卷设备，需要输入y来确认操作 第3步:删除卷组，此处只写卷组名称即可，不需要设备的绝对路径 第4步:删除物理卷设备 上述操作执行完毕之后，再执行lvdisplay、vgdisplay、pvdisplay命令查看LVM的信息时就不会再看到相关信息了(前提是上述步骤的操作是正确的) 7 使用iptables与firewalld防火墙7.1 防火墙管理工具​ 相较于企业内网，外部的公网环境更加恶劣，罪恶丛生。公网与企业内网之间充当保护屏障的防火墙(下图)虽然有软件或硬件之分，但主要功能都是依据策略对穿越防火墙自身的流量进行过滤。防火墙策略可以基于流量的源目地址、端口号、协议、应用等信息来定制，防火墙使用预先定制的策略规则监控出入的流量，若流量与某一条策略规则相匹配，则执行相应的处理，反之则丢弃。这样一来，就能够保证仅有合法的流量在企业内网和外部公网之间流动了 ​ 从RHEL 7系统开始，firewalld防火墙正式取代了iptables防火墙。iptables与firewalld都不是真正的防火墙，都只是用来定义防火墙策略的防火墙管理工具而已；或者说，只是一种服务。iptables服务会把配置好的防火墙策略交由内核层面的netfilter网络过滤器来处理，而firewalld服务则是把配置好的防火墙策略交由内核层面的nftables包过滤框架来处理。当前在Linux系统中其实存在多个防火墙管理工具，旨在方便运维人员管理Linux系统中的防火墙策略 7.2 iptables​ 早期的Linux系统默认使用的是iptables防火墙管理服务来配置防火墙。尽管新型的firewalld防火墙管理服务已被投入使用多年，但是大量的企业在生产环境中依然出于各种原因而继续使用 iptables 策略与规则链​ 防火墙按照从上到下的顺序来读取配置的策略规则，在找到匹配项后立即结束匹配工作并去执行匹配项中定义的行为(即放行或阻止)。如果在读取完所有的策略规则之后没有匹配项，就去执行默认的策略。防火墙策略规则的设置一般有两种：”通”(放行)和”堵”(阻止)。当防火墙的默认策略为拒绝时(堵)，就要设置允许规则(通)，否则谁都进不来；如果防火墙的默认策略为允许，就要设置拒绝规则，否则谁都能进来，防火墙也就失去了防范的作用 ​ iptables服务把用于处理或过滤流量的策略条目称之为规则，多条规则可以组成一个规则链，而规则链则依据数据包处理位置的不同进行分类，具体如下： 1.在进行路由选择前处理数据包(PREROUTING) 2.处理流入的数据包(INPUT) 3.处理流出的数据包(OUTPUT) 4.处理转发的数据包(FORWARD) 5.在进行路由选择后处理数据包(POSTROUTING) 从内网向外网发送的流量一般都是可控且良性的，因此使用最多的就是INPUT规则链，该规则链可以增大黑客人员从外网入侵内网的难度。 ​ iptables服务的术语中分别有ACCEPT（允许流量通过）、REJECT（拒绝流量通过）、LOG（记录日志信息）、DROP（拒绝流量通过）。”允许流量通过”和”记录日志信息”都比较好理解，就DROP来说，它是直接将流量丢弃而且不响应；REJECT则会在拒绝流量后再回复一条”信息已经收到，但是被扔掉了”信息，从而让流量发送方清晰地看到数据被拒绝的响应信息。 注明:在红帽认证考试中必须用REJECT进行拒绝，好让用于判分的脚本得到反应，以获得分值。而在工作中更多建议用DROP进行拒绝，这可以隐藏服务器的运行状态。 ​ 把Linux系统中的防火墙策略设置为REJECT动作后，流量发送方会看到端口不可达的响应： ​ 把Linux系统中的防火墙策略修改成DROP动作后，流量发送方会看到响应超时的提醒。但是流量发送方无法判断流量是被拒绝，还是接收方主机当前不在线： 基本命令参数​ 根据OSI七层模型的定义，iptables属于数据链路层的服务，所以可以根据流量的源地址、目的地址、传输协议、服务类型等信息进行匹配；一旦匹配成功，iptables就会根据策略规则所预设的动作来处理这些流量。防火墙策略规则的匹配顺序是从上到下的，因此要把较为严格、优先级较高的策略规则放到前面，以免发生错误。 常用的iptables命令参数如下： 实验1:在iptables命令后添加-L参数查看已有的防火墙规则链 实验2:在iptables命令后添加-F参数清空已有的防火墙规则链 实验3:把INPUT规则链的默认策略设置为拒绝 防火墙策略规则的设置无非有两种方式：”通”和”堵”。当把INPUT链设置为默认拒绝后，就要往里面写入允许策略了，否则所有流入的数据包都会被默认拒绝掉。规则链的默认策略拒绝动作只能是DROP，而不能是REJECT 实验4:向INPUT链中添加允许ICMP流量进入的策略规则 运维工作中，经常会使用ping命令来检查对方主机是否在线，而向防火墙的INPUT规则链中添加一条允许ICMP流量进入的策略规则就默认允许了这种ping命令检测行为 实验5:删除INPUT规则链中刚刚加入的那条策略(允许ICMP流量)，并把默认策略设置为允许 使用-F参数会清空已有的所有防火墙策略；使用-D参数可以删除某一条指定的策略，因此更加安全和准确 实验6:将INPUT规则链设置为只允许指定网段的主机访问本机的22端口，拒绝来自其他所有主机的流量 要对某台主机进行匹配，可直接写出它的IP地址；如需对网段进行匹配，则需要写为子网掩码的形式(比如192.168.10.0/24) 防火墙策略规则是按照从上到下的顺序匹配的，因此一定要把允许动作放到拒绝动作前面，否则所有的流量就将被拒绝掉，从而导致任何主机都无法访问我们的服务。这里提到的22号端口是ssh服务使用的 在设置完上述INPUT规则链之后，使用IP地址在192.168.10.0/24网段内的主机访问服务器的22端口，效果如下： 使用IP地址在192.168.20.0/24网段内的主机访问服务器的22端口（虽网段不同，但已确认可以相互通信），效果如下： 由上可以看到，提示连接请求被拒绝了(Connection failed) 实验7:向INPUT规则链中添加拒绝所有人访问本机12345端口的策略规则 实验8:向INPUT规则链中添加拒绝192.168.10.5主机访问本机80端口(Web服务)的策略规则 实验9:向INPUT规则链中添加拒绝所有主机访问本机1000～1024端口的策略规则 ​ 添加防火墙策略时，使用的是-I参数，默认会把规则添加到最上面的位置，因此优先级是最高的。如果需要添加一条最后”兜底”的规则，就用-A参数。这两个参数的效果差别还是很大的： 使用iptables命令配置的防火墙规则默认会在系统下一次重启时失效，如果想让配置的防火墙策略永久生效，还要执行保存命令： 如果服务器是5/6/7版本的话，对应的保存命令应该是： 7.3 firewalld​ RHEL 8系统中集成了多款防火墙管理工具，其中firewalld(动态防火墙管理器)服务是默认的防火墙配置管理工具，拥有基于CLI(命令行界面)和基于GUI(图形用户界面)的两种管理方式 ​ firewalld支持动态更新技术并加入了区域(zone)的概念。区域就是firewalld预先准备了几套防火墙策略集合(策略模板)，用户可以根据生产场景的不同而选择合适的策略集合，从而实现防火墙策略之间的快速切换 ​ firewalld中常见的区域名称(默认为public)以及相应的策略规则如表： 终端管理工具命令行终端是一种极富效率的工作方式，firewall-cmd是firewalld防火墙配置管理工具的CLI(命令行界面)版本。参数一般都是以”长格式”来提供的 firewall-cmd命令中使用的参数以及作用： 使用firewalld配置的防火墙策略默认为运行时(Runtime)模式，又称为当前生效模式，会随着系统的重启而失效。想让配置策略一直存在，需要使用永久(Permanent)模式了，方法是在用firewall-cmd命令正常设置防火墙策略时添加–permanent参数，这样配置的防火墙策略就可以永久生效了。但永久生效模式设置的策略只有在系统重启之后才能自动生效。如果想让配置的策略立即生效，需要手动执行firewall-cmd –reload命令 Runtime:当前立即生效，重启后失效 Permanent:当前不生效，重启后生效 配置时需注意是Runtime模式还是Permanent 模式 实验1：查看firewalld服务当前所使用的区域 配置防火墙策略前，必须查看当前生效的是哪个区域，否则配置的防火墙策略将不会立即生效 实验2：查询指定网卡在firewalld服务中绑定的区域 服务器大多不止有一块网卡 一般充当网关的服务器有对公网和对内网两块网卡，两块网卡在审查流量时所用的策略也不一致 根据网卡针对的流量来源，为网卡绑定不同的区域，实现对防火墙策略的灵活管控 实验3:把网卡默认区域修改为external，并在系统重启后生效 实验4:把firewalld服务的默认区域设置为public 默认区域也叫全局配置，指对所有网卡都生效的配置，优先级较低 当前默认区域为public，而ens160网卡的区域为external。此时便是以网卡的区域名称为准 默认区域是一种通用的政策 实验5:启动和关闭firewalld防火墙服务的应急状况模式 如果想在1s的时间内阻断一切网络连接，有什么好办法呢？ “拔掉网线”是一个物理级别的高招。但是，如果人在北京，服务器在异地呢？panic紧急模式在这个时候就派上用场了。 使用–panic-on 参数会立即切断一切网络连接，而使用–panic-off则会恢复网络连接。 紧急模式会切断一切网络连接，因此在远程管理服务器时，在按下回车键前一定要三思 实验6:查询SSH和HTTPS协议的流量是否允许放行 工作中可不使用–zone参数指定区域名称，firewall-cmd命令会自动依据默认区域进行查询，从而减少用户输入量。 但如果默认区域与网卡所绑定的不一致时，就会发生冲 突，因此规范写法的zone参数是一定要加的** 实验7:把HTTPS协议的流量设置为永久允许放行，并立即生效 默认情况下进行的修改都属于Runtime模式，即当前生效而重启后失效，因此在工作和考试中尽量避免使用 在使用–permanent参数时，则是当前不会立即看到效果，而在重启或重新加载后方可生效 在添加了允许放行 HTTPS 流量的策略后，查询当前模式策略，发现依然是不允许放行HTTPS协议的流量： 不重启服务器的话，就用–reload参数： 实验8:把HTTP协议的流量设置为永久拒绝，并立即生效 由于在默认情况下HTTP协议的流量就没有被允许，所以会有”Warning:NOT_ENABLED: http”这样的提示信息，因此对实际操作没有影响 实验9:把访问8080和8081端口的流量策略设置为允许，但仅限当前生效 实验10:把原本访问本机888端口的流量转发到22端口，要且求当前和长期均有效 第9章介绍的SSH远程控制协议是基于TCP/22端口传输控制指令的，如果想让用户通过其他端口号也能访问ssh服务，就可以试试端口转发技术了 通过这项技术，新的端口号在收到用户请求后会自动转发到原本服务的端口上，使得用户能够通过新的端口访问到原本的服务 使用firewall-cmd命令实现端口转发的格式有点长： 上述命令中的目标IP地址一般是服务器本机的IP地址： 在客户端使用ssh命令尝试访问192.168.10.10主机的888端口，访问成功： 实验11:富规则的设置。它的优先级在所有的防火墙策略中也是最高的 富规则也叫复规则，表示更细致、更详细的防火墙策略配置，可以针对系统服务、端口号、源地址和目标地址等诸多信息进行更有针对性的策略配置 优先级在所有的防火墙策略中也是最高的。在firewalld服务中配置一条富规则，使其拒绝192.168.10.0/24网段的所有用户访问本机的ssh服务(22端口)： 在客户端使用ssh命令尝试访问192.168.10.10主机的ssh服务(22端口)： 图形管理工具firewall-config是firewalld防火墙配置管理工具的GUI(图形用户界面)版本，几乎可以实现所有以命令行来执行的操作。 默认情况下系统并没有提供firewall-config命令，需要自行用dnf命令进行安装，所以需要先配置软件仓库。 首先将虚拟机的”CD/DVD(SATA)光盘选项设置为”使用ISO映像文件”，然后选择已经下载好的系统镜像，下图所示： 把光盘设备中的系统镜像挂载到/media/cdrom目录： 为了软件仓库一直为用户提供服务，严谨的做法是将系统镜像文件的挂载信息写到/etc/fstab文件中： 最后，使用Vim文本编辑器创建软件仓库的配置文件。与之前版本的系统不同，RHEL 8需要配置两个软件仓库([BaseOS]与[AppStream])，且缺一不可： 正确配置完软件仓库文件后，可以开始用yum或dnf命令安装软件了 这两个命令在实际操作中除了名字不同外，执行方法完全一致，可随时用yum替代dnf 安装firewalld图形用户界面工具： 终端执行firewall-config打开firewall-config工具的界面如下图所示，其功能具体如下： 1：选择运行时（Runtime）或永久（Permanent）模式的配置 2：可选的策略集合区域列表 3：常用的系统服务列表 4：主机地址的黑白名单 5：当前正在使用的区域 6：管理当前被选中区域中的服务 7：管理当前被选中区域中的端口 8：设置允许被访问的协议 9：设置允许被访问的端口 10：开启或关闭SNAT（源网络地址转换）技术 11：设置端口转发策略 12：控制请求icmp服务的流量 13：管理防火墙的富规则 14：被选中区域的服务，若勾选了相应服务前面的复选框，则表示允许与之相关的流量 15：firewall-config工具的运行状态 除上图中列出的功能，还有用于将网卡与区域绑定的Interfaces选项，以及用于将IP地址与区域绑定的Sources选项 在使用firewall-config工具配置完防火墙策略之后，无须进行二次确认，只要有修改内容，就自动进行保存： 1.将当前区域中请求http服务的流量设置为允许放行，但仅限当前生效。具体配置如下图所示： 2.添加一条防火墙策略规则，使其放行访问8080～8088端口(TCP协议)的流量，并将其设置为永久生效，以达到系统重启后防火墙策略依然生效的目的。在按照下图1所示的界面配置完毕之后，还需要在Options菜单中单击Reload Firewalld命令，让配置的防火墙策略立即生效(见图2)。这与在命令行中使用–reload 参数的效果一样。讲解firewall-config工具的功能时，提到了SNAT(Source Network Address Translation，源网络地址转换)技术 SNAT是一种为解决IP地址匮乏而设计的技术，可以使得多个内网中的用户通过同一个外网IP接入Internet 该技术的应用非常广泛，比如，当通过家中的网关设备（无线路由器） 访问本书配套站点www.linuxprobe.com时，就用到了SNAT技术。 放行访问8080～8088端口的流量： 让配置的防火墙策略规则立即生效： 网络中不使用SNAT技术(图1)和使用SNAT技术(图2)时的情况。在图1所示的局域网中有多台PC，如果网关服务器没有应用SNAT技术，则互联网中的网站服务器在收到PC的请求数据包，并回送响应数据包时，将无法在网络中找到这个私有网络的IP地址，所以PC也就收不到响应数据包了。在图2所示的局域网中，由于网关服务器应用了SNAT技术，所以互联网中的网站服务器会将响应数据包发给网关服务器，再由后者转发给局域网中的PC 没有使用SNAT技术的网络： 使用SNAT技术处理过的网络： 使用iptables命令实现SNAT技术是一件很麻烦的事情，但是在firewall-config中却很简单。用户只需按照下图进行配置，并选中Masqueradezone复选框，就自动开启了SNAT 技术。 为了直观查看不同工具在实现相同功能时的区别，针对前面使用firewall-cmd配置的防火墙策略规则，使用firewall-config工具进行了重新演示：将本机888端口的流量转发到22端口，且要求当前和长期均有效，具体如下面图1和图2所示。 配置本地端口转发： 让防火墙策略规则立即生效： 用命令配置富规则可真辛苦，幸好现在有了图形用户界面的工具。让192.168.1.31主机访问本机的1234端口号，如下图所示。其中Element选项能够根据服务名称、端口号、协议等信息进行匹配；Source与Destination选项后的inverted复选框代表反选功能，将其选中则代表对已填写信息进行反选，即选中填写信息以外的主机地址；Log复选框在选中后，日志不仅会被记录到日志文件中，而且还可以在设置日志的级别(Level)后，再将日志记录到日志文件中，以方便后续的筛查。 如果生产环境中的服务器有多块网卡在同时提供服务，则对内网和对外网提供服务的网卡要选择的防火墙策略区域也是不一样的。也就是说，可以把网卡与防火墙策略区域进行绑定(下图)，这样就可以使用不同的防火墙区域策略，对源自不同网卡的流量进行有针对性的监控，效果会更好。 7.4 服务的访问控制列表​ TCP Wrapper是RHEL 6/7系统中默认启用的一款流量监控程序，根据来访主机的地址与本机的目标服务程序做出允许或拒绝的操作 ​ 在RHEL 8中，已被firewalld正式替代。换句话说，Linux系统中其实有两个层面的防火墙，第一种是前面讲到的基于TCP/IP协议的流量过滤工具，而TCP Wrapper服务则是能允许或禁止Linux系统提供服务的防火墙，从而在更高层面保护了Linux系统的安全运行 ​ TCP Wrapper服务的防火墙策略由两个控制列表文件所控制，用户可以编辑允许控制列表文件来放行对服务的请求流量，也可以编辑拒绝控制列表文件来阻止对服务的请求流量 ​ 控制列表文件修改后会立即生效，系统将会先检查允许控制列表文件(/etc/hosts.allow)，如果匹配到相应的允许策略则放行流量；如果没有匹配，则会进一步匹配拒绝控制列表文件(/etc/hosts.deny)，若找到匹配项则拒绝该流量。如果这两个文件都没有匹配到，则默认放行流量 ​ 由于RHEL 8版本已不再支持TCP Wrapper服务程序，因此我们接下来选择在一台老版本的服务器上进行实验 ​ TCP Wrapper服务的控制列表文件配置起来并不复杂，常用的参数 如下表所示： 在配置TCP Wrapper服务时需要遵循两个原则： 1.编写拒绝策略规则时，填写的是服务名称，而非协议名称 2.建议先编写拒绝策略规则，再编写允许策略规则，以便直观地看到相应的效果 下面编写拒绝策略规则文件，禁止访问本机sshd服务的所有流量(无须修改/etc/hosts.deny文件中原有的注释信息)： ​ 在允许策略规则文件中添加一条规则，使其放行源自192.168.10.0/24网段，且访问本机sshd服务的所有流量。可以看到，服务器立刻就放行了访问sshd服务的流量，效果非常直观： 7.5 Cockpit驾驶舱管理工具Cockpit在默认情况下就已经被安装到系统中。执行dnf命令对此进行确认： Cockpit服务程序在RHEL8版本中没有自动运行，将它开启并加入到开机启动项中： 备注:Cookpit其他知识见书籍即可 8 提高命令行生产率8.1 脚本的分类1234# 系统 --------- --------------------------------Windows *.bat,*.cmd,.vbdLinux #!/bin/bash , chmod +x file.sh 8.2 创建和执行脚本指定命令解释器12345678910111213141516171819202122[root@foundation0 ~]# vim /test.sh #掌握#!/bin/bash #指定解释器为/bin/bashdateecho &quot;hello world&quot;[root@foundation0 ~]# chmod +x test.sh #赋予脚本执行权限[root@foundation0 ~]# cd /[root@foundation0 ~]# ./test.sh #可以通过./这种相对路径方式执行脚本[root@foundation0 ~]# /root/test.sh #或通过绝对路径方式执行脚本[root@foundation0 bin]# first.sh hello world[root@foundation0 bin]# sh first.sh hello world[root@foundation0 bin]# bash first.sh hello world[root@foundation0 bin]# source first.sh hello world[root@foundation0 bin]# . first.sh hello world脚本中需要书写解释器#!/bin/bash，脚本内容可以是linux命令。linux系统可以通过/etc/shells查看支持的shell类型。也可以通过echo $SHELL来查看当前系统正在使用的shell类型 执行Bash Shell12345678910111213141516[root@foundation0 /]# which ls[root@foundation0 /]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin练习:[root@servera ~]# lsfindfiles test.sh[root@servera opt]# echo $PATH/root/.local/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin[root@servera opt]# mkdir /root/bin [root@servera opt]# mv /root/test.sh /root/bin/[root@servera opt]# ls /root/bin/test.sh[root@servera opt]# test.shSat Jul 13 09:49:11 PM EDT 2024 对特殊字符加引号123456789101112131415161718192021222324\\, '' , &quot;&quot; #掌握[root@servera /]# echo $SHELL/bin/bash[root@servera /]# echo '$SHELL'$SHELL[root@servera /]# echo &quot;$SHELL&quot;/bin/bash[root@servera /]# echo '$SHELL world' #单引号内容是普通字符串，变量不生效$SHELL world[root@servera /]# echo &quot;$SHELL world&quot; #双引号内变量生效/bin/bash world【f0】[kiosk@foundation0 ~]$ echo $HOSTNAME[kiosk@foundation0 ~]$ ssh root@bastion 'touch /tmp/$HOSTNAME' [kiosk@foundation0 ~]$ ssh root@bastion 'ls /tmp'-rw-r--r--. 1 root root 0 Jul 13 22:13 bastion.lab.example.com[kiosk@foundation0 ~]$ ssh root@bastion &quot;touch /tmp/$HOSTNAME&quot; [kiosk@foundation0 ~]$ ssh root@bastion 'ls -l /tmp'total 16-rw-r--r--. 1 root root 0 Jul 13 22:13 bastion.lab.example.com-rw-r--r--. 1 root root 0 Jul 13 22:15 foundation0.ilt.example.com ``与$()123456789101112131415161718192021222324252627282930313233343536373839404142434445`命令` #掌握 优先执行$(命令)在shell中设置一个变量暂时使用[root@servera /]# var=`date`[root@servera /]# var=$(date)[root@servera /]# echo 'to day $var'to day $var[root@servera /]# touch $(date +%H%M%S).txt取消一个变量[root@servera /]# unset var[root@servera /]# echo $var第二个例子whoamiecho whoamiecho `whoami`echo $(whoami)dateecho `date`man datedate +%y%m%ddate +%Y%m%ddate +%Y-%m-%decho $(date +%Y-%m-%d)touch $(date +%Y-%m-%d).txtlstar -zcvf $(date +%Y-%m-%d).tar.gz /etc/lscd /既然可以通过``和$()的结果，通过echo来通过标准输出打印到屏幕上，那么我们也可以将其应用到脚本。vim os.current#!/bin/bash// echo的&quot;-e&quot;参数是启用反斜杠转义: echo -e &quot;User:\\t&quot; $(whoami)echo -e &quot;HOST:\\t&quot; `hostname`echo -e &quot;ipv4:\\t&quot; $(ip a s eth0 | awk '/inet / {print $2}')echo -e &quot;Memory:\\t&quot; $(free -h | awk '/Mem/ {print $2}') echo -e &quot;Disk:\\t&quot; $(df -ht xfs | awk '/dev/ {print $4}')chmod +x os.current ./os.current 8.3 使用循环更高效的命令for语句12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273语法：for variable in list do command variabledone例子：for i in 1 2 3 do echo $idone例子2：for i in 1 2 3;do echo $idone# echo {1..10}# echo $(seq 1 10)[root@servera ~]# echo {1..10} #掌握1 2 3 4 5 6 7 8 9 10[root@servera ~]# echo $(seq 1 10)1 2 3 4 5 6 7 8 9 10[root@servera ~]# for i in host1 host2 host3;do echo $i;done #掌握host1host2host3[root@servera ~]# for num in host{1..3};do echo $num;done #掌握host1host2host3[root@servera ~]# for num in `ls /`;do echo $num;done #掌握[root@servera ~]# cd /opt/[root@servera opt]# ls[root@servera opt]# touch file{1..3}[root@servera opt]# lsfile1 file2 file3[root@servera opt]# for i in file*;do ls $i;donefile1file2file3[root@servera opt]# vim for.sh #(考点)#!/bin/bashfor i in 1 2 3;do echo $idone[root@foundation0]#for i in bastion workstation utility servera serverb;do rht-vmctl start $i;done小技巧[root@servera ~]# echo ${HOSTNAME}Oservera.lab.example.comO[root@servera ~]# echo $HOSTNAME\\Oservera.lab.example.comO练习：[root@bastion /]# vim user_list.txt user1user2user3[root@servera ~]# vim user.sh#!/bin/bashfor i in `cat user_list.txt`;do useradd testuser2 echo P@ssw0rduser2a | passwd --stdin testuser2done[root@servera ~]# sh user.sh 在脚本中使用退出代码​ 处理完所有内容后，脚本会退出到调用它的进程。但是，有时候可能需要加载完成之前退出脚本，比如加载遇到错误条件时。可以通过加载脚本中使用exit命令来实现这一目的。​ 当脚本遇到exit命令时，脚本将立即退出且不会对脚本的其余内容进行处理。​ 可以使用可选的整数参数（0到255之间，表示退出代码）来执行exit命令。退出代码进程完成后返回的代码。退出代码值0表示没有错误。所有其他非零值都表示存在错误的退出代码。​ 尽可以使用不同的非零值来区分遇到的不同类型错误。此退出代码传回到父进程，后这将它存储在？变量中，并可通过$?进行访问。 123456[root@servera /]# vim hello.sh#!/bin/bashecho &quot;hello world&quot;exit 1[root@servera /]# echo $? 使用运算符执行测试1234567891011121314151617181920212223242526272829# test与[]:test 0 -ne 1 36 echo $? 37 test 0 -ge 0 38 echo $? 39 test 0 -ge 1 40 echo $? 41 test 8 -gt 4 42 echo $? 43 [ 0 -ge 0 ] 44 echo $?-eq 等于则为真-ne 不等于则为真-gt 大于则为真-lt 小于则为真-ge 大于等于则为真-le 小于等于则为真字符串判断= == != [ 字符 == 字符 ][ 字符 != 字符 ]单目，双目-e 文件和文件夹-f 普通文件-d 目录-c 设备文件 exit、if、elif、else12345678910111213141516171819202122232425262728293031323334353637383940414243444546if [ 条件 ] then 声明fiif [ xx ];then commandfi 一、vim if.sh#!/bin/bashif [ 0 -ge 0 ];then echo okfi二、vim test.sh#!/bin/bashif [ -e /file1 ];then echo one exit 10fi三、#!/bin/bashif [ -e /file1 ];then echo one exit 10else echo two exit 20fi四、#!/bin/bashif [ -e /file1 ];then echo one exit 10elif [ -e /opt/file1 ];then echo two exit 20else echo three exit 30fi 位置变量1234567891011121314151617181920212223242526272829303132# $0、$1、$2..$9、$#:vim ping.sh#!/bin/bashping -c $2 172.25.254.$1echo '$0: ' $0echo '$1: ' $1echo '$2: ' $2echo '$3: ' $3echo '$#: ' $#[root@servera ~]# chmod +x ping.sh[root@servera ~]# ./ping.sh 254 5PING 172.25.254.254 (172.25.254.254) 56(84) bytes of data.64 bytes from 172.25.254.254: icmp_seq=1 ttl=63 time=3.02 ms64 bytes from 172.25.254.254: icmp_seq=2 ttl=63 time=0.976 ms64 bytes from 172.25.254.254: icmp_seq=3 ttl=63 time=1.03 ms64 bytes from 172.25.254.254: icmp_seq=4 ttl=63 time=1.09 ms64 bytes from 172.25.254.254: icmp_seq=5 ttl=63 time=2.16 ms--- 172.25.254.254 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 12msrtt min/avg/max/mdev = 0.976/1.654/3.015/0.808 ms$0: ./ping.sh 脚本名$1: 254 执行脚本时赋予脚本的第一个值$2: 5 执行脚本时赋予脚本的第二个值$3: $#: 2 一共赋予多少个值补充:$@ 具体赋予了什么值$？ 看上一条命令的返回值0为正常执行，非0是非正常执行$$ 查看子进程进程号 123456789101112131415161718192021222324252627创建一个添加用户的脚本在system1上创建一个脚本，名为/root/makeusers，此脚本能实现为系统system1创建本地用户，并且这些用户的用户名来自一个包含列表的文件。用户满足下列要求：（1） 此脚本要求提供一个参数，此参数就是包含用户名列表的文件（2） 如果没有提供参数，此脚本应该给出下面的提示信息 Usage: /root/makeusers userfile然后退出并返回相应的值（3） 如果提供一个不存在的文件名，此脚本应该给出下面的提示信息 Input file not found然后退出并返回相应的值（4） 创建的用户登录shell为/bin/false（5） 此脚本不需要为用户设置密码[root@servera ~]# cd /root/[root@servera ~]# cat /root/userfiletestuser1testuser2testuser3[root@servera ~]# vim /root/makeusers#!/bin/bashif [ $# -lt 1 ];then echo &quot;Usage: $0 userfile&quot; exit 1elif [ ! -f $1 ];then echo &quot;Input file not found&quot; exit 1else for i in `cat $1`;do /usr/sbin/useradd -s /bin/false $i donefi[root@servera ~]# ./root/makeusers /root/userlist 8.4 cut1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495grep root /etc/passwd #重要 grep将文本中包含关键字的行打印出来，默认是从上到下的查询顺序。查文件内容用的grep root /etc/passwd | cut -d : -f 1,3-5 #截取文本中的列 -d 指定分隔符号 -f 指定列 grep root /etc/passwd | cut -c 1,3-5 #-c 指定字符 grep ^root /etc/passwdgrep nologin$ /etc/passwdcat -n /etc/passwd grep -n ^# /etc/selinux/config grep ^$ /etc/selinux/config | wc -lgrep -n ^$ /etc/selinux/config // 过滤空行 grep -v ^$ /etc/selinux/config // 过滤非空行 grep -vn ^# /etc/selinux/config // 过滤非#开头的行cat /etc/selinux/config | grep -v ^# | grep -v ^$[root@foundation0 /]# grep ng /usr/share/xml/iso-codes/iso_639_3.xml &gt; /1.txt[root@foundation0 /]# [root@foundation0 /]# [root@foundation0 /]# grep ^$ /1.txt[root@foundation0 /]# grep ^$ /1.txt | wc -l额外：# grep -e root -e 0 /etc/passwd grep -e可以在一个文件内单独匹配多个参数root:x:0:0:root:/root:/bin/bashsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown# grep -E 'root|0' /etc/passwd[root@foundation0 ~]# grep 'kiosk|root' /etc/passwd[root@foundation0 ~]# egrep 'kiosk|root' /etc/passwdroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologinkiosk:x:1000:1000::/home/kiosk:/bin/bash[root@foundation0 ~]# grep -E 'kiosk|root' /etc/passwdroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologinkiosk:x:1000:1000::/home/kiosk:/bin/bashgrep -A 2 root #查看root关键字后两行grep -B #前x行grep -C #前后x行grep -A 2 daemon /etc/passwdhead -5 /etc/passwdhead -5 /etc/passwd &gt; /opt/passwd.tstlscat passwd.tst grep -A 2 daemon grep -A 2 daemon passwd.tst grep -B 2 daemon passwd.tst grep -C 2 daemon passwd.tst // 正则表达式:[root@foundation0 /]# cat testfile catcotcutcutcutdogdogconcatenatedogmacategoryeducatedboondogglevindicationchilidogaacatcaaaaaaaat匹配行首和行尾[root@foundation0 /]# grep ^cat testfile catcategory [root@foundation0 /]# grep cat$ testfile cataacat[root@foundation0 /]# grep ^cat$ testfile cat向正则表达式中添加通配符和倍数1、.匹配换行符以外的任何“单”个字符[root@servera opt]# grep c.t testfile 2、选择[]内的一个字符。搜索结果应为c开头、中间a或o或u、结尾t[root@servera opt]# grep c[aou]t testfile cat cot cut3、倍数* 匹配前面的子表达式零次或多次[root@servera opt]# grep c*t testfile 4、倍数通常与通配符一起使用，如.* 这将匹配任何以包含c和t中间有0个或多个任意字符的行[root@servera opt]# grep c.*t testfile 9 Linux计划任务9.1 at1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192[root@servera /]# rpm -q atat-3.1.20-11.el8.x86_64[root@servera /]# systemctl status atd.service [root@servera /]# rpm -qc at/etc/at.deny/etc/pam.d/atd/etc/sysconfig/atd语法：at 选项 参数at 时间 #设置一个任务atq 查询atrm 删除任务创建13:49分时候执行touch指令[root@servera /]# at 13:49warning: commands will be executed using /bin/shat&gt; touch /at.txtat&gt; &lt;EOT&gt; ctrl+d退出3分钟后执行echo `date` &gt;&gt; /home/student/myjob.txt | at now +3min明天17:20点执行echoat 17:20 tomorrowat&gt; echo hello三天后下午5:10分执行/bin/lsat 5:20pm+3daysat&gt; /bin/ls使用时间，和月/日/年的方式指定任务at 17:20 5/20/2022在7月的31日上午10点at 10am Jul 31查看[root@servera /]# atq作业编号 执行日期和时间 队列a 运行作业所有者7 Sun Mar 8 13:51:00 2020 a root[root@servera /]# at -l查看任务内容[root@servera /]# at -c 7删除[root@servera /]# at -d 9[root@servera /]# atrm 8systemctl status atdman atat 17:30man atat - 1at -c 1atqatrm 1atqat 17:20atqat 17:30 08/19/2022at 17:30 2022-7-24man atat now +3minat now +3hourat now +3daysman atat 4pm +3daysat 16:02 +3daysman atat teatimeman athistoryecho 123 &gt; /root/backup | at 17:20atqat -c 11atq监控任务[root@servera ~]# watch atq ctrl+c 退出监控模式一次性计划任务重启后，任务未执行依然会保存，直至删除或执行完毕后消失。黑白名单/etc/at.deny 哪个用户在黑名单里，哪个用户不能使用at，该文件默认存在/etc/at.allow 在白名单内的用户可以使用at，白名单以外的人不能使用，白名单和黑名单同时存在时优先于黑名单，该文件默认不存在，如使用需自己创建。 9.2 crontab123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[root@servera /]# systemctl status crond.service #重要[root@servera /]# systemctl enable crond[root@servera /]# systemctl is-enabled crond[root@servera /]# systemctl enable --now crond[root@servera ~]# rpm -q crontabpackage crontab is not installed[root@servera ~]# rpm -qa | grep croncronie-anacron-1.5.7-8.el9.x86_64cronie-1.5.7-8.el9.x86_64crontabs-1.11-27.20190603git.el9_0.noarch[root@servera ~]# rpm -qc crontabs/etc/crontab/etc/sysconfig/run-parts[root@servera ~]# vim /etc/crontab[root@servera ~]# crontab 选项 -e 编辑计划任务 crontab -e-u 指定用户 crontab -u student -e-r 删除所有任务 crontab -r #删除某个任务时利用-e选项进入编辑模式手动删除即可。-l 列出 crontab -lcron时间表示方法:前五个字段使用相同的语法规则规则 说明* 表示“无关紧要”，始终x-y 表示范围,x到y(含)x,y 表示列表，也可以包含范围，如5,10-13,15*/x 表示x的时间间隔* * * * * command分 时 日 月 周 任务内容0-59 0-23 1-31 1-12 0-71.每年2月2日上午9点执行echo hello0 9 2 2 * /usr/bin/echo hello #which echo可以寻找命令绝对路径2.每天3到6点 第2分 执行一个脚本/root/1.sh2 3-6 * * * /bin/sh /root/1.sh3.每两个小时的第2分钟，执行一个脚本/root/1.sh2 */2 * * * /bin/sh /root/1.sh4.每年７月的第１天和第５天，两点２分，执行一个脚本/root/1.sh2 2 1,5 7 * /bin/sh /root/1.sh5.配置cron任务，每隔2分钟运行logger “Ex200 in progress”，以harry用户身份运行[root@servera /]# id harry [root@servera /]# crontab -u harry -e[root@servera /]# crontab -u harry -l (也可以su - harry切换到任务用户，运行crontab -l)*/2 * * * * /usr/bin/logger “Ex200 in progress” 删除某条可以crontab -e 进去编辑删除用户的所有任务 crontab -r 黑名单：vim /etc/cron.deny 限制用户使用crond服务白名单：mandbman -k cronman 1 crontabecho harry &gt;&gt; /etc/cron.allow白名单启用后，就不用黑名单了，谁在白名单里谁能用该功能 测试：su - harry,crontab -e #可以使用exitsu - tom,crontab -e #不可使用// 练习:1、每年2月2日上午9点执行echo hello任务 0 9 2 2 * /usr/bin/echo hello2、七月每周5的,9点至下午5点，每5分钟执行echo hi */5 9-17 * 7 5 /usr/bin/echo hello3、执行一个任务,该任务每隔2分钟运行以下命令/usr/bin/logger &quot;hello&quot;，给harry设定这个任务 */2 * * * * /usr/bin/logger &quot;hello&quot;4、使用root身份为用户tom设置一个计划任务，每天的下午2点43分执行/home/tom/tom.sh脚本 crontab -u tom -e 43 14 * * * /bin/bash /home/tom/tom.sh5、每天的，1点，6点，9点整，将/var/log/messages文件进行归档，要求使用gzip工具，归档文件保存在/tmp/当前时间.tar.gz 0 1,6,9 * * * tar -zcvf /tmp/$(date +%Y-%m-%d).tar.gz /var/log/messages 9.3 管理临时文件12345678910111213rhel6 tmpwatchsystemd-tmpfiles1 手动清理临时文件[root@servera /]# systemctl status systemd-tmpfiles-setup 查看服务状态[root@servera /]# rpm -qf /usr/lib/tmpfiles.d/tmp.conf systemd-239-13.el8.x86_64cp /usr/lib/tmpfiles.d/tmp.conf /etc/tmpfiles.d/cd /etc/tmpfiles.d/vim tmp.confq /tmp 1777 root root 5dsystemd-tmpfiles --clean /etc/tmpfiles.d/tmp.conf 12345678910111213141516172 清理临时文件小实验创建一个存放临时文件的目录，并且设置相应权限/run/momentary 0700 root root[root@servera /]# vim /etc/tmpfiles.d/momentary.confd /run/momentary 0700 root root 30s创建存放临时文件目录[root@servera /]# systemd-tmpfiles --create /etc/tmpfiles.d/momentary.conf [root@servera /]# ll /run/momentary/ -ddrwx------. 2 root root 40 Mar 8 15:08 /run/momentary/在目录中创建一个文件叫做mom.txt，此文件模拟临时文件[root@servera /]# touch /run/momentary/mom.txt[root@servera /]# sleep 30[root@servera /]# ll /run/momentary/mom.txt -rw-r--r--. 1 root root 0 Mar 8 15:08 /run/momentary/mom.txt输入清除临时文件命令，清除所有的临时文件[root@servera /]# systemd-tmpfiles --clean /etc/tmpfiles.d/momentary.conf [root@servera /]# ll /run/momentary/mom.txt ls: cannot access '/run/momentary/mom.txt': No such file or directory 10 系统性能调优10.1 tuned查看状态123[root@servera tmp]# yum install -y tuned[root@servera tmp]# systemctl enable --now tuned[root@servera tmp]# systemctl status tuned tuned-adm管理指令1234[root@servera tmp]# tuned-adm list #列出优化方案[root@servera tmp]# tuned-adm recommend #系统推荐的优化方案[root@servera tmp]# tuned-adm profile virtual-guest #修改优化方案为virtual-guest[root@servera tmp]# tuned-adm off #关闭优化，默认不关闭，考试时不要关闭 配置文件存储路径123每个优化方案对应要给配置文件，其中描述了修改的内核参数路径：/usr/lib/tuned/ 10.2 使用cockpit修改调优的方式1234【servera】systemctl start cockpit【foundation】打开浏览器输入：https://servera:9090--点击右侧高级选项---添加访问---账号密码：root redhat ----左边Overview---找到configure--找到调优 10.3 调节nice值1234567891011121314进程有默认优先级，但是优先级默认不能更改，但是可以通过修改nice值来影响进程优先级。nice值+old优先级=new优先级Nice + 80 = 新优先级Nice值调节范围： root：修改范围-20~19 调节优先级的工作由root来执行 user: 修改范围0~19 调节后无法降级 优先级： 数值越大优先度越低 数值越小优先级越高nice 直接给一个新的指令设置优先级renice 给一个现有的进程调节优先 nice值管理12345678910111213语法：nice 选项 command选项：-n ： -n 后面添加优先级 例：nice -n 10 vim 1.txt 语法：renice 选项 进程号-n： -n 后面添加优先级 例：renice -n 10 pid 10.4 练习1234567891011121314151617181920212223242526272829[root@clear ~]# cd /opt/[root@clear opt]# lsdir1 file1[root@clear opt]# rm -rf *[root@clear opt]# vim test.txt &amp; #生成一个进程，&amp;放后台[1] 59090[root@clear opt]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1139 1138 0 80 0 - 59210 - pts/2 00:00:03 bash0 T 0 59090 1139 1 80 0 - 60817 - pts/2 00:00:00 vim #出现了0 R 0 59091 1139 0 80 0 - 63799 - pts/2 00:00:00 ps[root@clear opt]# nice -n 10 vim 1.txt &amp; #创建新进程 并且添加nice值为正10[2] 59097[root@clear opt]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1139 1138 0 80 0 - 59243 - pts/2 00:00:03 bash0 T 0 59090 1139 0 80 0 - 60817 - pts/2 00:00:00 vim0 T 0 59097 1139 2 90 10 - 60817 - pts/2 00:00:00 vim #在这里0 R 0 59098 1139 0 80 0 - 63799 - pts/2 00:00:00 ps[2]+ Stopped nice -n 10 vim 1.txt[root@clear opt]# renice -n -5 59090 #修改现有进程，nice值为负5，指定进程号59090 (process ID) old priority 0, new priority -5[root@clear opt]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1139 1138 0 80 0 - 59243 - pts/2 00:00:03 bash0 T 0 59090 1139 0 75 -5 - 60817 - pts/2 00:00:00 vim #已经改变了0 T 0 59097 1139 0 90 10 - 60817 - pts/2 00:00:00 vim0 R 0 59100 1139 0 80 0 - 63799 - pts/2 00:00:00 ps[root@clear opt]# 11 管理SELinux11.1 介绍selinuxselinux简介123SELinux全称是Security Enhanced Linux (安全强化 Linux),是MAC (Mandatory Access Control，强制访问控制系统)的一个实现，在于明确的指明某个进程可以访问哪些资源(文件、网络端口等)强制访问控制系统的用途在于增强系统抵御0-Day攻击(利用尚未公开的漏洞实现的攻击行为)的能力它不是网络防火墙或FACL的替代品，在用途上也不重复。在目前的大多数发行版中，已经默认在内核集成了SELinux selinux实现原理12345678910 普通权限和selinux权限对比：​ 传统的Linux使用用户、文件权限的概念来限制资源的访问，通过对比进程的发起用户和文件权限以此来保证系统资源的安全，这是一种自由访问控制方式（DAC） SELinux是​Linux下的一种安全强化机制，为进程和文件加入了除权限之外更多的限制来增强访问条件，这种方式为强制访问控制(MAC) 进程和文件都有相应的标签，称为上下文，只有进程上下文和文件上下文对应上，该进程才可以访问文件 ​何为对应？系统中大量的进程和文件上下文已经被定义，如http服务进程上下文为httpd_t域，/var/www/html/目录上下文为httpd_sys_content_t，后者归属于前者的匹配域内，即可匹配 分类 源 目的--------------------- ---------------- ---------------------------- 传统文件系统权限DAC 用户 文件系统权限 SELinux权限MAC 用户进程上下文 文件（或目录端口等）上下文 selinux的限制范围1234567891011SElinux具有多种上下文类型，常见种类: ​安全上下文(限制文件的访问)：该种类上下文存在于内存和文件中，进程访问文件inode时读取到上下文类型进行对比​ 布尔值(限制软件功能的访问)：该种类型上下文主要控制某些进程是否可以访问服务常用功能中出现的文件​ 安全端口(限制服务的访问)：selinux会限制服务启用的非标准端口号 ID SELinux---- ------------ --------------------- --------------------------------------------- Filesystem chmod, chown, setfacl semanage fcontext ... restorecon ... chcon ... touch /.autorelabel Service vim /etc/\\*.conf setsebool -P ... Firewall firewall-cmd ... semanage port ... SELinux vim /etc/selinux/config 查看上下文123456789101112131415[root@servera ~]# dnf install -y httpd[root@servera ~]# systemctl enable --now httpd进程： ps auxZ ps -eZ[root@servera ~]# ps auxZ | grep httpdsystem_u:system_r:httpd_t:s0 root 1971 0.0 1.2 273800 10496 ? Ss 04:58 0:00 /usr/sbin/httpd -DFOREGROUND文件：ls -Z[root@servera ~]# ll -dZ /var/www/html/system_u:object_r:httpd_sys_content_t:s0 /var/www/html/只要进程和文件的安全上下文匹配，该进程就可以访问该文件资源 security context介绍1234567891011121314151617181920212223242526272829303132333435363738content一词是我们常说的上下文，安全上下文有5个字段，以：冒号分割，unconfined_u :object_r: httpd_sys_content_t: s0 [类别] 身份 user -u 角色roles -r 类型 type -t 灵敏度（级别） 类别 1) 身份字段（user) 用于标识该数据被哪个身份所拥有，相当于权限中的用户身份。这个字段并没有特别的作用，知道就好。常见的身份类型有以下 3 种：- root：表示安全上下文的身份是 root。- system_u：表示系统用户身份，其中“_u”代表 user。- user_u：表示与一般用户账号相关的身份，其中“_u”代表 user。- 以使用 seinfo 命令来进行查询2) 角色（role）主要用来表示此数据是进程还是文件或目录。这个字段在实际使用中也不需要修改，所以了解就好。常见的角色有以下两种：- object_r：代表该数据是文件或目录，这里的“_r”代表 role。- system_r：代表该数据是进程，这里的“_r”代表 role。3) 类型（type） 类型字段是安全上下文中最重要的字段，进程是否可以访问文件，主要就是看进程的安全上下文类型字段是否和文件的安全上下文类型字段相匹配，如果匹配则可以访问。#（掌握重点）注意，类型字段在文件或目录的安全上下文中被称作类型（type），但是在进程的安全上下文中被称作域（domain）。也就是说，在主体（Subject）的安全上下文中，这个字段被称为域；在目标（Object）的安全上下文中，这个字段被称为类型。域和类型需要匹配（进程的类型要和文件的类型相匹配），才能正确访问。context查询工具seinfo、sesearch seinfo -u # 查询所有的user字段的种类 seinfo -r # 查询所有的role字段的种类 seinfo -t # 查询所有的type字段的种类 sesearch -A 可以查询什么类型进程可以读取什么type类型的文件 sesearch -A -s 进程type # 查询type类型的进程能够读取的文件type sesearch -A -b 规则（规则的boolean值，所以为-b选项，理解为bool规则） [root@foundation0 ~]# sesearch -A -s httpd_t | grep '^allow httpd_t httpd_sys_content_t'4) 灵敏度 灵敏度一般是用 s0、s1、s2 来命名的，数字代表灵敏度的分级。数值越大，代表灵敏度越高。5) 类别 类别字段不是必须有的，所以我们使用 ls 和 ps 命令查询的时候并没有看到类别字段。通过seinfo -u -x查看 1234567891011121314151617181920212223242526272829303132-serverasystemctl stop firewalldsetenforce 1yum install -y httpdsystemctl enable --now httpdps auxZ | grep httpd #-Z 大写Z就表示上下文，发现http_t字段就是httpd服务进程上下文'类型'字段cd /var/www/htmlll -dZ /var/www/html #发现上下文是httpd开头echo test &gt; /var/www/html/index.html #创建测试页面ll -Z /var/www/html/index.html #发现所有文件类型字段上下文，都是httpd开头,看上下文的第三段 类型字段-foundation0 打开浏览器，网址位置输入 172.25.250.10/index.html #访问servera的网页发现可以访问到网页内容，证明上下文一致，不被限制，可以访问-serverachcon -t default_t /var/www/html/index.html #更改了file3的上下文ll -Z #发现index.html上下文类型字段变成default_t-foundation0 打开浏览器，网址位置输入 172.25.250.10/index.html #访问servera的网页发现不可以访问到网页内容，证明上下文不一致，被限制，不可访问-serverall -dZ /var/www/html/ #查看到httpd相关上下文chcon -t httpd_sys_content_t /var/www/html/index.html #更改上下文改为httpd_sys_content_t-foundation0 打开浏览器，网址位置输入 172.25.250.10/index.html #访问servera的网页发现可以访问到网页内容，证明上下文一致，不被限制，可以访问 11.2 更改SElinux强制模式临时开启或关闭selinux1234567891011121314#临时修改意思是重启失效[root@clear /]# getenforce Enforcing[root@clear /]# setenforce usage: setenforce [ Enforcing | Permissive | 1 | 0 ] # 强制 1|宽容 0[root@clear /]# setenforce 0[root@clear /]# getenforce Permissive[root@clear /]# setenforce Permissive[root@clear /]# getenforce Permissive[root@clear /]# setenforce 1[root@clear /]# getenforce Enforcing 永久开启或关闭selinux12345678-RHEL&lt;=9[root@servera ~]# vim /etc/selinux/config# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=enforcing修改后，重启操作系统生效 继承特性12345继承：1、在父目录下创建文件会继承selinux上下文touch2、cp时不继承，即cp时不保留原来的父目录上下文关系 创建了文件并且移动(mv)会保留原来的父目录上下文关系 cp -a(rp)复制时会保留之前的上下文关系 123456789101112131415161718192021222324【servera】 [root@servera /]# cd /var/www/html/[root@servera html]# ll -dZ .drwxr-xr-x. 2 root root system_u:object_r:httpd_sys_content_t:s0 23 May 21 09:13 .[root@servera html]# touch haha.txt[root@servera html]# ll -Z haha.txt -rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 0 May 21 10:06 haha.txt[root@servera ~]# cd /tmp/[root@servera tmp]# touch file{1..3}[root@servera tmp]# ls -Z /tmp/file*unconfined_u:object_r:user_tmp_t:s0 /tmp/file1 unconfined_u:object_r:user_tmp_t:s0 /tmp/file2 unconfined_u:object_r:user_tmp_t:s0 /tmp/file3[root@servera tmp]# cp /tmp/file1 /var/www/html/[root@servera tmp]# cp -a /tmp/file2 /var/www/html/[root@servera tmp]# mv /tmp/file3 /var/www/html/[root@servera tmp]# cd /var/www/html/[root@servera html]# ls -Z *unconfined_u:object_r:httpd_sys_content_t:s0 file1 unconfined_u:object_r:user_tmp_t:s0 file3 unconfined_u:object_r:user_tmp_t:s0 file2[root@servera html]# touch /var/www/html/file0[root@servera html]# ll -Z /var/www/html/file0-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 0 Apr 10 05:08 /var/www/html/file0 11.3 定义selinux默认文件上下文规则12345方法1：永久设置，但不记录至数据库 chcon 设置上下文关系方法2：永久设置，记录至数据库 semanage fcontext 添加、修改、查看、删除默认上下文 restorecon 恢复默认上下文 chcon12345678910111213141516chcon 选项 上下文类型 文件[root@servera html]# ll -Z total 0-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 0 Dec 11 05:36 file1-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 0 Dec 11 05:38 file2-rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Dec 11 05:37 file3-rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Dec 11 05:37 file4[root@servera html]# man chcon[root@servera html]# chcon -t httpd_sys_content_t file3[root@servera html]# ll -Z total 0-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 0 Dec 11 05:36 file1-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 0 Dec 11 05:38 file2-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 0 Dec 11 05:37 file3-rw-r--r--. 1 root root unconfined_u:object_r:usexr_tmp_t:s0 0 Dec 11 05:37 file4 semanage fcontext12345678910111213141516171819202122232425262728293031323334353637383940帮助中查看示例：man semanageman semanage fcontext选项：semanage fcontext -a 添加 添加至数据库-d 删除-l 查看-t 指定上下文-m 修改修改流程：1、查看selinux上下文类型数据库是否有记录？有就恢复I(restorecon)，没有就添加(semanage fcontext -a)2、man semanage fcontext添加文件标签类型至数据库. 添加后，文件并不会更改标签类型3、使用restorecon命令同步和数据库一致【servera】-l[root@clear /]# man semanage fcontext | grep \\#root@servera ~]# yum provides sesearchLast metadata expiration check: 2:54:21 ago on Sat Mar 1 00:44:02 2025.setools-console-4.4.3-1.el9.x86_64 : Policy analysis command-line tools for SELinuxRepo : rhel-9.3-for-x86_64-baseos-rpmsMatched from:Filename : /usr/bin/sesearch[root@clear /]# semanage fcontext -l | grep /var/www 看第一行即可[root@clear /]# semanage fcontext -l | grep /var/www/html/file3/var/www/html/file3 all files system_u:object_r:default_t:s0 [root@clear /]# semanage fcontext -a -t httpd_sys_content_t /var/www/html/file3 #数据库没有上下文记录，用-a添加，如已存在则用-m修改数据库中错误的上下文[root@clear /]# semanage fcontext -l | grep /var/www/html/file3/var/www/html/file1 all files system_u:object_r:httpd_sys_content_t:s0 restorecon 恢复文件上下文和数据库一致-v 显示修改标签内容-R 递归[root@clear /]# restorecon -R -v /var/www/html/file3Relabeled /var/www/html/file3 from system_u:object_r:default_t:s0 to system_u:object_r:httpd_sys_content_t:s0[root@clear /]# ll -Z /var/www/html/file3 1234567891011-chcon chcont -t httpd_sys_content_t /var/www/html/index.htmlman semanage fcontext -l | grep \\#-semanage fcontextman semanage fcontext -l | grep \\# semanage fcontext -a -t httpd_sys_content_t /var/www/html/index.html semanage fcontext -m -t httpd_sys_content_t /var/www/html/index.htmlsemanage fcontext -d -t httpd_sys_content_t /var/www/html/index.htmlsemanage fcontext -l /var/www/html/index.htmlrestorecon -R -v /var/www/html/index.html 12345678课上练习：1.servera上安装httpd软件2.在/var/www/html目录创建3个文件file0、file1、file2、对应内容test0、test1、test23.修改file0上下文为default_t4.开启httpd服务。关闭防火墙 systemctl stop firewalld5.在f0上用浏览器访问三个网页文件，测试发现，file0访问不到内容，而file1、file2可以6.回到servera上修改file0上下文为httpd_sys_content_t7.再测试 11.4 管理布尔值管理布尔值1234567891011121314151617181920212223布尔值主要对应的是应用的功能的开启或关闭getsebool 列出布尔值状态 usersetsebool 设置布尔值开启或关闭on 开，off 关-P 更改布尔值永久生效 setsebool -psemanage boolean -l 查看布尔值是否永久[root@servera /]# yum install -y selinux-policy-doc[root@servera /]# mandb[root@servera /]# man -k '_selinux' | grep httpd[root@servera /]# man 8 httpd_selinux[root@servera /]# man 8 httpd_selinux | grep -B 1 homedir[root@servera /]# getsebool -a [root@servera /]# getsebool -a | grep httpd | grep homedir修改布尔值状态[root@servera /]# setsebool httpd_enable_homedirs on[root@servera /]# semanage boolean -l | grep httpd_enable_homehttpd_enable_homedirs (off , off) Allow httpd to enable homedirs[root@servera /]# setsebool -P httpd_enable_homedirs on 永久生效[root@servera /]# semanage boolean -l | grep httpd_enable_homehttpd_enable_homedirs (on , on) Allow httpd to enable homedirs[root@servera /]# setsebool -P httpd_enable_homedirs off 使用布尔值调整123456789101112131415161718192021222324252627282930313233343536373839404142434445461.关闭selinux正常使用 apache发布页面托管在用户主页上的web内容功能1.关闭selinux[root@clear /]# setenforce 0;systemctl stop firewalld 2.编辑配置文件[root@clear /]# vim /etc/httpd/conf.d/userdir.conf #UserDir disabled #注释该行UserDir public_html #解除该行注释，启用该功能3.切换student用户，制作发布目录和网页[root@clear /]# su - student[student@clear ~]$ mkdir public_html #创建发布目录[student@clear ~]$ echo test_web_page &gt; public_html/index.html #制作网页[student@clear ~]$ logout #退出[root@clear /]# chmod 711 /home/student/ #root用户身份，修改用户家目录权限为711，目的是为了让其他人有权限访问子目录及文件[root@clear /]# systemctl restart httpd4.在foundation上做访问测试firefox http://172.25.250.10/~student/index.htmltest_web_page5.开启selinux再次访问【172.25.250.10】[root@clear /]# setenforce 1【foundation】firefox http://172.25.250.10/~student/index.htmlForbiddenYou don't have permission to access /~student/index.html on this server.6.开启对应布尔值，允许该功能[root@clear /]# yum install -y selinux-policy-doc[root@clear /]# mandb[root@clear /]# man -k 'http'[root@clear /]# man 8 httpd_selinux #文档里搜索homedirs，复制该命令，开启该功能[root@clear /]# setsebool -P httpd_enable_homedirs 1[root@clear /]# getsebool[root@clear /]# getsebool -a[root@clear /]# getsebool -a | grep http | grep homedir #第一种查询方式root@clear /]# semanage boolean -l | grep homedirs #第二种查询方式，查布尔值数据库httpd_enable_homedirs (on , on) Allow httpd to enable homedirs （功能打开，永久生效）7.在foundation上做访问测试firefox http://172.25.250.10/~student/index.htmltest_web_page 11.5 安全端口123456789101112131415161718192021222324252627282930313233实验目标：当apache服务发布端口从80修改为82时，保证服务可以启动并开机自启动。【servera】[root@servera html]# systemctl stop firewalld[root@servera html]# yum install -y httpd[root@servera html]# setenforce 1[root@servera html]# systemctl enable --now httpd #服务正常启动【f0】curl http://servera/file1 #正常可以访问【servera】[root@servera html]# rpm -qc httpd #倒数第四行[root@servera html]# vim /etc/httpd/conf/httpd.conf Listen 82 #47行 练习时可以从80改为82[root@servera html]# systemctl restart httpd #服务起不来[root@servera html]# cat /var/log/messages #看日志文件最后的部分*[root@servera html]# man semanage port | grep \\# # semanage port -l # semanage port -a -t http_port_t -p tcp 81 # semanage port -a -t ssh_port_t -p tcp 8991[root@servera html]# semanage port -l | grep http_port_t http_port_t tcp 80, 81, 443, 488, 8008, 8009, 8443, 9000*[root@servera html]# semanage port -a -t http_port_t -p tcp 82 #添加安全端口*[root@servera html]# semanage port -l | grep http_port_t #并查看是否有82http_port_t tcp 82, 80, 81, 443, 488, 8008, 8009, 8443, 9000pegasus_http_port_t tcp 5988[root@servera html]# echo haha &gt; /var/www/html/index.html #手动制作一个测试页[root@servera html]# systemctl restart httpd #可以启动服务[root@servera html]# systemctl enable httpd #开机自启动[root@servera html]# netstat -ntlp | grep 82[root@servera html]# localhost:82/index.html #本机测试82端口访问index.html网页文件【f0】# curl http://servera:82/index.html #再次访问 11.6 SElinux日志​ SELinux 会使用被称为 AVC（Access VectorCache，访问矢量缓存）的缓存，如果访问被拒绝（也被称为 AVC拒绝），则会在一个日志文件中记录下拒绝消息。 ​ 这些被拒绝的消息可以帮助诊断和解决常规的SELinux策略违规行为，至于这些拒绝消息到底被记录在什么位置，则取决于 auditd 和rsyslogd 守护进程的状态： 若auditd守护进程正在运行，则拒绝消息将被记录与 /var/log/audit/audit.log 中。 ​ 若 auditd 守护进程没有运行，但 rsyslogd守护进程正在运行，则拒绝消息会记录到 /var/log/messages 中。 12-a：分析指定的日志文件sealert -a /var/log/audit/audit.log 回显比较慢多等一会 12 管理基本存储12.1 添加分区分区方案-MBR&amp;GPT12345MBR分区方案:MBR叫做主引导记录，存在于磁盘的0柱面0磁道0扇区中，是磁盘的第一个扇区内，大小为512字节，446字节初始化程序加载器，64字节分区表，2字节校验码，每个分区16字节，所以最多4个分区，最大磁盘空间支持2T。主分区和扩展分区编号用1-4，逻辑分区编号5以上GPT分区方案:GPT是GUID Partition Table，全局唯一标识磁盘分区表。它由UEFI启动硬盘，这样就有了UEFI取代传动BIOS，而GPT则取代传统的MBR，windows支持最多128个GPT分区 1234567891011MBR分区方案分区表=64字节1个分区=16字节4x16=64 最多只能记录4个分区 主分区4个分区不够用？比如5个以上？1.主分区最多4个，占分区编号1-4，可以格式化(可使用存放数据)2.扩展分区最多可以有1个，牺牲一个主分区来做扩展分区，它不能格式化（不能使用，不能存放数据），它的作用是装载逻辑分区用的，也就是用扩展分区的空间再划分多个逻辑分区3.逻辑分区-由扩展分区的空间划分而来的，约可以分15个左右。可以格式化（可以使用，可以存放数据），逻辑分区必定从5号开始分区方案:4P、3P+1E 分区工具:fdisk&amp;parted123456789101112使用新磁盘流程规划：1.分区（可选）fdisk parted2.格式化 mkfs3.挂载 mount4.使用 fdisk是一个分区工具，既可以查看磁盘状况，也可以对磁盘进行分区:语法：fdisk 选项 设备名选项：-l 查看所有磁盘状态例：fdisk -l #查所有磁盘信息fdisk -l /dev/vdb #查某一个磁盘信息 分区方案练习1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283841 MBR分区方案：5G磁盘，每个分区1G3P+1E ， 1E（L5、L6）[root@clear /]# fdisk -l[root@clear /]# fdisk -l /dev/vdb[root@clear /]# fdisk /dev/vdbn #创建分区p #选择p主 或 e扩展1 #分区编号回车，不通过扇区范围分配 +1G:设置一个1G大小分区 p:查看分区状态 d:删除 w:保存退出-删除分区Command (m for help): dPartition number (1-6, default 6): 6Partition 6 has been deleted.#partprobe 磁盘分区正常结束后，此命令可以正常执行，不返回任何信息，主要做刷新分表信息通知内核[root@clear /]# fdisk -l /dev/vdb[root@servera ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTSvda 252:0 0 10G 0 disk ├─vda1 252:1 0 1M 0 part ├─vda2 252:2 0 200M 0 part /boot/efi├─vda3 252:3 0 600M 0 part /boot└─vda4 252:4 0 9.2G 0 part /vdb 252:16 0 5G 0 disk ├─vdb1 252:17 0 1G 0 part ├─vdb2 252:18 0 1G 0 part ├─vdb3 252:19 0 1G 0 part ├─vdb4 252:20 0 1K 0 part ├─vdb5 252:21 0 1G 0 part └─vdb6 252:22 0 1021M 0 part vdc 252:32 0 5G 0 disk vdd 252:48 0 5G 0 disk 2 GPT分区方案Command (m for help): gCreated a new GPT disklabel (GUID: 1BA96F11-6DA0-204D-82D6-0CD15E42851E).The old dos signature will be removed by a write command.Command (m for help): nPartition number (1-128, default 1): 回车First sector (2048-10485726, default 2048): 回车 Last sector, +sectors or +size{K,M,G,T,P} (2048-10485726, default 10485726): +1GCreated a new partition 1 of type 'Linux filesystem' and of size 1 GiB.Command (m for help): nPartition number (2-128, default 2): First sector (2099200-10485726, default 2099200): Last sector, +sectors or +size{K,M,G,T,P} (2099200-10485726, default 10485726): +2GCreated a new partition 2 of type 'Linux filesystem' and of size 2 GiB.Command (m for help): l #列出分区标识类型Command (m for help): t #改变分区标识类型Partition number (1,2, default 2): 2Partition type (type L to list all types): 19Changed type of partition 'Linux filesystem' to 'Linux swap'.Command (m for help): pDisk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: 1BA96F11-6DA0-204D-82D6-0CD15E42851EDevice Start End Sectors Size Type/dev/vdb1 2048 2099199 2097152 1G Linux filesystem/dev/vdb2 2099200 6293503 4194304 2G Linux swapCommand (m for help): wThe partition table has been altered.Calling ioctl() to re-read partition table.Syncing disks.# partprobe 同步数据至内核 parted使用MBR分区方案1234567891011121314151617181920212223241.mbr方式定义分区方案分区分3个区 主、扩展 、逻辑。再用非交互式创建第二个逻辑[root@servera /]# parted /dev/vdc #直接指定设备，而不是分区(parted) mklabel #按回车键 定义分区方案 New disk label type? msdos #两一下tab键，填写 Number Start End Size Type File system Flags(parted) mkpart #分区 Partition type? primary/extended? p #主/扩展/逻辑 File system type? [ext2]? ext4 #无用，不生效Start? 2048s#第一个分区，一定从1M或2048s开始 End? 1000MB #分1G分区，1000M、1G、（10G磁盘，可以写10%，完全使用就是100%） (parted) p #查看 Number Start End Size Type File system Flags 1 1049kB 1000MB 999MB primary ext4 lba(parted) quit #退出 Information: You may need to update /etc/fstab.[root@servera ~]#udevadm settle (更新通知内核，建议敲上，不是必要)非交互式方式命令：[root@servera /]# parted /dev/vdc mkpart p ext4 1000MB 2000MB parted使用GPT方案12345678910111213141516171819202122232425262728293031323334353637[root@clear /]# parted /dev/vdc #使用了上一个实验的同样的分区，重复使用了vdc(parted) mklabel New disk label type? gpt #再更改分区方案时会抹掉之前的分区数据Warning: The existing disk label on /dev/vdc will be destroyed and all data on this disk will be lost. Doyou want to continue?Yes/No? yes #yes (parted) p BPartition Table: gpt #此处显示gpt分区方案(parted) mkpart Partition name? []? part1 #分区名字 File system type? [ext2]? #直接回车 Start? 1M End? 10% (parted) p Number Start End Size File system Name Flags 1 1049kB 1074MB 1073MB ext2 part1 #注意Number和Name(parted) mkpartPartition name? []? part2 File system type? [ext2]? Start? 10% End? 20%(parted) pNumber Start End Size File system Name Flags 1 1049kB 1074MB 1073MB ext2 part1 2 1074MB 2147MB 1074MB ext2 part2 #第二个分区完成(parted) rm Partition number? 2 #选择第二个分区的Number (parted) p Number Start End Size File system Name Flags 1 1049kB 1074MB 1073MB ext2 part1(parted) q #非交互式方式重新分第二个分区[root@clear /]# parted /dev/vdc mkpart part2 ext2 10% 20% [root@clear /]# parted /dev/vdc pNumber Start End Size File system Name Flags 1 1049kB 1074MB 1073MB part1 2 1074MB 2147MB 1074MB part2 #成功分配 lsblk lsblk /dev/vdc blkid 12.2 文件系统123456781.分区后，会给分区定义文件系统类型windos: fat32 ntfs exfat linux : exfat 有ext2 ext3 ext4 xfs vfat MacOS : exfat 2.为什么定义文件系统？给磁盘定义一种存储数据的方法，这样块存储设备才可以记录文件数据。3.每个分区都可以定义一个独立的文件系统，定义的方法就是格式化。设备有了文件系统后才可以存储数据。4.操作系统里分区很多，每个分区可能会有相同或不同的文件系统类型，彼此独立5.VFS 虚拟文件系统：作用，将用户发送的指令给任何文件系统做翻译，对于用户来说，我不必学习不同文件系统的操作方法，而是使用常用shell管理指令，就可以通过VFS传递给不同的文件系统了。 格式化123456789语法：mkfs 选项 文件系统类型 设备名mkfs -t ext4 /dev/vdb1选项：-t： 指定文件系统 -t ext4例：[root@servera /]# mkfs -t ext4 /dev/vdc1 例2：使用.点代替-t[root@servera /]# mkfs.ext4 /dev/vdc2 lsblk12345678910111213141516171819202122232425[root@servera /]# lsblk --fs /dev/vdc #-f --fsNAME FSTYPE LABEL UUID MOUNTPOINTvdc ├─vdc1 ext4 af656cc6-80e3-4b05-abcf-a162907c2f0a └─vdc2 xfs 64237913-4937-48ff-8afa-28c6fc05124d [root@servera /]# parted /dev/vdc pModel: Virtio Block Device (virtblk)Disk /dev/vdc: 5369MBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags 1 1049kB 1000MB 999MB primary ext4 2 1000MB 2000MB 999MB primary xfs 查看文件系统lsblk --fs /dev/vdcblkid 超级管理员可用 也可查文件系统类型[root@servera ~]# lsblk -f /dev/vdb1NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTSvdb1 xfs e36c91a4-83a0-4dba-a015-3f62bdc32b60 [root@servera ~]# blkid /dev/vdb1/dev/vdb1: UUID=&quot;e36c91a4-83a0-4dba-a015-3f62bdc32b60&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;bd4097dc-01&quot; 12.3 挂载与永久挂载mount12345678910111213141516mount挂载也称为手动挂载，或临时挂载，重启失效语法：mount 文件系统 挂载点 #源设备需要格式化后，成为文件系统才可挂载 #挂载点需要是空目录例1：mount /dev/vdb1 /mnt/disk_vdb1选项：-t 指定文件系统类型（默认自动识别） mount -t ext4 /dev/vdb1 /mnt/disk_vdb1-o 指定挂载权限卸载--》再指定权限 mount -o ro /dev/vdb1 /mnt/disk_vdb1例2：remount 重新挂载 （不卸载的基础上重新挂载）mount -o remount,ro /dev/vdc2卸载：umount 文件系统/挂载点例：umount /dev/vdb1 #或者 umount /mnt/disk_vdb1 #卸载时要退出/mnt/disk_vdb1目录 1234567891011121314151617181920212223242526271.临时挂载 [root@clear dev]# mkdir /mnt/disk1 #创建挂载点[root@clear dev]# mount /dev/vdc1 /mnt/disk1 #挂载[root@clear dev]# df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/vdc1 ext4 991M 2.6M 922M 1% /mnt/disk1[root@clear dev]# cd /mnt/disk1 #进入目录 [root@clear disk1]# touch haha #使用[root@clear disk1]# lshaha lost+found[root@clear disk1]# cd /卸载[root@clear /]# umount /dev/vdc1[root@clear /]# df -Th #查看发现已经没有/dev/vdc1的挂载信息，就算成功了 。2.使用UUID方式挂载[root@clear /]# lsblk -f #也可以查看UUID[root@clear /]# blkid[root@clear /]# blkid /dev/vdc1/dev/vdc1: UUID=&quot;9bdeba87-5ad0-4c52-b577-0234115df2e1&quot; TYPE=&quot;ext4&quot; PARTLABEL=&quot;part1&quot; PARTUUID=&quot;28f6e868-cd04-434c-9fd3-618f17382797&quot;[root@clear /]# mount UUID=&quot;9bdeba87-5ad0-4c52-b577-0234115df2e1&quot; /mnt/disk1 #鼠标左键选定后按滑轮中间。[root@clear /]# df -Th/dev/vdc1 ext4 991M 2.6M 922M 1% /mnt/disk1mount方式挂载，重启失效[root@clear /]# df -Th[root@clear /]# reboot #重启后再查看挂载 /etc/fstab1234567891011121314151617181920212223242526mount临时挂载:mount /dev/vdb1 /mnt/disk1mount -t ext4 -o rw /dev/vdb1 /mnt/disk1 //指定参数以只读方式挂载2.开机自动挂载 永久[root@clear /]# df -h/dev/vdc1 991M 2.6M 922M 1% /mnt/disk1[root@clear /]# umount /dev/vdc1[root@clear /]# df -h[root@clear /]# vim /etc/fstab /dev/vdc1 /mnt/disk1 ext4 defaults 0 0设备ID或设备名称 挂载点 文件系统类型 选项（权限，认证设定，其他） 内核日志检测机制0不检测，1、2检测，1比2优先级高 磁盘检测机制0不检测、1、2检测，1比2优先级高[root@clear /]# mount -a #挂载/etc/fstab中所有未挂载的设备[root@clear /]# df -Th/dev/vdc1 ext4 991M 2.6M 922M 1% /mnt/disk1#reboot #模拟考试环境，没做完selinux安全端口别重启，会起不来`注意`如果/etc/fstab中配置错误导致不能启动系统，重启时会自动进入紧急模式.解决方案流程：1 此时输入管理员root密码2 编辑挂载权限 #mount -o remount,rw / 3 vi /etc/fstab更改正内容或将其用 #井号注释挂载内容后，4 reboot即可 12.4 管理交换分区创建交换分区swap1234567891011121314151617181920212223242526272829303132333435363738394041424344454647创建swap 流程：1. 分区---1G[root@clear /]# parted /dev/vdc mkpart part3 ext2 20% 30% Information: You may need to update /etc/fstab.[root@clear /]# parted /dev/vdc pNumber Start End Size File system Name Flags 1 1049kB 1074MB 1073MB ext4 part1 2 1074MB 2147MB 1074MB xfs part2 3 2147MB 3221MB 1074MB part32. 查询系统中swap空间，使用free命令[root@servera ~]# free -m total used free shared buff/cache availableMem: 1763 371 1398 13 144 1391Swap: 0 0 0[root@servera ~]# free -h total used free shared buff/cache availableMem: 1.7Gi 371Mi 1.4Gi 13Mi 144Mi 1.4GiSwap: 0B 0B 0B3. 格式化为swap[root@servera ~]# mkswap /dev/vdb2Setting up swapspace version 1, size = 1024 MiB (1073737728 bytes)no label, UUID=2cc08ea9-26ea-4e28-9ff5-070c84366f914. 给系统加载swap空间 4.1 临时加载与卸载，重启失效 [root@servera ~]# swapon /dev/vdb2[root@servera ~]# free -m total used free shared buff/cache availableMem: 1763 372 1398 13 144 1391Swap: 1023 0 1023[root@servera ~]# swapoff /dev/vdb2[root@servera ~]# free -m total used free shared buff/cache availableMem: 1763 372 1396 13 148 1391Swap: 0 0 0 4.2 写入/etc/fstab文件永久生效[root@servera ~]# vim /etc/fstab/dev/vdb2 swap swap defaults 0 0[root@servera ~]# swapon -a #swap空间用swapon -a挂载[root@servera ~]# free -m total used free shared buff/cache availableMem: 1763 377 1390 13 149 1385Swap: 1023 0 1023 使用本地存储创建swap12345678910swap-file# df -h# dd if=/dev/zero of=/pagefile.sys bs=512M count=1# mkswap /pagefile.sys # chmod 0600 /pagefile.sys # vim /etc/fstab .../pagefile.sys swap swap defaults 0 0 # swapon -a# free -h 13 逻辑卷管理LVM13.1 创建和扩展逻辑卷逻辑卷管理概述12345逻辑卷(LVM):基于内核的一种逻辑卷管理器，LVM适合于管理大存储设备，并允许用户动态调整文件系统的大小物理卷(PV)：LVM的最底层概念，是LVM的逻辑存储块，物理卷与磁盘分区是逻辑的对应关系卷组(VG)：LVM逻辑概念上的磁盘设备，通过将单个或多个物理卷组合后生成卷组。卷组的大小取决于物理卷的容量以及个数物理长度(PE)：物理长度是将物理卷组合为卷组后，所划分的最小存储单位，即逻辑意义上磁盘的最小存储单元。LVM默认PE大小为4MB逻辑卷(LV):LVM逻辑意义上的分区，可以指定从卷组中提取多少容量来创建逻辑卷，最后对逻辑卷格式化并挂载使用 构建LVM存储123456789101.分区或者添加物理硬盘 fdisk,parted (mbr,gpt)2.pv让物理磁盘或分区变成lvm可用的卷 pvcreate 3.创建vg同时指定pe块大小 vgcreate -s 4.在vg中划分lv空间 lvcreate -L | -l -n vgname5.格式化lv空间6.挂载或者永久挂载PV语法：pvcreate /dev/vdb1 /dev/vdb #分区名称或磁盘名VG语法：vgcreate -s 16M vgname pvname #-s不写默认4M，扩大的话可以是2的次方，比如4、8、16、32...，vgname是vg名称自定义，pvname就是创建pv时的物理分区名，就是pv名称LV语法：lvcreate -L 容量 -n lvname vgname #指定容量可以使用-L 容量 或 -l 块数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281.分区 至少分3个分区，模拟物理磁盘，作为pv的底层物理块设备[root@clear ~]# fdisk -l /dev/vdbDevice Boot Start End Sectors Size Id Type/dev/vdb1 2048 2099199 2097152 1G 83 Linux/dev/vdb2 2099200 4196351 2097152 1G 83 Linux/dev/vdb3 4196352 6293503 2097152 1G 83 Linux/dev/vdb4 6293504 20971519 14678016 7G 5 Extended/dev/vdb5 6295552 8392703 2097152 1G 83 Linux2.PV阶段[root@clear ~]# man pvcreate ,搜索/EXAMPLES[root@servera ~]# pvcreate /dev/vdb{1,3,5} Physical volume &quot;/dev/vdb1&quot; successfully created. Physical volume &quot;/dev/vdb3&quot; successfully created. Physical volume &quot;/dev/vdb5&quot; successfully created.[root@clear ~]# pvs 或者 pvscan 或者 pvdisplay进行查询[root@servera ~]# pvscan PV /dev/vdb1 lvm2 [1.00 GiB] PV /dev/vdb3 lvm2 [1.00 GiB] PV /dev/vdb5 lvm2 [1.00 GiB] Total: 3 [3.00 GiB] / in use: 0 [0 ] / in no VG: 3 [3.00 GiB]3.VG阶段[root@clear ~]# man vgcreate ,搜索/myvg[root@clear ~]# man vgcreate | grep myvg[root@clear ~]# vgcreate -s 16M vg100 /dev/vdb{1..3} # -s指定物理扩展块16M一个Physical volume &quot;/dev/vdb2&quot; successfully created. Volume group &quot;vg100&quot; successfully created[root@clear ~]# vgs #主要查看下vg的大小，拥有的pv和lv数量VG #PV #LV #SN Attr VSize VFree vg100 3 0 0 wz--n- 2.95g 2.95g[root@clear ~]# vgdisplay #vg名，容量，块大小，块的使用量--- Volume group --- VG Name vg100 System ID Format lvm2 Metadata Areas 3 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 3 Act PV 3 VG Size 2.95 GiB PE Size 16.00 MiB Total PE 189 Alloc PE / Size 0 / 0 Free PE / Size 189 / 2.95 GiB VG UUID mA8syS-SLSn-VO3X-VReN-Awik-2eVW-Geqqv2pv vg pe 65535 固定 16M x 65535 = 270G 2^2 4.LV阶段[root@clear ~]# man lvcreate ,搜索/64m[root@clear ~]# man lvcreate | grep 64mlvcreate -L 64m -n mylv vg00 /dev/sda:0-7 /dev/sdb:0-7[root@clear ~]# lvcreate -L 1G -n lv100 vg100 //(-L:指定具体容量、-n:指定逻辑卷名称)Logical volume &quot;lv100&quot; created.[root@clear ~]# lvsLV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert lv100 vg100 -wi-a----- 1.00g [root@clear ~]# lvdisplay --- Logical volume --- LV Path /dev/vg100/lv100 LV Name lv100 VG Name vg100 LV UUID zxJzck-SgUJ-1kBV-5rfo-aG2I-OHZ9-5KOLPt LV Write Access read/write LV Creation host, time servera.lab.example.com, 2025-03-02 00:14:49 -0500 LV Status available # open 0 LV Size 1.00 GiB Current LE 64 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:05.格式化[root@servera ~]# mkfs.xfs /dev/vg100/lv100meta-data=/dev/vg100/lv100 isize=512 agcount=4, agsize=65536 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0data = bsize=4096 blocks=262144, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0, ftype=1log =internal log bsize=4096 blocks=16384, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0Discarding blocks...Done. [root@servera ~]# lsblk -f /dev/vg100/lv100NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTSvg100-lv100 xfs e665f339-9faf-4fc9-8cbd-d29468bed8126.创建挂载点与临时挂载# mkdir /mnt/lvm1# mount /dev/vg100/lv100 /mnt/lvm1# df -h# cd /mnt/lvm/7.永久挂载[root@clear ~]# cd /[root@clear /]# echo &quot;/dev/vg100/lv100 /mnt/lvm1 xfs defaults 0 0&quot; &gt;&gt; /etc/fstab // &quot;&gt;&gt;&quot;表示追加[root@clear /]# mount -a[root@servera ~]# df -Th/dev/mapper/vg100-lv100 xfs 960M 39M 922M 5% /mnt/lvm1[root@servera ~]# ll /dev/mapper/vg100-lv100 /dev/vg100/lv100lrwxrwxrwx. 1 root root 7 Mar 2 00:24 /dev/mapper/vg100-lv100 -&gt; ../dm-0lrwxrwxrwx. 1 root root 7 Mar 2 00:24 /dev/vg100/lv100 -&gt; ../dm-08.reboot重启验证9.删除逻辑卷思路： 创建顺序---分区--lv（pv-vg-lv）--格式化---挂载 卸载顺序---卸载--删lv--删vg--删pv--删分区 vgremove vg100 扩展逻辑卷123扩展流程：1、扩展lv大小增加容量2、重新定义文件系统大小 1234567891011121314151617181920212223242526272829301 扩展语法：lvextend 【-L| -l】【size|PE】 lv_name选项：-L：指定容量 直接指定最终大小 -L 1500M ，或者在原有基础上增加多少容量 -L2-l：指定块数 -l 60 ， -l +30[root@servera ~]# lvextend -L 1500M /dev/vg100/lv100 Rounding size to boundary between physical extents: &lt;1.47 GiB. Size of logical volume vg100/lv100 changed from 1.00 GiB (64 extents) to &lt;1.47 GiB (94 extents). Logical volume vg100/lv100 successfully resized.[root@servera ~]# lvdisplay --- Logical volume --- LV Path /dev/vg100/lv100 LV Name lv100 VG Name vg100 LV UUID zxJzck-SgUJ-1kBV-5rfo-aG2I-OHZ9-5KOLPt LV Write Access read/write LV Creation host, time servera.lab.example.com, 2025-03-02 00:14:49 -0500 LV Status available # open 1 LV Size &lt;1.47 GiB Current LE 94 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:0$ lvextend -l 60 /dev/vg100/lv100 12345678910111213142 重新定义文件系统大小ext 文件系统 ”resize2fs 设备名/挂载点“ resize2fs /dev/vg100/lv100xfs 文件系统 “xfs_growfs 设备名/挂载点” xfs_growfs /dev/vg100/lv100 或 xfs_growfs 挂载点 例：如果是xfs文件系统使用：[root@servera /]# xfs_growfs /mnt/lvm (挂载点)[root@servera ~]# df -Th/dev/mapper/vg100-lv100 xfs 1.5G 43M 1.4G 3% /mnt/lvm1如果是ext4文件系统使用：[root@servera /]# resize2fs /dev/myvg/mylv （lv设备名）创建的lv名称为myly，属于myvg卷组，lv需要30个pe，每个pe16M，需要开机自动挂载到/mnt/mylvdir. 并且使用xfs文件系统 缩小逻辑卷12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879格式化:[root@servera ~]# mkfs.ext4 /dev/vg200/lv200mke2fs 1.46.5 (30-Dec-2021)Discarding device blocks: done Creating filesystem with 385024 4k blocks and 96384 inodesFilesystem UUID: acc0ec9f-90af-4373-8daa-c9525954aeefSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): doneWriting superblocks and filesystem accounting information: done永久挂载:[root@servera ~]# mkdir /mnt/lvm2[root@servera ~]# echo &quot;/dev/vg200/lv200 /mnt/lvm2 ext4 defaults 0 0&quot; &gt;&gt; /etc/fstab[root@servera ~]# mount -amount: (hint) your fstab has been modified, but systemd still uses the old version; use 'systemctl daemon-reload' to reload.[root@servera ~]# systemctl daemon-reload[root@servera ~]# mount -a[root@servera ~]# df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/mapper/vg100-lv100 xfs 1.5G 43M 1.4G 3% /mnt/lvm1/dev/mapper/vg200-lv200 ext4 1.5G 24K 1.4G 1% /mnt/lvm2ext文件系统可以缩小，xfs不支持:流程：1. 卸载[root@servera ~]# umount /mnt/lvm22. resize2fs 定义缩小后的大小[root@servera ~]# resize2fs /dev/vg200/lv200 1Gresize2fs 1.46.5 (30-Dec-2021)Please run 'e2fsck -f /dev/vg200/lv200' first.3. 磁盘检测[root@servera ~]# e2fsck -f /dev/vg200/lv200e2fsck 1.46.5 (30-Dec-2021)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary information/dev/vg200/lv200: 11/96384 files (0.0% non-contiguous), 15380/385024 blocks4. 重新执行resize2fs命令[root@servera ~]# resize2fs /dev/vg200/lv200 1Gresize2fs 1.46.5 (30-Dec-2021)Resizing the filesystem on /dev/vg200/lv200 to 262144 (4k) blocks.The filesystem on /dev/vg200/lv200 is now 262144 (4k) blocks long.5.重新挂载并查看[root@servera ~]# mount -a[root@servera ~]# df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/mapper/vg100-lv100 xfs 1.5G 43M 1.4G 3% /mnt/lvm1/dev/mapper/vg200-lv200 ext4 973M 24K 906M 1% /mnt/lvm25. lvresize -L 1G 逻辑卷名[root@servera ~]# lvresize -L 1G /dev/vg200/lv200[root@servera ~]# lvdisplay --- Logical volume --- LV Path /dev/vg200/lv200 LV Name lv200 VG Name vg200 LV UUID 0jTstG-VDnH-8IAu-ixXm-blym-tenp-d6t4Jt LV Write Access read/write LV Creation host, time servera.lab.example.com, 2025-03-02 00:49:11 -0500 LV Status available # open 1 LV Size 1.00 GiB Current LE 64 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:1 13.2 管理分层存储stratis123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657581.1 安装$ yum install -y stratis-cli stratisd$ systemctl enable --now stratisd```1.2 部署存储池1.为磁盘分区(可选用磁盘或分区)，模拟块设备（磁盘） [root@servera ~]# stratis pool create pool1 /dev/vdb #创建池名为pool1[root@servera ~]# stratis pool list #列出池信息包括大小[root@servera ~]# stratis blockdev list #列出池有哪些物理块设备组成[root@servera ~]# stratis pool add-data pool1 /dev/vdc #添加额外存储[root@servera ~]# stratis pool list[root@servera ~]# stratis blockdev list ool Name Device Node Physical Size Tierpool1 /dev/vdb 5 GiB Datapool1 /dev/vdc 5 GiB Data1.3 创建文件系统[root@servera ~]# man stratis #搜/example[root@servera ~]# stratis filesystem create pool1 fs1 #在pool1中创建文件系统fs1[root@servera ~]# stratis filesystem list #列出文件系统信息Pool Name Name Used Created Device UUID pool1 fs1 546 MiB Sep 16 2023 21:30 /dev/stratis/pool1/fs1 0e40b338-6178-4bee-87b3-6e13b8be5ad0 6d1ed6e714a6428eb374549b4fdd8d2b [root@servera ~]# mkdir /mnt/stratisdisk #创建挂载点如何查看uuid[root@servera ~]# lsblk --output=UUID /dev/stratis/pool1/fs1[root@servera ~]# mount /dev/stratis/pool1/fs1 /mnt/stratisdisk/ #临时挂载```1.4 备份-创建测试文件[root@servera ~]# touch /mnt/stratisdisk/haha.txt-备份[root@servera ~]# stratis filesystem snapshot pool1 fs1 fs1.bak[root@servera ~]# rm -f /mnt/stratisdisk/haha.txt [root@servera ~]# umount /mnt/stratisdisk [root@servera ~]# mount /dev/stratis/pool1/fs1.bak /mnt/stratisdisk[root@servera ~]# ls /mnt/stratisdiskhaha.txt1.5 开机自动挂载```bashvim /etc/fstab/dev/stratis/pool1/fs1.bak /mnt/stratisdisk xfs _netdev 0 0 #_netdev 延迟挂载，先连接网络再挂载文件系统mount -a[root@clear ~]# systemctl start stratis[root@clear ~]# stratis pool create mypool /dev/vdb1[root@clear ~]# stratis pool add-cache mypool /dev/vdb2[root@clear ~]# stratis blockdev list mypool Pool Name Device Node Physical Size Tiermypool /dev/vdb1 1 GiB Datamypool /dev/vdb2 1 GiB Cache VDO123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475## 2 VDO### 1.1安装```bash该实验需要在普通环境上做，恢复init快照，默认是294，要切换至rh134课程切换方法：【foundation】[kiosk@foundation0 ~]$ rht-clearcourse 0[kiosk@foundation0 ~]$ rht-setcourse rh134[kiosk@foundation0 ~]$ cat /etc/rht | grep RHT_COURSE RHT_COURSE=rh134 #必须保证课程是rh134才可以[kiosk@foundation0 ~]$ for i in classroom bastion workstation servera;do rht-vmctl start $i;done #开这4台虚拟机，打开时稍等3分钟左右，让其保证都开启，然后ping一下测试连通性[kiosk@foundation0 ~]$ for i in classroom bastion workstation servera;do ping -c 4 $i;done 结果都是以下结果即可，应出现icmp_seq=1 ttl=64 time=1.13 ms字样：PING bastion.lab.example.com (172.25.250.254) 56(84) bytes of data.64 bytes from bastion.lab.example.com (172.25.250.254): icmp_seq=1 ttl=64 time=1.13 ms64 bytes from bastion.lab.example.com (172.25.250.254): icmp_seq=2 ttl=64 time=0.180 ms64 bytes from bastion.lab.example.com (172.25.250.254): icmp_seq=3 ttl=64 time=0.234 ms64 bytes from bastion.lab.example.com (172.25.250.254): icmp_seq=4 ttl=64 time=0.265 m【foundation】ssh student@workstation【workstation】lab advstorage-vdo start #运行脚本，运行前，保证servera已打开ssh student@servera【servera】[root@servera ~]# sudo -i[root@servera ~]# yum search vdo[root@servera ~]# yum install -y vdo.x86_64 kmod-kvdo.x86_64[root@servera ~]# systemctl enable --now vdo```### 1.2 部署vdo```bash[root@servera ~]# man vdo #/EXAMPLE*[root@servera ~]# vdo create --name=vdo1 --device=/dev/vdd --vdoLogicalSize=50G[root@servera ~]# vdo list[root@servera ~]# vdo status --name=vdo1[root@servera ~]# vdo status --name=vdo1 | grep Dedu[root@servera ~]# vdo status --name=vdo1 | grep Comp```### 1.3 格式化及临时挂载测试```bash格式化*[root@servera ~]# mkfs.xfs -K /dev/mapper/vdo1 #-K让命令可以快速返回，效果类似快速格式化 ，如果已有文件系统 可以使用 -f强制执行 挂载*[root@servera ~]# mkdir /mnt/vdo1[root@servera ~]# mount /dev/mapper/vdo1 /mnt/vdo1[root@servera ~]# df -Th[root@servera ~]# vdostats --human-readable[root@servera ~]# cp /root/install.img /mnt/vdo1/install.img.1[root@servera ~]# vdostats --human-readable[root@servera ~]# vdostats --human-readable[root@servera ~]# cp /root/install.img /mnt/vdo1/install.img.2[root@servera ~]# vdostats --human-readable vdostats --human-readablels /mnt/vdo1/lab advstorage-vdo finish```### 1.4 开机自动挂载```bash开机自动挂载方法：教材推荐*[root@servera ~]# man vdo- | grep x-systemd/dev/mapper/vdo1 /mnt/vdo1 xfs defaults,x-systemd.requires=vdo.service 0 0mount -a方法二：/dev/mapper/vdo1 /mnt/vdo1 xfs defaults,_netdev 0 0 14 访问网络存储14.1 NFS挂载网络附加存储查看NFS共享1234普通环境[root@servera ~]# showmount -e 172.25.254.250 #后面IP地址是NFS服务器的地址，环境默认已经部署好Export list for 172.25.254.250: #来自250的共享/content 172.25.0.0/255.255.0.0 #服务器共享的目录 客户端挂载 mount123456789语法：mount -t nfs ip/域名:共享目录 本地挂载点练习：[root@servera ~]# mkdir /mnt/nfs1#mount -t nfs -o ro 172.25.254.250:/content /mnt/nfs1 #-t nfs指定文件系统为nfs，-o ro指定挂载权限为只读，这两个选项是可选的。[root@servera ~]# mount 172.25.254.250:/content /mnt/nfs1[root@servera ~]# df -ThFilesystem Type Size Used Avail Use% Mounted on172.25.254.250:/content nfs4 244G 102G 142G 42% /mnt/nfs1 开机自动挂载 /etc/fstab12345678[root@servera ~]# umount /mnt/nfs1[root@servera ~]# vim /etc/fstab172.25.254.250:/content /mnt/nfs1 nfs defaults 0 0[root@servera ~]# mount -a[root@servera ~]# df -ThFilesystem Type Size Used Avail Use% Mounted on172.25.254.250:/content nfs4 244G 102G 142G 42% /mnt/nfs1[root@servera ~]# reboot (重启后使用df -Th查看挂载状态)模拟考试环境没做selinux题，先别重启，起不来 14.2 自动挂载网络附加存储autofs自动挂载示例1123456789101112131415161718192021222324252627282930313233343536373839404142servera上做该练习，挂载f0，172.25.254.250上的/content到servera上的/mnt/nfs10，读写权限-mountmkdir /mnt/nfs10mount 172.25.254.250:/content /mnt/nfs10-fstabmkdir /mnt/nfs10vim /etc/fstab172.25.254.250:/content /mnt/nfs10 nfs4 defaults 0 0-autofs1.[root@clear home]# yum search autofs [root@clear home]# yum install -y autofs2.[root@servera ~]# rpm -qc autofs /etc/auto.master # 主配置文件 /etc/auto.misc # 映射文件 /etc/auto.net /etc/auto.smb /etc/autofs.conf /etc/autofs_ldap_auth.conf /etc/sysconfig/autofs /usr/lib/systemd/system/autofs.service3.[root@servera ~]# vim /etc/auto.master 基础挂载点 映射文件 /data /etc/auto.misc # /data是基础挂载点，表示挂载是从这里开始的，系统自动创建/data4.[root@servera ~]# vim /etc/auto.misc 映射挂载点 文件系统类型 权限 共享目录 nfs10 -fstype=nfs,rw 172.25.250.250:/content # auto.misc是映射文件，内容为映射挂载点、挂载权限、共享目录路径等5.[root@servera ~]# systemctl restart autofs # 重启让以上配置生效6.验证[root@servera ~]# ls /data/nfs10 # ls是使用挂载点目录boot courses ebook ks manifests rhel7.0 rhel9.0 rhel9.3 rhtops slides ucf[root@servera ~]# df -h | tail -1172.25.254.250:/content 244G 102G 142G 42% /data/nfs10 # 才会出现挂载项注明:autofs共享服务只有在访问时才会看到挂载信息 autofs 直接映射1234567891011121314[root@clear /]# umount /mnt/nfs101.主配置文件[root@clear /]# vim /etc/auto.master.d/nfs.autofs #nfs.autofs 名称规则和上个实验一样/- /etc/auto.nfs #所有直接映射条目使用/-作为基础目录2.映射文件[root@clear /]# vim /etc/auto.nfs #固定文件名/mnt/nfs10 -rw 172.25.254.250:/content3.重启服务[root@clear /]# systemctl restart autofs4.测试[root@clear /]# ll /mnt/nfs1 #cd或ll方式都可以访问共享目录内容，即可挂载[root@servera /]# df -hFilesystem Size Used Avail Use% Mounted on172.25.254.250:/content 491G 66G 426G 14% /mnt/nfs1 autofs 间接映射12345678910111213141.主配置文件[root@clear /]# vim /etc/auto.master.d/nfs.autofs /mnt /etc/auto.nfs2.映射文件[root@clear /]# vim /etc/auto.nfs * -rw 172.25.254.250:/&amp; #如果共享导出了多个子目录可以用&amp;代替子目录的名称，而星*号会自动匹配共享的目录名称* -rw 172.25.254.250:/&amp; /content /public /share 3.重启服务[root@clear /]# systemctl restart autofs4.测试[root@clear /]# cd /mnt/content;ls[root@clear remoteuser1]# df -h #应该能看到挂载信息 15 启动流程15.1 选择启动目标Linux 9启动过程12345678910111213141516171、计算机接通电源。系统固件（现代UEFI或更旧的BIOS）运行开机自检（POST），并开始初始化部分硬件 使用系统BIOS或UEFI配置屏幕(早期按F2可进入设置)2、系统固件会搜索可启动设备，可能是在UEFI启动固件中配置的，也可能按照BIOS中配置的顺序搜索所有磁盘上的主引导记录（MBR） 使用系统BIOS或UEFI配置屏幕（早期按F2可进入设置）3、系统固件会从磁盘读取启动加载器，然后将系统控制权交给启动加载器。红帽企业版Linux8中，启动加载器为GRand Unified Bootloader version2（GRUB2） 使用grub2-install命令进行配置，它将安装GRUB2作为磁盘上的启动加载器。4、GRUB2将从/boot/grub2/grub.cfg文件加载配置并显示一个菜单，从中可以选择要启动的内核。 使用/etc/grub.d/目录、/etc/default/grub文件和grub2-mkconfig命令进行配置，以生成/boot/grub2/grub.cfg文件。5、选择内核超时到期后，启动加载器从磁盘中加载内核和initramfs，并将他们放入内存中。initramfs是一个存档，其中包含启动时所有必要硬件的内核模块、初始化脚本等等。在redhat8中，initramfs包含自身可用的整个系统。 使用/etc/dracut.conf.d/目录、dracut命令和lsinitrd命令进行配置，以检查initramfs文件。6、启动加载器将控制权交给内核，从而传递启动加载器的内核命令行中指定的任何选项，以及initramfs在内存中的位置7、对于内核可在initramfs中找到驱动程序的所有硬件，内核会初始化这些硬件，然后作为PID1从initramfs执行/sbin/init。在redhat8中/sbin/init是一个指向systemd的链接。8、initramfs中的systemd进程会执行initrd.target目标的所有单元。这包括将磁盘上的root文件系统挂载于/sysroot目录。 使用/etc/fstab进行配置9、内核将root文件系统从initramfs切换回/sysroot中的root文件系统。随后，systemd会使磁盘中安装的systemd副本来重新执行。10、systemd会查找从内核命令行传递或系统中配置的默认目标，然后启动或停止单元，以符合该目标的配置，从而自动解决单元之间依赖关系。本质上，systemd进程是一组系统应激活以达到所需状态的单元。这些进程通常启动一个基于文本的登录或图形登录屏幕 可使用/etc/systemd/system/default.target和/etc/systemd/system/进行配置。 重启和关机12345678-RHEL9&lt;=关机：systemctl poweroff init 0 重启：systemctl reboot init 6reboot 选择SYSTEND TERGET下表列出了systemd启动达到的systemd单元 目标 用途 graphical.target 多用户、图形、文本登录 multi-user.target 多用户、文本登录 rescue.target 救援 emergency.target 紧急，进入initramfs环境，root只读形式挂载于/上 123systemctl list-units --type=target --allsystemctl list-unit-files --state=enabledsystemctl list-dependencies graphical.target | grep target 运行时选择target12345切换图形或字符[root@workstation ~]# systemctl isolate multi-user.target 或 init 3 [root@workstation ~]# systemctl isolate graphical.target 或 init 5 [root@workstation ~]# grep AllowIsolate /usr/lib/systemd/system/multi-user.targetAllowIsolate=yes 单元文件中包含AllowIsolate=yes才可以进行切换 设置默认target1234设置默认的启动目标[root@servera ~]# systemctl get-default [root@servera ~]# syetemctl set-default multi-user.target[root@servera ~]# ll /etc/systemd/system/default.target 启动时选择其他目标123456789101112【f0】grub 账号：rootgrub 密码： Asimovreboot选择内核的位置按elinux.....&lt;按键end或者ctrl+e&gt; systemd.unit=emergency.target #multi-user.target 或graphical.target ctrl+x 如果进入了紧急模式，需要输入root密码mount -o remount,rw /vim /etc/fstabexit 或 reboot 15.2 重置root密码修改登录密码12345678910111213141516171819202122232425262728293031切换模拟考试环境，serverb-serverb#打开serverb的视图模式，-Activites-所有应用-Virtaul Machine Manager-双击serverb-点击左上角三个小方块-ctrl+alt+del1.重新启动系统。$ reboot（Send key 选择ctrl+alt+del）2.按任意键(Enter除外)中断启动加载器倒计时。$ 按方向键 ↓ 下3.将光标移到要引导的救援内核条目 (名称中带有rescue一词的条目)。$ 光标停留第二个内核rescue位置4.按e编辑选定的条目。$ e5.将光标移到内核命令行 (以inux开头的行)。$ linux... 按ctrl+e移动光标至最后6.附加rd.break。利用该选项，就在系统从initramfs 向实际系统移交控制权前，系统将会中断。$ rd.break console=tty07.按ctrl+x使用这些更改来启动。$ ctrl+x8.提示时，按Enter执行维护。$ 救援模式下回车9.给根目录设置rw权限$ mount -o remount,rw /sysroot/10.进入根目录$ chroot /sysroot/ #cd / ,ls -a /11.修改密码$ echo redhat | passwd --stdin root12.添加/.autorelabel文件，刷新selinux标记$ touch /.autorelabel #[root@servera ~]# systemctl status selinux-autorelabel13.退出当前shell，会自动重启$ exit$ exit 15.3 诊断和修复文件系统123456789当/etc/fstab中挂载信息写错可能会导致系统起不来，会自动进入紧急模式下表为常见错误： 问题 结果----------------------------- --------------------------------------------------------------- 文件系统损坏 systemd尝试修复，无法修复进入紧急(emergency)模式 /etc/fstab systemd等待一定时间，等设备变得可用。如不可用，则进入紧急模式 引用的设备/UUID不存在 /etc/fstab 挂载点不存在 直接进入紧急模式 /etc/fstab挂载点错误 直接进入紧急模 文件系统、选项错误(权限) 直接进入紧急模式123456789版本建议：编辑/etc/fstab后使用systemd daemon-reload加载。否则systemd可能会继续使用用旧版本。解决方法：#当进入紧急模式后Give root password for maintenance(or pressControl-D to continue): redhat #输入root密码后按回车进入系统1 输入root密码2 mount -o remount,rw / #RHEL9中/根目录权限是可写的，所以无需敲该指令3 vim /etc/fstab #可将错误字段需改，或在挂载行前，直接添加#井号注释掉该行。然后reboot重启或者将之前编写的挂载项注释掉，让系统先正常启动，然后再修改调试。 16 firewalld1234firewalld、iptables 策略限制MAC、IP、PORT、APPselinux 上下文、布尔值，端口软件权限 读写执行等文件系统 rwx，隐藏权限，acl 16.1 防火墙架构概念123456789101112131 现代的计算机环境不仅需要流畅的网络连通，更需要具备安全性的网络，所以我们除了及时更新系统补丁以外事件，可以针对自己的计算机环境设置接收哪些数据和不接收哪些数据。我们可以使用软件来实现这一目的，这种软件就叫做防火墙。防火墙这种软件会制定一些规则，我们也叫做策略，管理数据包的通过。 2 firewalld优点 iptables在于可使用zone、service简化配置任务，易于管理。而且属于动态管理（修改规则后不必重启整个防火墙服务）。3.zone区域firewalld预先提供了多个区域，不同区域拥有不同设置好的规则，类似不同的管理方案。系统默认区域为public。所有区域默认在系统中会同时开启但是 如何判断我们的数据包在哪个区域中实现了数据限制，通过以下三种方法来判断：- source，源地址- interface，接收请求的网卡（ 一个网卡，只能绑定一个区域。多个网卡，可以绑定同一区域）- firewalld.conf中配置的默认zone三个优先级按次序降低。匹配后不再向下匹配。 nftables增强了netfilter1nftables---&gt;替代---&gt;netfilter firewalld简介12rhel7之前---iptables（静态）rhel7之后---firewalld（动态） 预定义区域12[root@foundation0 ~]# firewall-cmd --get-zonesblock dmz drop external home internal libvirt public trusted work 预定义服务1[root@foundation0 ~]# firewall-cmd --get-service 16.2 配置防火墙123456管理方法 解释----------------------------------- --------------------- /etc/firewalld 直接修改配置文件 Web控制台图形界面 通过cockpit firewalld-cmd命令行 shell命令行直接执行 firewall-config 图形 Web控制台配置防火墙1234567891011121314151617181920212223242526【servera】[root@servera ~]# systemctl start cockpit #开启cockpit功能[root@servera ~]# netstat -ntlp #查看是否开启了9090端口[root@servera ~]# firewall-cmd --list-all #查看防火墙策略services: cockpit dhcpv6-client http ssh #策略中包含了cockpit，表示允许访问cockpit【f0】firefox https://172.25.250.10:9090 #可以访问【servera】[root@servera ~]# firewall-cmd --permanent --remove-service=cockpitsuccess[root@servera ~]# firewall-cmd --reloadsuccess[root@servera ~]# firewall-cmd --list-all【f0】firefox https://172.25.250.10:9090 #不可以访问【servera】[root@servera ~]# firewall-cmd --permanent --add-service=cockpit #添加cockpit服务，客户端再次访问即可success[root@servera ~]# firewall-cmd --reloadsuccess[root@servera ~]# firewall-cmd --list-all 命令行配置防火墙1234567891011防火墙命令 说明---------------------------------- ------------------------------------------------------------- --get-default-zone 查看默认区域 --set-default-zone=public 设置默认区域为public，永久设置 --get-zones 列出所有可用区域 --get-active-zones 列出当前正在使用所有区域 --add-source=CIDR \\[--zone=ZONE\\] 将IP或网络到指定区域，如果未提供--zone=选项，则使用默认区域 --remove-source=CIDR 删除IP或网络.....，如果未提供--zone=选项，...则删除默认区域 \\[--zone=ZONE\\] --add-interface= 在某个区域中添加端口 --change-interface= 改变端口至某个区域 –list-all-zones123456789 防火墙命令 解释----------------------------------- ------------------ --list-all \\[--zone=ZONE\\] 查看区域所有策略 --add-service= \\[--zone=ZONE\\] 允许一个服务 --add-port= \\[--zone=ZONE\\] 允许一个端口 --remove-service= \\[--zone=ZONE\\] 移除一个服务 --remove-port= \\[--zone=ZONE\\] 移除一个端口 --permanent 永久生效 --reload 立即加载 配置防火墙练习1234561、管理防火墙服务状态systemctl status firewalld systemctl start firewalld systemctl stop firewalld systemctl restart firewalld systemctl enable --now firewalld 123456782、查看与设置默认区域man firewall-cmd #/Exfirewall-cmd --get-servicefirewall-cmd --get-zonesfirewall-cmd --get-default-zone firewall-cmd --set-default-zone=home 设置默认区域，了解就可以，不是必须要设置firewall-cmd --list-allman 5 firewalld.zones 1234567891011121314151617181920212223243.配置防火墙（考）允许服务[root@servera ~]# firewall-cmd --list-all #查看区域所有配置[root@servera ~]# firewall-cmd --permanent --add-service=http #允许http服务永久生效，要结合reload[root@servera ~]# firewall-cmd --reload[root@servera ~]# firewall-cmd --list-all移除服务[root@servera ~]# firewall-cmd --permanent --remove-service=http[root@servera ~]# firewall-cmd --reload[root@servera ~]# firewall-cmd --list-all允许端口[root@servera ~]# firewall-cmd --permanent --add-port=8000/tcp[root@servera ~]# firewall-cmd --reload[root@servera ~]# firewall-cmd --list-all移除端口[root@servera ~]# firewall-cmd --permanent --remove-port=8000/tcp[root@servera ~]# firewall-cmd --reload[root@servera ~]# firewall-cmd --list-all允许源地址到某个区域（了解）firewall-cmd --add-source=172.25.250.100 --zone=trusted --permanentfirewall-cmd --reload firewall-cmd --add-interface=enp2s0 --zone=trusted --permanentfirewall-cmd --reload 典型举例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354允许apache服务foundation-----访问-----servera的web服务思路【servera】$ firewall-cmd --permanent --remove-service=http $ firewall-cmd --reload #立即生效$ firewall-cmd --list-all$ yum install -y httpd$ echo test_page &gt; /var/www/html/index.html$ systemctl enable --now httpd$ curl localhost 访问成功后，证明本地可以正常访问test_page【foundation】$ curl http://servera 发现不能访问【servera】systemctl enable --now firewalldsystemctl status firewalldfirewall-cmd --permanent --add-service=http #--permanent 永久生效，必须添加firewall-cmd --reload #立即生效firewall-cmd --list-all【foundation】curl http://servera 发现可以访问练习：允许ftp服务servera端：yum install -y vsftpd #安装vsftpd软件systemctl enable --now vsftpdvim /etc/vsftpd/vsftpd.conf #进入配置文件 anonymou_enable=NO 改为YES #开启匿名访问systemctl restart vsftpdyum install -y ftp #安装ftp可客户端$ ftp localhost #访问本机ftp服务器Trying ::1...Connected to localhost (::1).220 (vsFTPd 3.0.3)Name (localhost:root): ftp #ftp表示匿名用户登录331 Please specify the password.Password: 直接回车f0充当client端测试：yum install -y ftpftp localhost 发现不能登录，需要在server端允许ftp服务。 16.3 SElinux安全端口12345【servera】[root@servera ~]# setenforce 1[root@servera ~]# vim /etc/httpd/conf/httpd.conf Listen 80 改成了Listen 82[root@servera ~]# systemctl restart httpd 重启不了，因为selinux 123456[root@servera ~]# man semanage port #/EX[root@servera ~]# semanage port -a -t http_port_t -p tcp 82 #数据库没有标签内容则-a，有则-m[root@servera ~]# semanage port -l | grep http重启web服务[root@servera ~]# systemctl restart httpd[root@servera ~]# systemctl enable --now httpd 123456[root@servera ~]# firewall-cmd --permanent --add-port=82/tcp #本地访问无须配置，远端主机访问需要配置[root@servera ~]# firewall-cmd --reload[root@servera ~]# firewall-cmd --list-all测试[serverb curl http://servea:82] 12富规则参考[root@servera ~]# man 5 firewalld.richlanguage 17 kickstart12345 安装方式 简介----------- -------------------------------- DVD 光盘 物理系统光盘 ISO镜像 光盘制作成为的镜像.iso文件 QCOW2镜像 云环境或虚拟环境中部署为虚拟机 17.1 KICKSTART自动安装系统1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556P293 Kickstart练习[kiosk@foundation0 ~]$ ssh workstation[student@workstation ~]$ lab start installing kickstart....[student@workstation ~]$ ssh servera[student@workstation ~]$ sudo -i redhat1.后面按照教材要求进行配置[root@servera ks-config]# vim /var/www/html/ks-config/kickstart.cfgbootloader --append=&quot;console=ttyS0 console=ttyS0,115200n8 no_timer_check net.ifnames=0 crashkernel=auto&quot; --location=mbr --timeout=1 --boot-drive=vdagraphicalkeyboard --vckeymap=uslang en_US.UTF-8repo --name=&quot;BaseOS&quot; --baseurl=&quot;http://classroom.example.com/content/rhel9.0/x86_64/dvd/BaseOS/&quot;repo --name=&quot;Appstream&quot; --baseurl=&quot;http://classroom.example.com/content/rhel9.0/x86_64/dvd/AppStream&quot;url --url=&quot;http://classroom.example.com/content/rhel9.0/x86_64/dvd/&quot;rootpw --plaintext redhatauthselect select sssdselinux --enforcingservices --disabled=&quot;kdump,rhsmcertd&quot; --enabled=&quot;sshd,rngd,chronyd&quot;timezone America/New_York --utcignoredisk --only-use=vdaautopart%pre --erroronfail echo &quot;Kickstarted on $(date)&quot; &gt;&gt; /etc/issue%end%post --erroronfailecho -n &quot;Setting default runlevel to multiuser text mode&quot;rm -f /etc/systemd/system/default.targetln -s /lib/systemd/system/multi-user.target /etc/systemd/system/default.targetecho .echo &quot;Removing linux-firmware package.&quot;dnf -C -y remove linux-firmwareecho &quot;virtual-guest&quot; &gt; /etc/tuned/active_profilecat &gt; /etc/hosts &lt;&lt; EOF127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6EOF%end%packages@core chronydracut-config-genericdracut-norescuefirewalldgrub2kernelrsynctar httpd-plymouth%end2.重启httpd服务，并开机自启动。3.关闭防火墙，selinux。 dhcp设置1234567891011121314151617181920212223242526【root@servera】[root@servera ~]# systemctl stop firewalld;setenforce 0一、dhcp1 安装[root@servera ~]# yum install -y dhcp-server2 建立配置文件[root@servera ~]# cp /usr/share/doc/dhcp-server/dhcpd.conf.example /etc/dhcp/dhcpd.confcp: overwrite '/etc/dhcp/dhcpd.conf'? y[root@servera ~]# vim /etc/dhcp/dhcpd.conf #man 5 dhcpd.conf /next-server3 配置服务[root@servera ~]# vim /etc/dhcp/dhcpd.confallow bootp;allow booting;subnet 172.25.250.0 netmask 255.255.255.0 { range 172.25.250.100 172.25.250.200; option routers 172.25.250.254; default-lease-time 600; max-lease-time 7200; filename &quot;/pxelinux.0&quot;; next-server 172.25.250.10;}4 启动并测试[root@servera ~]# systemctl enable --now dhcpd5 测试dhcp功能：设置serverb开机启动为网卡启动，当做客户端，测试能够获取ip即可 tftp and syslinux12345678910111213141516171819202122232425262728293031323334351、安装tftp服务[root@servera ~]# yum install -y tftp-server tftp[root@servera ~]# rpm -ql tftp-server2、安装syslinux-tftpboot该软件后提供了/tftpboot目录其中包含了一些引导文件、内核文件及pxelinux.0文件等，建立pxelinux.cfg目录，准备存放default文件。[root@servera ~]# yum install -y syslinux-tftpboot.noarch[root@servera ~]# rpm -ql syslinux-tftpboot[root@servera ~]# mkdir /tftpboot/pxelinux.cfg/3、将254.250上的光盘镜像相关文件挂载至本机[root@servera ~]# mkdir /content[root@servera ~]# mount 172.25.254.250:/content /content/[root@servera ~]# df -h[root@serveran ~]# cp /content/rhel9.0/x86_64/dvd/images/pxeboot/{initrd.img,vmlinuz} /tftpboot/[root@servera ~]# cp /content/rhel9.0/x86_64/dvd/isolinux/boot.msg /tftpboot/[root@servera ~]# cp /content/rhel9.0/x86_64/dvd/isolinux/isolinux.cfg /tftpboot/pxelinux.cfg/default[root@servera ~]# vim /tftpboot/pxelinux.cfg/default default vesamenu.c32 timeout 60 可以改为60，就是6秒display boot.msglabel linux menu label ^Install Red Hat Enterprise Linux 9.0.0 menu default kernel vmlinuz append initrd=initrd.img inst.stage2=ftp://172.25.250.10/dvd inst.ks=http://172.25.250.10/ks-config/kickstart.cfg quiet4、修改tftp发布目录，并启动服务及测试[root@servera ~]# vim /usr/lib/systemd/system/tftp.service [Service]ExecStart=/usr/sbin/in.tftpd -s /tftpboot (将-s /var/lib/tftpboot，更成-s /tftpboot)[root@servera ~]# systemctl enable --now tftp测试：登录servera，yum install -y tftp ,tftp 172.25.250.10,get ls.c32 quit ftp123456789101112131415161718191、安装[root@servera ~]# yum install -y vsftpd.x86_64 ftp[root@servera ~]# rpm -qc vsftpd2、配置服务[root@servera ~]# vim /etc/vsftpd/vsftpd.conf anonymous_enable=YES[root@servera ~]# mkdir /var/ftp/dvd[root@servera ~]# mount /content/rhel9.0/x86_64/isos/rhel-baseos-9.0-x86_64-dvd.iso /var/ftp/dvd/3、启动及测试df -h[root@servera ~]# systemctl enable --now vsftpd4 从serverb上测试一下tftp功能是否正常（可选）【serverb】cleint以网卡方式启动--安装1、指定ftp路径172.25.250.9/dvd 2 最小化 3 设置lvm分区进度条走完-重启，添加硬盘启动 ， 从本地硬盘启动。 保证所有服务测试正常1234561.dhcpd 分配IP，指导访问tftp2.tftp 分配启动所需文件及pxelinux安装程序3.vsftpd 共享光盘镜像4.httpd 共享ks文件其实vsftpd和httpd可以二选一，例如只用httpd服务共享光盘镜像、ks文件。 17.2 cockpit安装虚拟机[root@servera ~]# yum install -y cockpit-machines 以服务的形式启动http，一旦启动就会长期运行，在内存中产生相应的进程systemct start httpd 容器以进程的方式运行http，一旦运行完毕容器就会自动关闭，自动退出，结束运行。当再想运行http的时候需要新开启一个容器 rhel8查看一个文件， 容器 容器：1个应用 -- 好管理 apache php mysql +linux= lamp -- 支持php网站 容器：多个应用 -- 不好管理 （apache php mysql）","link":"/2025/04/20/Linux/Linux%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Kubernetes企业级运维","slug":"Kubernetes企业级运维","link":"/tags/Kubernetes%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%BF%90%E7%BB%B4/"},{"name":"Kubernetes网络系统原理","slug":"Kubernetes网络系统原理","link":"/tags/Kubernetes%E7%BD%91%E7%BB%9C%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/"},{"name":"Docker&amp;K8S","slug":"Docker-K8S","link":"/tags/Docker-K8S/"},{"name":"podman容器","slug":"podman容器","link":"/tags/podman%E5%AE%B9%E5%99%A8/"},{"name":"CKA笔记","slug":"CKA笔记","link":"/tags/CKA%E7%AC%94%E8%AE%B0/"},{"name":"MySQL数据库","slug":"MySQL数据库","link":"/tags/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Linux的基础服务","slug":"Linux的基础服务","link":"/tags/Linux%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9C%8D%E5%8A%A1/"},{"name":"Linux的基本使用","slug":"Linux的基本使用","link":"/tags/Linux%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"name":"Shell脚本","slug":"Shell脚本","link":"/tags/Shell%E8%84%9A%E6%9C%AC/"},{"name":"Ansible的基本使用","slug":"Ansible的基本使用","link":"/tags/Ansible%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"name":"Linux进阶","slug":"Linux进阶","link":"/tags/Linux%E8%BF%9B%E9%98%B6/"}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"},{"name":"容器技术","slug":"容器技术","link":"/categories/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"CKA认证","slug":"CKA认证","link":"/categories/CKA%E8%AE%A4%E8%AF%81/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"自动化运维","slug":"自动化运维","link":"/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"pages":[{"title":"","text":"个人简介 -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;个人信息：出生年月：1995年4月1日(农历)毕业院校：山西大同大学本科专业：光电信息科学与工程专业学历：统招本科常住地：四川省成都市成华区 技术知识点 Linux基础命令、Linux服务、数据库、容器技术、K8S、网络 待补充","link":"/about/index.html"},{"title":"","text":"🎈🎈相信技术的力量🎈🎈 RHCE认证 K8S认证 🎈🎈生活🎈🎈 一屋、两人、三餐、四季","link":"/album/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://luovip.github.io/img/avatar.png 网站名称：罗宇 网站地址：https://luovip.github.io/ 网站简介：Linux运维技术交流 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出!","link":"/media/index.html"},{"title":"音乐歌单收藏","text":"温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"},{"title":"","text":"碎碎念 tips：github登录后按时间正序查看、可点赞加❤️、本插件地址..「+99次查看」 碎碎念加载中，请稍等... $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ clientID: '46a9f3481b46ea0129d8', clientSecret: '79c7c9cb847e141757d7864453bcbf89f0655b24', id: '666666', repo: 'issue_database', owner: 'removeif', admin: \"removeif\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container1'); });","link":"/self-talking/index.html"}]}